\section*{Chapter 8}
\section*{Optimal Tests of Hypotheses}
\subsection*{8.1 Most Powerful Tests}
In Section 4.5, we introduced the concept of hypotheses testing and followed it with the introduction of likelihood ratio tests in Chapter 6. In this chapter, we discuss certain best tests.

For convenience to the reader, in the next several paragraphs we quickly review concepts of testing that were presented in Section 4.5. We are interested in a random variable $X$ that has pdf or $\operatorname{pmf} f(x ; \theta)$, where $\theta \in \Omega$. We assume that $\theta \in \omega_{0}$ or $\theta \in \omega_{1}$, where $\omega_{0}$ and $\omega_{1}$ are disjoint subsets of $\Omega$ and $\omega_{0} \cup \omega_{1}=\Omega$. We label the hypotheses as


\begin{equation*}
H_{0}: \theta \in \omega_{0} \text { versus } H_{1}: \theta \in \omega_{1} . \tag{8.1.1}
\end{equation*}


The hypothesis $H_{0}$ is referred to as the null hypothesis, while $H_{1}$ is referred to as the alternative hypothesis. The test of $H_{0}$ versus $H_{1}$ is based on a sample $X_{1}, \ldots, X_{n}$ from the distribution of $X$. In this chapter, we often use the vector $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ to denote the random sample and $\mathbf{x}^{\prime}=\left(x_{1}, \ldots, x_{n}\right)$ to denote the values of the sample. Let $\mathcal{S}$ denote the support of the random sample $\mathbf{X}^{\prime}=$ $\left(X_{1}, \ldots, X_{n}\right)$.

A test of $H_{0}$ versus $H_{1}$ is based on a subset $C$ of $\mathcal{S}$. This set $C$ is called the critical region and its corresponding decision rule is

\[
\begin{array}{ll}
\text { Reject } H_{0}\left(\text { Accept } H_{1}\right) & \text { if } \mathbf{X} \in C  \tag{8.1.2}\\
\text { Retain } H_{0}\left(\text { Reject } H_{1}\right) & \text { if } \mathbf{X} \in C^{c} .
\end{array}
\]

Note that a test is defined by its critical region. Conversely, a critical region defines a test.

Recall that the $2 \times 2$ decision table, Table 4.5.1, summarizes the results of the hypothesis test in terms of the true state of nature. Besides the correct decisions, two errors can occur. A Type I error occurs if $H_{0}$ is rejected when it is true, while a Type II error occurs if $H_{0}$ is accepted when $H_{1}$ is true. The size or significance\\
level of the test is the probability of a Type I error; i.e.,


\begin{equation*}
\alpha=\max _{\theta \in \omega_{0}} P_{\theta}(\mathbf{X} \in C) . \tag{8.1.3}
\end{equation*}


Note that $P_{\theta}(\mathbf{X} \in C)$ should be read as the probability that $\mathbf{X} \in C$ when $\theta$ is the true parameter. Subject to tests having size $\alpha$, we select tests that minimize Type II error or equivalently maximize the probability of rejecting $H_{0}$ when $\theta \in \omega_{1}$. Recall that the power function of a test is given by


\begin{equation*}
\gamma_{C}(\theta)=P_{\theta}(\mathbf{X} \in C) ; \quad \theta \in \omega_{1} . \tag{8.1.4}
\end{equation*}


In Chapter 4, we gave examples of tests of hypotheses, while in Sections 6.3 and 6.4, we discussed tests based on maximum likelihood theory. In this chapter, we want to construct best tests for certain situations.

We begin with testing a simple hypothesis $H_{0}$ against a simple alternative $H_{1}$. Let $f(x ; \theta)$ denote the pdf or pmf of a random variable $X$, where $\theta \in \Omega=\left\{\theta^{\prime}, \theta^{\prime \prime}\right\}$. Let $\omega_{0}=\left\{\theta^{\prime}\right\}$ and $\omega_{1}=\left\{\theta^{\prime \prime}\right\}$. Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ be a random sample from the distribution of $X$. We now define a best critical region (and hence a best test) for testing the simple hypothesis $H_{0}$ against the alternative simple hypothesis $H_{1}$.

Definition 8.1.1. Let $C$ denote a subset of the sample space. Then we say that $C$ is a best critical region of size $\alpha$ for testing the simple hypothesis $H_{0}: \theta=\theta^{\prime}$ against the alternative simple hypothesis $H_{1}: \theta=\theta^{\prime \prime}$ if\\
(a) $P_{\theta^{\prime}}[\mathbf{X} \in C]=\alpha$.\\
(b) And for every subset $A$ of the sample space,

$$
P_{\theta^{\prime}}[\mathbf{X} \in A]=\alpha \Rightarrow P_{\theta^{\prime \prime}}[\mathbf{X} \in C] \geq P_{\theta^{\prime \prime}}[\mathbf{X} \in A] .
$$

This definition states, in effect, the following: In general, there is a multiplicity of subsets $A$ of the sample space such that $P_{\theta^{\prime}}[\mathbf{X} \in A]=\alpha$. Suppose that there is one of these subsets, say $C$, such that when $H_{1}$ is true, the power of the test associated with $C$ is at least as great as the power of the test associated with every other $A$. Then $C$ is defined as a best critical region of size $\alpha$ for testing $H_{0}$ against $H_{1}$.

As Theorem 8.1.1 shows, there is a best test for this simple versus simple case. But first, we offer a simple example examining this definition in some detail.

Example 8.1.1. Consider the one random variable $X$ that has a binomial distribution with $n=5$ and $p=\theta$. Let $f(x ; \theta)$ denote the pmf of $X$ and let $H_{0}: \theta=\frac{1}{2}$ and $H_{1}: \theta=\frac{3}{4}$. The following tabulation gives, at points of positive probability density, the values of $f\left(x ; \frac{1}{2}\right), f\left(x ; \frac{3}{4}\right)$, and the ratio $f\left(x ; \frac{1}{2}\right) / f\left(x ; \frac{3}{4}\right)$.

\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
$x$ & 0 & 1 & 2 \\
\hline
$f(x ; 1 / 2)$ & $1 / 32$ & $5 / 32$ & $10 / 32$ \\
\hline
$f(x ; 3 / 4)$ & $1 / 1024$ & $15 / 1024$ & $90 / 1024$ \\
\hline
$f(x ; 1 / 2) / f(x ; 3 / 4)$ & $32 / 1$ & $32 / 3$ & $32 / 9$ \\
\hline\hline
$x$ & 3 & 4 & 5 \\
\hline
$f(x ; 1 / 2)$ & $10 / 32$ & $5 / 32$ & $1 / 32$ \\
\hline
$f(x ; 3 / 4)$ & $270 / 1024$ & $405 / 1024$ & $243 / 1024$ \\
\hline
$f(x ; 1 / 2) / f(x ; 3 / 4)$ & $32 / 27$ & $32 / 81$ & $32 / 243$ \\
\hline
\end{tabular}
\end{center}

We shall use one random value of $X$ to test the simple hypothesis $H_{0}: \theta=\frac{1}{2}$ against the alternative simple hypothesis $H_{1}: \theta=\frac{3}{4}$, and we shall first assign the significance level of the test to be $\alpha=\frac{1}{32}$. We seek a best critical region of size $\alpha=\frac{1}{32}$. If $A_{1}=\{x: x=0\}$ or $A_{2}=\{x: x=5\}$, then $P_{\{\theta=1 / 2\}}(X \in$ $\left.A_{1}\right)=P_{\{\theta=1 / 2\}}\left(X \in A_{2}\right)=\frac{1}{32}$ and there is no other subset $A_{3}$ of the space $\{x$ : $x=0,1,2,3,4,5\}$ such that $P_{\{\theta=1 / 2\}}\left(X \in A_{3}\right)=\frac{1}{32}$. Then either $A_{1}$ or $A_{2}$ is the best critical region $C$ of size $\alpha=\frac{1}{32}$ for testing $H_{0}$ against $H_{1}$. We note that $P_{\{\theta=1 / 2\}}\left(X \in A_{1}\right)=\frac{1}{32}$ and $P_{\{\theta=3 / 4\}}\left(X \in A_{1}\right)=\frac{1}{1024}$. Thus, if the set $A_{1}$ is used as a critical region of size $\alpha=\frac{1}{32}$, we have the intolerable situation that the probability of rejecting $H_{0}$ when $H_{1}$ is true ( $H_{0}$ is false) is much less than the probability of rejecting $H_{0}$ when $H_{0}$ is true.

On the other hand, if the set $A_{2}$ is used as a critical region, then $P_{\{\theta=1 / 2\}}(X \in$ $\left.A_{2}\right)=\frac{1}{32}$ and $P_{\{\theta=3 / 4\}}\left(X \in A_{2}\right)=\frac{243}{1024}$. That is, the probability of rejecting $H_{0}$ when $H_{1}$ is true is much greater than the probability of rejecting $H_{0}$ when $H_{0}$ is true. Certainly, this is a more desirable state of affairs, and actually $A_{2}$ is the best critical region of size $\alpha=\frac{1}{32}$. The latter statement follows from the fact that when $H_{0}$ is true, there are but two subsets, $A_{1}$ and $A_{2}$, of the sample space, each of whose probability measure is $\frac{1}{32}$ and the fact that

$$
\frac{243}{1024}=P_{\{\theta=3 / 4\}}\left(X \in A_{2}\right)>P_{\{\theta=3 / 4\}}\left(X \in A_{1}\right)=\frac{1}{1024} .
$$

It should be noted in this problem that the best critical region $C=A_{2}$ of size $\alpha=\frac{1}{32}$ is found by including in $C$ the point (or points) at which $f\left(x ; \frac{1}{2}\right)$ is small in comparison with $f\left(x ; \frac{3}{4}\right)$. This is seen to be true once it is observed that the ratio $f\left(x ; \frac{1}{2}\right) / f\left(x ; \frac{3}{4}\right)$ is a minimum at $x=5$. Accordingly, the ratio $f\left(x ; \frac{1}{2}\right) / f\left(x ; \frac{3}{4}\right)$, that is given in the last line of the above tabulation, provides us with a precise tool by which to find a best critical region $C$ for certain given values of $\alpha$. To illustrate this, take $\alpha=\frac{6}{32}$. When $H_{0}$ is true, each of the subsets $\{x: x=0,1\},\{x: x=0,4\}$, $\{x: x=1,5\},\{x: x=4,5\}$ has probability measure $\frac{6}{32}$. By direct computation it is found that the best critical region of this size is $\{x: x=4,5\}$. This reflects the fact that the ratio $f\left(x ; \frac{1}{2}\right) / f\left(x ; \frac{3}{4}\right)$ has its two smallest values for $x=4$ and $x=5$. The power of this test, which has $\alpha=\frac{6}{32}$, is

$$
P_{\{\theta=3 / 4\}}(X=4,5)=\frac{405}{1024}+\frac{243}{1024}=\frac{648}{1024} .
$$

The preceding example should make the following theorem, due to Neyman and Pearson, easier to understand. It is an important theorem because it provides a systematic method of determining a best critical region.

Theorem 8.1.1. Neyman-Pearson Theorem. Let $X_{1}, X_{2}, \ldots, X_{n}$, where $n$ is a fixed positive integer, denote a random sample from a distribution that has pdf or pmf $f(x ; \theta)$. Then the likelihood of $X_{1}, X_{2}, \ldots, X_{n}$ is

$$
L(\theta ; \mathbf{x})=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right), \quad \text { for } \mathbf{x}^{\prime}=\left(x_{1}, \ldots, x_{n}\right)
$$

Let $\theta^{\prime}$ and $\theta^{\prime \prime}$ be distinct fixed values of $\theta$ so that $\Omega=\left\{\theta: \theta=\theta^{\prime}, \theta^{\prime \prime}\right\}$, and let $k$ be a positive number. Let $C$ be a subset of the sample space such that\\
(a) $\frac{L\left(\theta^{\prime} ; \mathbf{x}\right)}{L\left(\theta^{\prime \prime} ; \mathbf{x}\right)} \leq k$, for each point $\mathbf{x} \in C$.\\
(b) $\frac{L\left(\theta^{\prime} ; \mathbf{x}\right)}{L\left(\theta^{\prime \prime} ; \mathbf{x}\right)} \geq k$, for each point $\mathbf{x} \in C^{c}$.\\
(c) $\alpha=P_{H_{0}}[\mathbf{X} \in C]$.

Then $C$ is a best critical region of size $\alpha$ for testing the simple hypothesis $H_{0}: \theta=\theta^{\prime}$ against the alternative simple hypothesis $H_{1}: \theta=\theta^{\prime \prime}$.

Proof: We shall give the proof when the random variables are of the continuous type. If $C$ is the only critical region of size $\alpha$, the theorem is proved. If there is another critical region of size $\alpha$, denote it by $A$. For convenience, we shall let $\int \underset{R}{\ldots} \int L\left(\theta ; x_{1}, \ldots, x_{n}\right) d x_{1} \cdots d x_{n}$ be denoted by $\int_{R} L(\theta)$. In this notation we wish to show that

$$
\int_{C} L\left(\theta^{\prime \prime}\right)-\int_{A} L\left(\theta^{\prime \prime}\right) \geq 0
$$

Since $C$ is the union of the disjoint sets $C \cap A$ and $C \cap A^{c}$ and $A$ is the union of the disjoint sets $A \cap C$ and $A \cap C^{c}$, we have


\begin{align*}
\int_{C} L\left(\theta^{\prime \prime}\right)-\int_{A} L\left(\theta^{\prime \prime}\right) & =\int_{C \cap A} L\left(\theta^{\prime \prime}\right)+\int_{C \cap A^{c}} L\left(\theta^{\prime \prime}\right)-\int_{A \cap C} L\left(\theta^{\prime \prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime \prime}\right) \\
& =\int_{C \cap A^{c}} L\left(\theta^{\prime \prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime \prime}\right) . \tag{8.1.5}
\end{align*}


However, by the hypothesis of the theorem, $L\left(\theta^{\prime \prime}\right) \geq(1 / k) L\left(\theta^{\prime}\right)$ at each point of $C$, and hence at each point of $C \cap A^{c}$; thus,

$$
\int_{C \cap A^{c}} L\left(\theta^{\prime \prime}\right) \geq \frac{1}{k} \int_{C \cap A^{c}} L\left(\theta^{\prime}\right) .
$$

But $L\left(\theta^{\prime \prime}\right) \leq(1 / k) L\left(\theta^{\prime}\right)$ at each point of $C^{c}$, and hence at each point of $A \cap C^{c}$; accordingly,

$$
\int_{A \cap C^{c}} L\left(\theta^{\prime \prime}\right) \leq \frac{1}{k} \int_{A \cap C^{c}} L\left(\theta^{\prime}\right) .
$$

These inequalities imply that

$$
\int_{C \cap A^{c}} L\left(\theta^{\prime \prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime \prime}\right) \geq \frac{1}{k} \int_{C \cap A^{c}} L\left(\theta^{\prime}\right)-\frac{1}{k} \int_{A \cap C^{c}} L\left(\theta^{\prime}\right) ;
$$

and, from Equation (8.1.5), we obtain


\begin{equation*}
\int_{C} L\left(\theta^{\prime \prime}\right)-\int_{A} L\left(\theta^{\prime \prime}\right) \geq \frac{1}{k}\left[\int_{C \cap A^{c}} L\left(\theta^{\prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime}\right)\right] \tag{8.1.6}
\end{equation*}


However,

$$
\begin{aligned}
\int_{C \cap A^{c}} L\left(\theta^{\prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime}\right)= & \int_{C \cap A^{c}} L\left(\theta^{\prime}\right)+\int_{C \cap A} L\left(\theta^{\prime}\right) \\
& -\int_{A \cap C} L\left(\theta^{\prime}\right)-\int_{A \cap C^{c}} L\left(\theta^{\prime}\right) \\
= & \int_{C} L\left(\theta^{\prime}\right)-\int_{A} L\left(\theta^{\prime}\right)=\alpha-\alpha=0
\end{aligned}
$$

If this result is substituted in inequality (8.1.6), we obtain the desired result,

$$
\int_{C} L\left(\theta^{\prime \prime}\right)-\int_{A} L\left(\theta^{\prime \prime}\right) \geq 0
$$

If the random variables are of the discrete type, the proof is the same with integration replaced by summation.

Remark 8.1.1. As stated in the theorem, conditions (a), (b), and (c) are sufficient ones for region $C$ to be a best critical region of size $\alpha$. However, they are also necessary. We discuss this briefly. Suppose there is a region $A$ of size $\alpha$ that does not satisfy (a) and (b) and that is as powerful at $\theta=\theta^{\prime \prime}$ as $C$, which satisfies (a), (b), and (c). Then expression (8.1.5) would be zero, since the power at $\theta^{\prime \prime}$ using $A$ is equal to that using $C$. It can be proved that to have expression (8.1.5) equal zero, $A$ must be of the same form as $C$. As a matter of fact, in the continuous case, $A$ and $C$ would essentially be the same region; that is, they could differ only by a set having probability zero. However, in the discrete case, if $P_{H_{0}}\left[L\left(\theta^{\prime}\right)=k L\left(\theta^{\prime \prime}\right)\right]$ is positive, $A$ and $C$ could be different sets, but each would necessarily enjoy conditions (a), (b), and (c) to be a best critical region of size $\alpha$.

It would seem that a test should have the property that its power should never fall below its significance level; otherwise, the probability of falsely rejecting $H_{0}$ (level) is higher than the probability of correctly rejecting $H_{0}$ (power). We say a test having this property is unbiased, which we now formally define:

Definition 8.1.2. Let $X$ be a random variable which has pdf or pmf $f(x ; \theta)$, where $\theta \in \Omega$. Consider the hypotheses given in expression (8.1.1). Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ denote a random sample on $X$. Consider a test with critical region $C$ and level $\alpha$. We say that this test is unbiased if

$$
P_{\theta}(\mathbf{X} \in C) \geq \alpha
$$

for all $\theta \in \omega_{1}$.

As the next corollary shows, the best test given in Theorem 8.1.1 is an unbiased test.

Corollary 8.1.1. As in Theorem 8.1.1, let $C$ be the critical region of the best test of $H_{0}: \theta=\theta^{\prime}$ versus $H_{1}: \theta=\theta^{\prime \prime}$. Suppose the significance level of the test is $\alpha$. Let $\gamma_{C}\left(\theta^{\prime \prime}\right)=P_{\theta^{\prime \prime}}[\mathbf{X} \in C]$ denote the power of the test. Then $\alpha \leq \gamma_{C}\left(\theta^{\prime \prime}\right)$.

Proof: Consider the "unreasonable" test in which the data are ignored, but a Bernoulli trial is performed which has probability $\alpha$ of success. If the trial ends in success, we reject $H_{0}$. The level of this test is $\alpha$. Because the power of a test is the probability of rejecting $H_{0}$ when $H_{1}$ is true, the power of this unreasonable test is $\alpha$ also. But $C$ is the best critical region of size $\alpha$ and thus has power greater than or equal to the power of the unreasonable test. That is, $\gamma_{C}\left(\theta^{\prime \prime}\right) \geq \alpha$, which is the desired result.

Another aspect of Theorem 8.1.1 to be emphasized is that if we take $C$ to be the set of all points x which satisfy

$$
\frac{L\left(\theta^{\prime} ; \mathbf{x}\right)}{L\left(\theta^{\prime \prime} ; \mathbf{x}\right)} \leq k, \quad k>0,
$$

then, in accordance with the theorem, $C$ is a best critical region. This inequality can frequently be expressed in one of the forms (where $c_{1}$ and $c_{2}$ are constants)

$$
u_{1}\left(\mathbf{x} ; \theta^{\prime}, \theta^{\prime \prime}\right) \leq c_{1}
$$

or

$$
u_{2}\left(\mathbf{x} ; \theta^{\prime}, \theta^{\prime \prime}\right) \geq c_{2} .
$$

Suppose that it is the first form, $u_{1} \leq c_{1}$. Since $\theta^{\prime}$ and $\theta^{\prime \prime}$ are given constants, $u_{1}\left(\mathbf{X} ; \theta^{\prime}, \theta^{\prime \prime}\right)$ is a statistic; and if the pdf or pmf of this statistic can be found when $H_{0}$ is true, then the significance level of the test of $H_{0}$ against $H_{1}$ can be determined from this distribution. That is,

$$
\alpha=P_{H_{0}}\left[u_{1}\left(\mathbf{X} ; \theta^{\prime}, \theta^{\prime \prime}\right) \leq c_{1}\right] .
$$

Moreover, the test may be based on this statistic; for if the observed vector value of $\mathbf{X}$ is $\mathbf{x}$, we reject $H_{0}\left(\operatorname{accept} H_{1}\right)$ if $u_{1}(\mathbf{x}) \leq c_{1}$.

A positive number $k$ determines a best critical region $C$ whose size is $\alpha=$ $P_{H_{0}}[\mathbf{X} \in C]$ for that particular $k$. It may be that this value of $\alpha$ is unsuitable for the purpose at hand; that is, it is too large or too small. However, if there is a statistic $u_{1}(\mathbf{X})$ as in the preceding paragraph, whose pdf or pmf can be determined when $H_{0}$ is true, we need not experiment with various values of $k$ to obtain a desirable significance level. For if the distribution of the statistic is known, or can be found, we may determine $c_{1}$ such that $P_{H_{0}}\left[u_{1}(\mathbf{X}) \leq c_{1}\right]$ is a desirable significance level.

An illustrative example follows.

Example 8.1.2. Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ denote a random sample from the distribution that has the pdf

$$
f(x ; \theta)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{(x-\theta)^{2}}{2}\right), \quad-\infty<x<\infty
$$

It is desired to test the simple hypothesis $H_{0}: \theta=\theta^{\prime}=0$ against the alternative simple hypothesis $H_{1}: \theta=\theta^{\prime \prime}=1$. Now

$$
\begin{aligned}
\frac{L\left(\theta^{\prime} ; \mathbf{x}\right)}{L\left(\theta^{\prime \prime} ; \mathbf{x}\right)} & =\frac{(1 / \sqrt{2 \pi})^{n} \exp \left[-\sum_{1}^{n} x_{i}^{2} / 2\right]}{(1 / \sqrt{2 \pi})^{n} \exp \left[-\sum_{1}^{n}\left(x_{i}-1\right)^{2} / 2\right]} \\
& =\exp \left(-\sum_{1}^{n} x_{i}+\frac{n}{2}\right)
\end{aligned}
$$

If $k>0$, the set of all points $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ such that

$$
\exp \left(-\sum_{1}^{n} x_{i}+\frac{n}{2}\right) \leq k
$$

is a best critical region. This inequality holds if and only if

$$
-\sum_{1}^{n} x_{i}+\frac{n}{2} \leq \log k
$$

or, equivalently,

$$
\sum_{1}^{n} x_{i} \geq \frac{n}{2}-\log k=c
$$

In this case, a best critical region is the set $C=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right): \sum_{1}^{n} x_{i} \geq c\right\}$, where $c$ is a constant that can be determined so that the size of the critical region is a desired number $\alpha$. The event $\sum_{1}^{n} X_{i} \geq c$ is equivalent to the event $\bar{X} \geq$ $c / n=c_{1}$, for example, so the test may be based upon the statistic $\bar{X}$. If $H_{0}$ is true, that is, $\theta=\theta^{\prime}=0$, then $\bar{X}$ has a distribution that is $N(0,1 / n)$. Given the significance level $\alpha$, the number $c_{1}$ is computed in R as $c_{1}=\mathbf{q n o r m}(1-\alpha, 0,1 / \sqrt{n})$; hence, $P_{H_{0}}\left(\bar{X} \geq c_{1}\right)=\alpha$. So, if the experimental values of $X_{1}, X_{2}, \ldots, X_{n}$ were, respectively, $x_{1}, x_{2}, \ldots, x_{n}$, we would compute $\bar{x}=\sum_{1}^{n} x_{i} / n$. If $\bar{x} \geq c_{1}$, the simple hypothesis $H_{0}: \theta=\theta^{\prime}=0$ would be rejected at the significance level $\alpha$; if $\bar{x}<c_{1}$, the hypothesis $H_{0}$ would be accepted. The probability of rejecting $H_{0}$ when $H_{0}$ is true is $\alpha$ the level of significance. The probability of rejecting $H_{0}$, when $H_{0}$ is false, is the value of the power of the test at $\theta=\theta^{\prime \prime}=1$, which is,


\begin{equation*}
P_{H_{1}}\left(\bar{X} \geq c_{1}\right)=\int_{c_{1}}^{\infty} \frac{1}{\sqrt{2 \pi} \sqrt{1 / n}} \exp \left[-\frac{(\bar{x}-1)^{2}}{2(1 / n)}\right] d \bar{x} \tag{8.1.7}
\end{equation*}


For example, if $n=25$ and $\alpha$ is $0.05, c_{1}=$ qnorm $(0.95,0,1 / 5)=0.329$, using R. Hence, the power of the test to detect $\theta=1$, given in expression (8.1.7), is computed by $1-\operatorname{pnorm}(0.329,1,1 / 5)=0.9996$.

There is another aspect of this theorem that warrants special mention. It has to do with the number of parameters that appear in the pdf. Our notation suggests that there is but one parameter. However, a careful review of the proof reveals that nowhere was this needed or assumed. The pdf or pmf may depend upon any finite number of parameters. What is essential is that the hypothesis $H_{0}$ and the alternative hypothesis $H_{1}$ be simple, namely, that they completely specify the distributions. With this in mind, we see that the simple hypotheses $H_{0}$ and $H_{1}$ do not need to be hypotheses about the parameters of a distribution, nor, as a matter of fact, do the random variables $X_{1}, X_{2}, \ldots, X_{n}$ need to be independent. That is, if $H_{0}$ is the simple hypothesis that the joint pdf or pmf is $g\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, and if $H_{1}$ is the alternative simple hypothesis that the joint pdf or pmf is $h\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, then $C$ is a best critical region of size $\alpha$ for testing $H_{0}$ against $H_{1}$ if, for $k>0$,

\begin{enumerate}
  \item $\frac{g\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{h\left(x_{1}, x_{2}, \ldots, x_{n}\right)} \leq k$ for $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C$.
  \item $\frac{g\left(x_{1}, x_{2}, \ldots, x_{n}\right)}{h\left(x_{1}, x_{2}, \ldots, x_{n}\right)} \geq k$ for $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C^{c}$.
  \item $\alpha=P_{H_{0}}\left[\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in C\right]$.
\end{enumerate}

Consider the following example.\\
Example 8.1.3. Let $X_{1}, \ldots, X_{n}$ denote a random sample on $X$ that has pmf $f(x)$ with support $\{0,1,2, \ldots\}$. It is desired to test the simple hypothesis

$$
H_{0}: f(x)= \begin{cases}\frac{e^{-1}}{x!} & x=0,1,2, \ldots \\ 0 & \text { elsewhere }\end{cases}
$$

against the alternative simple hypothesis

$$
H_{1}: f(x)= \begin{cases}\left(\frac{1}{2}\right)^{x+1} & x=0,1,2, \ldots \\ 0 & \text { elsewhere }\end{cases}
$$

That is, we want to test whether $X$ has a Poisson distribution with mean $\lambda=1$ versus $X$ has a geometric distribution with $p=1 / 2$. Here

$$
\begin{aligned}
\frac{g\left(x_{1}, \ldots, x_{n}\right)}{h\left(x_{1}, \ldots, x_{n}\right)} & =\frac{e^{-n} /\left(x_{1}!x_{2}!\cdots x_{n}!\right)}{\left(\frac{1}{2}\right)^{n}\left(\frac{1}{2}\right)^{x_{1}+x_{2}+\cdots+x_{n}}} \\
& =\frac{\left(2 e^{-1}\right)^{n} 2^{\sum x_{i}}}{\prod_{1}^{n}\left(x_{i}!\right)}
\end{aligned}
$$

If $k>0$, the set of points $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ such that

$$
\left(\sum_{1}^{n} x_{i}\right) \log 2-\log \left[\prod_{1}^{n}\left(x_{i}!\right)\right] \leq \log k-n \log \left(2 e^{-1}\right)=c
$$

is a best critical region $C$. Consider the case of $k=1$ and $n=1$. The preceding inequality may be written $2^{x_{1}} / x_{1}!\leq e / 2$. This inequality is satisfied by all points in the set $C=\left\{x_{1}: x_{1}=0,3,4,5, \ldots\right\}$. Using R , the level of significance is

$$
P_{H_{0}}\left(X_{1} \in C\right)=1-P_{H_{0}}\left(X_{1}=1,2\right)=1-\text { dpois }(1,1)-\operatorname{dpois}(2,1)=0.4482 .
$$

The power of the test to detect $H_{1}$ is computed as

$$
P_{H_{1}}\left(X_{1} \in C\right)=1-P_{H_{1}}\left(X_{1}=1,2\right)=1-\left(\frac{1}{4}+\frac{1}{8}\right)=0.625 .
$$

Note that these results are consistent with Corollary 8.1.1.

Remark 8.1.2. In the notation of this section, say $C$ is a critical region such that

$$
\alpha=\int_{C} L\left(\theta^{\prime}\right) \quad \text { and } \quad \beta=\int_{C^{c}} L\left(\theta^{\prime \prime}\right)
$$

where $\alpha$ and $\beta$ equal the respective probabilities of the Type I and Type II errors associated with $C$. Let $d_{1}$ and $d_{2}$ be two given positive constants. Consider a certain linear function of $\alpha$ and $\beta$, namely,

$$
\begin{aligned}
d_{1} \int_{C} L\left(\theta^{\prime}\right)+d_{2} \int_{C^{c}} L\left(\theta^{\prime \prime}\right) & =d_{1} \int_{C} L\left(\theta^{\prime}\right)+d_{2}\left[1-\int_{C} L\left(\theta^{\prime \prime}\right)\right] \\
& =d_{2}+\int_{C}\left[d_{1} L\left(\theta^{\prime}\right)-d_{2} L\left(\theta^{\prime \prime}\right)\right] .
\end{aligned}
$$

If we wished to minimize this expression, we would select $C$ to be the set of all $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ such that

$$
d_{1} L\left(\theta^{\prime}\right)-d_{2} L\left(\theta^{\prime \prime}\right)<0
$$

or, equivalently,

$$
\frac{L\left(\theta^{\prime}\right)}{L\left(\theta^{\prime \prime}\right)}<\frac{d_{2}}{d_{1}}, \quad \text { for all }\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C
$$

which according to the Neyman-Pearson theorem provides a best critical region with $k=d_{2} / d_{1}$. That is, this critical region $C$ is one that minimizes $d_{1} \alpha+d_{2} \beta$. There could be others, including points on which $L\left(\theta^{\prime}\right) / L\left(\theta^{\prime \prime}\right)=d_{2} / d_{1}$, but these would still be best critical regions according to the Neyman-Pearson theorem.

\section*{EXERCISES}
8.1.1. In Example 8.1.2 of this section, let the simple hypotheses read $H_{0}: \theta=$ $\theta^{\prime}=0$ and $H_{1}: \theta=\theta^{\prime \prime}=-1$. Show that the best test of $H_{0}$ against $H_{1}$ may be carried out by use of the statistic $\bar{X}$, and that if $n=25$ and $\alpha=0.05$, the power of the test is 0.9996 when $H_{1}$ is true.\\
8.1.2. Let the random variable $X$ have the pdf $f(x ; \theta)=(1 / \theta) e^{-x / \theta}, 0<x<\infty$, zero elsewhere. Consider the simple hypothesis $H_{0}: \theta=\theta^{\prime}=2$ and the alternative hypothesis $H_{1}: \theta=\theta^{\prime \prime}=4$. Let $X_{1}, X_{2}$ denote a random sample of size 2 from this distribution. Show that the best test of $H_{0}$ against $H_{1}$ may be carried out by use of the statistic $X_{1}+X_{2}$.\\
8.1.3. Repeat Exercise 8.1 .2 when $H_{1}: \theta=\theta^{\prime \prime}=6$. Generalize this for every $\theta^{\prime \prime}>2$.\\
8.1.4. Let $X_{1}, X_{2}, \ldots, X_{10}$ be a random sample of size 10 from a normal distribution $N\left(0, \sigma^{2}\right)$. Find a best critical region of size $\alpha=0.05$ for testing $H_{0}: \sigma^{2}=1$ against $H_{1}: \sigma^{2}=2$. Is this a best critical region of size 0.05 for testing $H_{0}: \sigma^{2}=1$ against $H_{1}: \sigma^{2}=4$ ? Against $H_{1}: \sigma^{2}=\sigma_{1}^{2}>1$ ?\\
8.1.5. If $X_{1}, X_{2}, \ldots, X_{n}$ is a random sample from a distribution having pdf of the form $f(x ; \theta)=\theta x^{\theta-1}, 0<x<1$, zero elsewhere, show that a best critical region for testing $H_{0}: \theta=1$ against $H_{1}: \theta=2$ is $C=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right): c \leq \prod_{i=1}^{n} x_{i}\right\}$.\\
8.1.6. Let $X_{1}, X_{2}, \ldots, X_{10}$ be a random sample from a distribution that is $N\left(\theta_{1}, \theta_{2}\right)$. Find a best test of the simple hypothesis $H_{0}: \theta_{1}=\theta_{1}^{\prime}=0, \theta_{2}=\theta_{2}^{\prime}=1$ against the alternative simple hypothesis $H_{1}: \theta_{1}=\theta_{1}^{\prime \prime}=1, \theta_{2}=\theta_{2}^{\prime \prime}=4$.\\
8.1.7. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a normal distribution $N(\theta, 100)$. Show that $C=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right): c \leq \bar{x}=\sum_{1}^{n} x_{i} / n\right\}$ is a best critical region for testing $H_{0}: \theta=75$ against $H_{1}: \theta=78$. Find $n$ and $c$ so that

$$
P_{H_{0}}\left[\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in C\right]=P_{H_{0}}(\bar{X} \geq c)=0.05
$$

and

$$
P_{H_{1}}\left[\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in C\right]=P_{H_{1}}(\bar{X} \geq c)=0.90
$$

approximately.\\
8.1.8. If $X_{1}, X_{2}, \ldots, X_{n}$ is a random sample from a beta distribution with parameters $\alpha=\beta=\theta>0$, find a best critical region for testing $H_{0}: \theta=1$ against $H_{1}: \theta=2$.\\
8.1.9. Let $X_{1}, X_{2}, \ldots, X_{n}$ be iid with $\operatorname{pmf} f(x ; p)=p^{x}(1-p)^{1-x}, x=0,1$, zero elsewhere. Show that $C=\left\{\left(x_{1}, \ldots, x_{n}\right): \sum_{1}^{n} x_{i} \leq c\right\}$ is a best critical region for testing $H_{0}: p=\frac{1}{2}$ against $H_{1}: p=\frac{1}{3}$. Use the Central Limit Theorem to find $n$ and $c$ so that approximately $P_{H_{0}}\left(\sum_{1}^{n} X_{i} \leq c\right)=0.10$ and $P_{H_{1}}\left(\sum_{1}^{n} X_{i} \leq c\right)=0.80$.\\
8.1.10. Let $X_{1}, X_{2}, \ldots, X_{10}$ denote a random sample of size 10 from a Poisson distribution with mean $\theta$. Show that the critical region $C$ defined by $\sum_{1}^{10} x_{i} \geq 3$ is a best critical region for testing $H_{0}: \theta=0.1$ against $H_{1}: \theta=0.5$. Determine, for this test, the significance level $\alpha$ and the power at $\theta=0.5$. Use the R function ppois.

\subsection*{8.2 Uniformly Most Powerful Tests}
This section takes up the problem of a test of a simple hypothesis $H_{0}$ against an alternative composite hypothesis $H_{1}$. We begin with an example.\\
Example 8.2.1. Consider the pdf

$$
f(x ; \theta)= \begin{cases}\frac{1}{\theta} e^{-x / \theta} & 0<x<\infty \\ 0 & \text { elsewhere }\end{cases}
$$

of Exercises 8.1.2 and 8.1.3. It is desired to test the simple hypothesis $H_{0}: \theta=2$ against the alternative composite hypothesis $H_{1}: \theta>2$. Thus $\Omega=\{\theta: \theta \geq 2\}$. A random sample, $X_{1}, X_{2}$, of size $n=2$ is used, and the critical region is $C=$ $\left\{\left(x_{1}, x_{2}\right): 9.5 \leq x_{1}+x_{2}<\infty\right\}$. It was shown in the exercises cited that the significance level of the test is approximately 0.05 and the power of the test when $\theta=4$ is approximately 0.31 . The power function $\gamma(\theta)$ of the test for all $\theta \geq 2$ is

$$
\begin{aligned}
\gamma(\theta) & =1-\int_{0}^{9.5} \int_{0}^{9.5-x_{2}} \frac{1}{\theta^{2}} \exp \left(-\frac{x_{1}+x_{2}}{\theta}\right) d x_{1} d x_{2} \\
& =\left(\frac{\theta+9.5}{\theta}\right) e^{-9.5 / \theta}, \quad 2 \leq \theta
\end{aligned}
$$

For example, $\gamma(2)=0.05, \gamma(4)=0.31$, and $\gamma(9.5)=2 / e \approx 0.74$. It is shown (Exercise 8.1.3) that the set $C=\left\{\left(x_{1}, x_{2}\right): 9.5 \leq x_{1}+x_{2}<\infty\right\}$ is a best critical region of size 0.05 for testing the simple hypothesis $H_{0}: \theta=2$ against each simple hypothesis in the composite hypothesis $H_{1}: \theta>2$.

The preceding example affords an illustration of a test of a simple hypothesis $H_{0}$ that is a best test of $H_{0}$ against every simple hypothesis in the alternative composite hypothesis $H_{1}$. We now define a critical region, when it exists, which is a best critical region for testing a simple hypothesis $H_{0}$ against an alternative composite hypothesis $H_{1}$. It seems desirable that this critical region should be a best critical region for testing $H_{0}$ against each simple hypothesis in $H_{1}$. That is, the power function of the test that corresponds to this critical region should be at least as great as the power function of any other test with the same significance level for every simple hypothesis in $H_{1}$.\\
Definition 8.2.1. The critical region $C$ is a uniformly most powerful (UMP) critical region of size $\alpha$ for testing the simple hypothesis $H_{0}$ against an alternative composite hypothesis $H_{1}$ if the set $C$ is a best critical region of size $\alpha$ for testing $H_{0}$ against each simple hypothesis in $H_{1}$. A test defined by this critical region $C$ is called a uniformly most powerful (UMP) test, with significance level $\alpha$, for testing the simple hypothesis $H_{0}$ against the alternative composite hypothesis $H_{1}$.

As will be seen presently, uniformly most powerful tests do not always exist. However, when they do exist, the Neyman-Pearson theorem provides a technique for finding them. Some illustrative examples are given here.

Example 8.2.2. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a distribution that is $N(0, \theta)$, where the variance $\theta$ is an unknown positive number. It will be shown that there exists a uniformly most powerful test with significance level $\alpha$ for testing the simple hypothesis $H_{0}: \theta=\theta^{\prime}$, where $\theta^{\prime}$ is a fixed positive number, against the alternative composite hypothesis $H_{1}: \theta>\theta^{\prime}$. Thus $\Omega=\left\{\theta: \theta \geq \theta^{\prime}\right\}$. The joint pdf of $X_{1}, X_{2}, \ldots, X_{n}$ is

$$
L\left(\theta ; x_{1}, x_{2}, \ldots, x_{n}\right)=\left(\frac{1}{2 \pi \theta}\right)^{n / 2} \exp \left\{-\frac{1}{2 \theta} \sum_{i=1}^{n} x_{i}^{2}\right\} .
$$

Let $\theta^{\prime \prime}$ represent a number greater than $\theta^{\prime}$, and let $k$ denote a positive number. Let $C$ be the set of points where

$$
\frac{L\left(\theta^{\prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)}{L\left(\theta^{\prime \prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)} \leq k
$$

that is, the set of points where

$$
\left(\frac{\theta^{\prime \prime}}{\theta^{\prime}}\right)^{n / 2} \exp \left[-\left(\frac{\theta^{\prime \prime}-\theta^{\prime}}{2 \theta^{\prime} \theta^{\prime \prime}}\right) \sum_{1}^{n} x_{i}^{2}\right] \leq k
$$

or, equivalently,

$$
\sum_{1}^{n} x_{i}^{2} \geq \frac{2 \theta^{\prime} \theta^{\prime \prime}}{\theta^{\prime \prime}-\theta^{\prime}}\left[\frac{n}{2} \log \left(\frac{\theta^{\prime \prime}}{\theta^{\prime}}\right)-\log k\right]=c .
$$

The set $C=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right): \sum_{1}^{n} x_{i}^{2} \geq c\right\}$ is then a best critical region for testing the simple hypothesis $H_{0}: \theta=\theta^{\prime}$ against the simple hypothesis $\theta=\theta^{\prime \prime}$. It remains to determine $c$, so that this critical region has the desired size $\alpha$. If $H_{0}$ is true, the random variable $\sum_{1}^{n} X_{i}^{2} / \theta^{\prime}$ has a chi-square distribution with $n$ degrees of freedom. Since $\alpha=P_{\theta^{\prime}}\left(\sum_{1}^{n} X_{i}^{2} / \theta^{\prime} \geq c / \theta^{\prime}\right), c / \theta^{\prime}$ may be computed, for example, by the R code qchisq $(1-\alpha, n)$. Then $C=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right): \sum_{1}^{n} x_{i}^{2} \geq c\right\}$ is a best critical region of size $\alpha$ for testing $H_{0}: \theta=\theta^{\prime}$ against the hypothesis $\theta=\theta^{\prime \prime}$. Moreover, for each number $\theta^{\prime \prime}$ greater than $\theta^{\prime}$, the foregoing argument holds. That is, $C=\left\{\left(x_{1}, \ldots, x_{n}\right): \sum_{1}^{n} x_{i}^{2} \geq c\right\}$ is a uniformly most powerful critical region of size $\alpha$ for testing $H_{0}: \theta=\theta^{\prime}$ against $H_{1}: \theta>\theta^{\prime}$. If $x_{1}, x_{2}, \ldots, x_{n}$ denote the experimental values of $X_{1}, X_{2}, \ldots, X_{n}$, then $H_{0}: \theta=\theta^{\prime}$ is rejected at the significance level $\alpha$, and $H_{1}: \theta>\theta^{\prime}$ is accepted if $\sum_{1}^{n} x_{i}^{2} \geq c$; otherwise, $H_{0}: \theta=\theta^{\prime}$ is accepted.

If, in the preceding discussion, we take $n=15, \alpha=0.05$, and $\theta^{\prime}=3$, then the two hypotheses are $H_{0}: \theta=3$ and $H_{1}: \theta>3$. Using $\mathrm{R}, c / 3$ is computed by qchisq(0.95,15) $=24.996$. Hence, $c=74.988$.

Example 8.2.3. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a distribution that is $N(\theta, 1)$, where $\theta$ is unknown. It will be shown that there is no uniformly most powerful test of the simple hypothesis $H_{0}: \theta=\theta^{\prime}$, where $\theta^{\prime}$ is a fixed number against the alternative composite hypothesis $H_{1}: \theta \neq \theta^{\prime}$. Thus $\Omega=\{\theta:-\infty<\theta<\infty\}$. Let $\theta^{\prime \prime}$ be a number not equal to $\theta^{\prime}$. Let $k$ be a positive number and consider

$$
\frac{(1 / 2 \pi)^{n / 2} \exp \left[-\sum_{1}^{n}\left(x_{i}-\theta^{\prime}\right)^{2} / 2\right]}{(1 / 2 \pi)^{n / 2} \exp \left[-\sum_{1}^{n}\left(x_{i}-\theta^{\prime \prime}\right)^{2} / 2\right]} \leq k
$$

The preceding inequality may be written as

$$
\exp \left\{-\left(\theta^{\prime \prime}-\theta^{\prime}\right) \sum_{1}^{n} x_{i}+\frac{n}{2}\left[\left(\theta^{\prime \prime}\right)^{2}-\left(\theta^{\prime}\right)^{2}\right]\right\} \leq k
$$

or

$$
\left(\theta^{\prime \prime}-\theta^{\prime}\right) \sum_{1}^{n} x_{i} \geq \frac{n}{2}\left[\left(\theta^{\prime \prime}\right)^{2}-\left(\theta^{\prime}\right)^{2}\right]-\log k
$$

This last inequality is equivalent to

$$
\sum_{1}^{n} x_{i} \geq \frac{n}{2}\left(\theta^{\prime \prime}+\theta^{\prime}\right)-\frac{\log k}{\theta^{\prime \prime}-\theta^{\prime}}
$$

provided that $\theta^{\prime \prime}>\theta^{\prime}$, and it is equivalent to

$$
\sum_{1}^{n} x_{i} \leq \frac{n}{2}\left(\theta^{\prime \prime}+\theta^{\prime}\right)-\frac{\log k}{\theta^{\prime \prime}-\theta^{\prime}}
$$

if $\theta^{\prime \prime}<\theta^{\prime}$. The first of these two expressions defines a best critical region for testing $H_{0}: \theta=\theta^{\prime}$ against the hypothesis $\theta=\theta^{\prime \prime}$ provided that $\theta^{\prime \prime}>\theta^{\prime}$, while the second expression defines a best critical region for testing $H_{0}: \theta=\theta^{\prime}$ against the hypothesis $\theta=\theta^{\prime \prime}$ provided that $\theta^{\prime \prime}<\theta^{\prime}$. That is, a best critical region for testing the simple hypothesis against an alternative simple hypothesis, say $\theta=\theta^{\prime}+1$, does not serve as a best critical region for testing $H_{0}: \theta=\theta^{\prime}$ against the alternative simple hypothesis $\theta=\theta^{\prime}-1$. By definition, then, there is no uniformly most powerful test in the case under consideration.

It should be noted that had the alternative composite hypothesis been one-sided, either $H_{1}: \theta>\theta^{\prime}$ or $H_{1}: \theta<\theta^{\prime}$, a uniformly most powerful test would exist in each instance.

Example 8.2.4. In Exercise 8.1.10, the reader was asked to show that if a random sample of size $n=10$ is taken from a Poisson distribution with mean $\theta$, the critical region defined by $\sum_{1}^{n} x_{i} \geq 3$ is a best critical region for testing $H_{0}: \theta=0.1$ against\\
$H_{1}: \theta=0.5$. This critical region is also a uniformly most powerful one for testing $H_{0}: \theta=0.1$ against $H_{1}: \theta>0.1$ because, with $\theta^{\prime \prime}>0.1$,

$$
\frac{(0.1)^{\sum x_{i}} e^{-10(0.1)} /\left(x_{1}!x_{2}!\cdots x_{n}!\right)}{\left(\theta^{\prime \prime}\right)^{\sum x_{i}} e^{-10\left(\theta^{\prime \prime}\right)} /\left(x_{1}!x_{2}!\cdots x_{n}!\right)} \leq k
$$

is equivalent to

$$
\left(\frac{0.1}{\theta^{\prime \prime}}\right)^{\sum x_{i}} e^{-10\left(0.1-\theta^{\prime \prime}\right)} \leq k
$$

The preceding inequality may be written as

$$
\left(\sum_{1}^{n} x_{i}\right)\left(\log 0.1-\log \theta^{\prime \prime}\right) \leq \log k+10\left(1-\theta^{\prime \prime}\right)
$$

or, since $\theta^{\prime \prime}>0.1$, equivalently as

$$
\sum_{1}^{n} x_{i} \geq \frac{\log k+10-10 \theta^{\prime \prime}}{\log 0.1-\log \theta^{\prime \prime}}
$$

Of course, $\sum_{1}^{n} x_{i} \geq 3$ is of the latter form.\\
Let us make an important observation, although obvious when pointed out. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a distribution that has pdf $f(x ; \theta), \theta \in$ $\Omega$. Suppose that $Y=u\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ is a sufficient statistic for $\theta$. In accordance with the factorization theorem, the joint pdf of $X_{1}, X_{2}, \ldots, X_{n}$ may be written

$$
L\left(\theta ; x_{1}, x_{2}, \ldots, x_{n}\right)=k_{1}\left[u\left(x_{1}, x_{2}, \ldots, x_{n}\right) ; \theta\right] k_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right),
$$

where $k_{2}\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ does not depend upon $\theta$. Consequently, the ratio

$$
\frac{L\left(\theta^{\prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)}{L\left(\theta^{\prime \prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)}=\frac{k_{1}\left[u\left(x_{1}, x_{2}, \ldots, x_{n}\right) ; \theta^{\prime}\right]}{k_{1}\left[u\left(x_{1}, x_{2}, \ldots, x_{n}\right) ; \theta^{\prime \prime}\right]}
$$

depends upon $x_{1}, x_{2}, \ldots, x_{n}$ only through $u\left(x_{1}, x_{2}, \ldots, x_{n}\right)$. Accordingly, if there is a sufficient statistic $Y=u\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ for $\theta$ and if a best test or a uniformly most powerful test is desired, there is no need to consider tests that are based upon any statistic other than the sufficient statistic. This result supports the importance of sufficiency.

In the above examples, we have presented uniformly most powerful tests. For some families of pdfs and hypotheses, we can obtain general forms of such tests. We sketch these results for the general one-sided hypotheses of the form


\begin{equation*}
H_{0}: \theta \leq \theta^{\prime} \text { versus } H_{1}: \theta>\theta^{\prime} . \tag{8.2.1}
\end{equation*}


The other one-sided hypotheses with the null hypothesis $H_{0}: \theta \geq \theta^{\prime}$, is completely analogous. Note that the null hypothesis of (8.2.1) is a composite hypothesis. Recall from Chapter 4 that the level of a test for the hypotheses (8.2.1) is defined by\\
$\max _{\theta \leq \theta^{\prime}} \gamma(\theta)$, where $\gamma(\theta)$ is the power function of the test. That is, the significance level is the maximum probability of Type I error.

Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ be a random sample with common pdf (or pmf) $f(x ; \theta)$, $\theta \in \Omega$, and, hence with the likelihood function

$$
L(\theta, \mathbf{x})=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right), \quad \mathbf{x}^{\prime}=\left(x_{1}, \ldots, x_{n}\right)
$$

We consider the family of pdfs that has monotone likelihood ratio as defined next.\\
Definition 8.2.2. We say that the likelihood $L(\theta, \mathbf{x})$ has monotone likelihood ratio ( $\mathbf{m l r}$ ) in the statistic $y=u(\mathbf{x})$ if, for $\theta_{1}<\theta_{2}$, the ratio


\begin{equation*}
\frac{L\left(\theta_{1}, \mathbf{x}\right)}{L\left(\theta_{2}, \mathbf{x}\right)} \tag{8.2.2}
\end{equation*}


is a monotone function of $y=u(\mathbf{x})$.\\
Assume then that our likelihood function $L(\theta, \mathbf{x})$ has a monotone decreasing likelihood ratio in the statistic $y=u(\mathbf{x})$. Then the ratio in (8.2.2) is equal to $g(y)$, where $g$ is a decreasing function. The case where the likelihood function has a monotone increasing likelihood ratio (i.e., $g$ is an increasing function) follows similarly by changing the sense of the inequalities below. Let $\alpha$ denote the significance level. Then we claim that the following test is UMP level $\alpha$ for the hypotheses (8.2.1):


\begin{equation*}
\text { Reject } H_{0} \text { if } Y \geq c_{Y} \tag{8.2.3}
\end{equation*}


where $c_{Y}$ is determined by $\alpha=P_{\theta^{\prime}}\left[Y \geq c_{Y}\right]$. To show this claim, first consider the simple null hypothesis $H_{0}^{\prime}$ : $\theta=\theta^{\prime}$. Let $\theta^{\prime \prime}>\theta^{\prime}$ be arbitrary but fixed. Let $C$ denote the most powerful critical region for $\theta^{\prime}$ versus $\theta^{\prime \prime}$. By the Neyman-Pearson Theorem, $C$ is defined by

$$
\frac{L\left(\theta^{\prime}, \mathbf{X}\right)}{L\left(\theta^{\prime \prime}, \mathbf{X}\right)} \leq k \text { if and only if } \mathbf{X} \in C
$$

where $k$ is determined by $\alpha=P_{\theta^{\prime}}[\mathbf{X} \in C]$. But by Definition 8.2.2, because $\theta^{\prime \prime}>\theta^{\prime}$,

$$
\frac{L\left(\theta^{\prime}, \mathbf{X}\right)}{L\left(\theta^{\prime \prime}, \mathbf{X}\right)}=g(Y) \leq k \Leftrightarrow Y \geq g^{-1}(k)
$$

where $g^{-1}(k)$ satisfies $\alpha=P_{\theta^{\prime}}\left[Y \geq g^{-1}(k)\right]$; i.e., $c_{Y}=g^{-1}(k)$. Hence the NeymanPearson test is equivalent to the test defined by (8.2.3). Furthermore, the test is UMP for $\theta^{\prime}$ versus $\theta^{\prime \prime}>\theta^{\prime}$ because the test only depends on $\theta^{\prime \prime}>\theta^{\prime}$ and $g^{-1}(k)$ is uniquely determined under $\theta^{\prime}$.

Let $\gamma_{Y}(\theta)$ denote the power function of the test (8.2.3). To finish, we need to show that $\max _{\theta \leq \theta^{\prime}} \gamma_{Y}(\theta)=\alpha$. But this follows immediately if we can show that $\gamma_{Y}(\theta)$ is a nondecreasing function. To see this, let $\theta_{1}<\theta_{2}$. Note that since $\theta_{1}<\theta_{2}$, the test (8.2.3) is the most powerful test for testing $\theta_{1}$ versus $\theta_{2}$ with the level $\gamma_{Y}\left(\theta_{1}\right)$. By Corollary 8.1.1, the power of the test at $\theta_{2}$ must not be below the level; i.e., $\gamma_{Y}\left(\theta_{2}\right) \geq \gamma_{Y}\left(\theta_{1}\right)$. Hence $\gamma_{Y}(\theta)$ is a nondecreasing function. Since the power function is nondecreasing, it follows from Definition 8.1.2 that the mlr tests are unbiased tests for the hypotheses (8.2.1); see Exercise 8.2.14.

Example 8.2.5. Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from a Bernoulli distribution with parameter $p=\theta$, where $0<\theta<1$. Let $\theta^{\prime}<\theta^{\prime \prime}$. Consider the ratio of likelihoods

$$
\frac{L\left(\theta^{\prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)}{L\left(\theta^{\prime \prime} ; x_{1}, x_{2}, \ldots, x_{n}\right)}=\frac{\left(\theta^{\prime}\right)^{\sum x_{i}}\left(1-\theta^{\prime}\right)^{n-\sum x_{i}}}{\left(\theta^{\prime \prime}\right)^{\sum x_{i}}\left(1-\theta^{\prime \prime}\right)^{n-\sum x_{i}}}=\left[\frac{\theta^{\prime}\left(1-\theta^{\prime \prime}\right)}{\theta^{\prime \prime}\left(1-\theta^{\prime}\right)}\right]^{\sum x_{i}}\left(\frac{1-\theta^{\prime}}{1-\theta^{\prime \prime}}\right)^{n}
$$

Since $\theta^{\prime} / \theta^{\prime \prime}<1$ and $\left(1-\theta^{\prime \prime}\right) /\left(1-\theta^{\prime}\right)<1$, so that $\theta^{\prime}\left(1-\theta^{\prime \prime}\right) / \theta^{\prime \prime}\left(1-\theta^{\prime}\right)<1$, the ratio is a decreasing function of $y=\sum x_{i}$. Thus we have a monotone likelihood ratio in the statistic $Y=\sum X_{i}$.

Consider the hypotheses


\begin{equation*}
H_{0}: \theta \leq \theta^{\prime} \text { versus } H_{1}: \theta>\theta^{\prime} . \tag{8.2.4}
\end{equation*}


By our discussion above, the UMP level $\alpha$ decision rule for testing $H_{0}$ versus $H_{1}$ is given by

$$
\text { Reject } H_{0} \text { if } Y=\sum_{i=1}^{n} X_{i} \geq c,
$$

where $c$ is such that $\alpha=P_{\theta^{\prime}}[Y \geq c]$.\\
In the last example concerning a Bernoulli pmf, we obtained a UMP test by showing that its likelihood possesses mlr. The Bernoulli distribution is a regular case of the exponential family and our argument, under the one assumption below, can be generalized to the entire regular exponential family. To show this, suppose that the random sample $X_{1}, X_{2}, \ldots, X_{n}$ arises from a pdf or pmf representing a regular case of the exponential class, namely,

$$
f(x ; \theta)= \begin{cases}\exp [p(\theta) K(x)+H(x)+q(\theta)] & x \in \mathcal{S} \\ 0 & \text { elsewhere }\end{cases}
$$

where the support of $X, \mathcal{S}$, is free of $\theta$. Further assume that $p(\theta)$ is an increasing function of $\theta$. Then

$$
\begin{aligned}
\frac{L\left(\theta^{\prime}\right)}{L\left(\theta^{\prime \prime}\right)} & =\frac{\exp \left[p\left(\theta^{\prime}\right) \sum_{1}^{n} K\left(x_{i}\right)+\sum_{1}^{n} H\left(x_{i}\right)+n q\left(\theta^{\prime}\right)\right]}{\exp \left[p\left(\theta^{\prime \prime}\right) \sum_{1}^{n} K\left(x_{i}\right)+\sum_{1}^{n} H\left(x_{i}\right)+n q\left(\theta^{\prime \prime}\right)\right]} \\
& =\exp \left\{\left[p\left(\theta^{\prime}\right)-p\left(\theta^{\prime \prime}\right)\right] \sum_{1}^{n} K\left(x_{i}\right)+n\left[q\left(\theta^{\prime}\right)-q\left(\theta^{\prime \prime}\right)\right]\right\}
\end{aligned}
$$

If $\theta^{\prime}<\theta^{\prime \prime}, p(\theta)$ being an increasing function, requires this ratio to be a decreasing function of $y=\sum_{1}^{n} K\left(x_{i}\right)$. Thus, we have a monotone likelihood ratio in the statistic $Y=\sum_{1}^{n} K\left(X_{i}\right)$. Hence consider the hypotheses


\begin{equation*}
H_{0}: \theta \leq \theta^{\prime} \text { versus } H_{1}: \theta>\theta^{\prime} . \tag{8.2.5}
\end{equation*}


By our discussion above concerning mlr, the UMP level $\alpha$ decision rule for testing $H_{0}$ versus $H_{1}$ is given by

$$
\text { Reject } H_{0} \text { if } Y=\sum_{i=1}^{n} K\left(X_{i}\right) \geq c
$$

where $c$ is such that $\alpha=P_{\theta^{\prime}}[Y \geq c]$. Furthermore, the power function of this test is an increasing function in $\theta$.

For the record, consider the other one-sided alternative hypotheses,


\begin{equation*}
H_{0}: \theta \geq \theta^{\prime} \text { versus } H_{1}: \theta<\theta^{\prime} \tag{8.2.6}
\end{equation*}


The UMP level $\alpha$ decision rule is, for $p(\theta)$ an increasing function,

$$
\text { Reject } H_{0} \text { if } Y=\sum_{i=1}^{n} K\left(X_{i}\right) \leq c
$$

where $c$ is such that $\alpha=P_{\theta^{\prime}}[Y \leq c]$.\\
If in the preceding situation with monotone likelihood ratio we test $H_{0}: \theta=$ $\theta^{\prime}$ against $H_{1}: \theta>\theta^{\prime}$, then $\sum K\left(x_{i}\right) \geq c$ would be a uniformly most powerful critical region. From the likelihood ratios displayed in Examples 8.2.2-8.2.5, we see immediately that the respective critical regions

$$
\sum_{i=1}^{n} x_{i}^{2} \geq c, \quad \sum_{i=1}^{n} x_{i} \geq c, \quad \sum_{i=1}^{n} x_{i} \geq c, \quad \sum_{i=1}^{n} x_{i} \geq c
$$

are uniformly most powerful for testing $H_{0}: \theta=\theta^{\prime}$ against $H_{1}: \theta>\theta^{\prime}$.\\
There is a final remark that should be made about uniformly most powerful tests. Of course, in Definition 8.2.1, the word uniformly is associated with $\theta$; that is, $C$ is a best critical region of size $\alpha$ for testing $H_{0}: \theta=\theta_{0}$ against all $\theta$ values given by the composite alternative $H_{1}$. However, suppose that the form of such a region is

$$
u\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq c
$$

Then this form provides uniformly most powerful critical regions for all attainable $\alpha$ values by, of course, appropriately changing the value of $c$. That is, there is a certain uniformity property, also associated with $\alpha$, that is not always noted in statistics texts.

\section*{EXERCISES}
8.2.1. Let $X$ have the $\operatorname{pmf} f(x ; \theta)=\theta^{x}(1-\theta)^{1-x}, x=0,1$, zero elsewhere. We test the simple hypothesis $H_{0}: \theta=\frac{1}{4}$ against the alternative composite hypothesis $H_{1}: \theta<\frac{1}{4}$ by taking a random sample of size 10 and rejecting $H_{0}: \theta=\frac{1}{4}$ if and only if the observed values $x_{1}, x_{2}, \ldots, x_{10}$ of the sample observations are such that $\sum_{1}^{10} x_{i} \leq 1$. Find the power function $\gamma(\theta), 0<\theta \leq \frac{1}{4}$, of this test.\\
8.2.2. Let $X$ have a pdf of the form $f(x ; \theta)=1 / \theta, 0<x<\theta$, zero elsewhere. Let $Y_{1}<Y_{2}<Y_{3}<Y_{4}$ denote the order statistics of a random sample of size 4 from this distribution. Let the observed value of $Y_{4}$ be $y_{4}$. We reject $H_{0}: \theta=1$ and accept $H_{1}: \theta \neq 1$ if either $y_{4} \leq \frac{1}{2}$ or $y_{4}>1$. Find the power function $\gamma(\theta), 0<\theta$, of the test.\\
8.2.3. Consider a normal distribution of the form $N(\theta, 4)$. The simple hypothesis $H_{0}: \theta=0$ is rejected, and the alternative composite hypothesis $H_{1}: \theta>0$ is accepted if and only if the observed mean $\bar{x}$ of a random sample of size 25 is greater than or equal to $\frac{3}{5}$. Find the power function $\gamma(\theta), 0 \leq \theta$, of this test.\\
8.2.4. Consider the distributions $N\left(\mu_{1}, 400\right)$ and $N\left(\mu_{2}, 225\right)$. Let $\theta=\mu_{1}-\mu_{2}$. Let $\bar{x}$ and $\bar{y}$ denote the observed means of two independent random samples, each of size $n$, from these two distributions. We reject $H_{0}: \theta=0$ and accept $H_{1}: \theta>0$ if and only if $\bar{x}-\bar{y} \geq c$. If $\gamma(\theta)$ is the power function of this test, find $n$ and $c$ so that $\gamma(0)=0.05$ and $\gamma(10)=0.90$, approximately.\\
8.2.5. Consider Example 8.2.2. Show that $L(\theta)$ has a monotone likelihood ratio in the statistic $\sum_{i=1}^{n} X_{i}^{2}$. Use this to determine the UMP test for $H_{0}: \theta=\theta^{\prime}$, where $\theta^{\prime}$ is a fixed positive number, versus $H_{1}: \theta<\theta^{\prime}$.\\
8.2.6. If, in Example 8.2 .2 of this section, $H_{0}: \theta=\theta^{\prime}$, where $\theta^{\prime}$ is a fixed positive number, and $H_{1}: \theta \neq \theta^{\prime}$, show that there is no uniformly most powerful test for testing $H_{0}$ against $H_{1}$.\\
8.2.7. Let $X_{1}, X_{2}, \ldots, X_{25}$ denote a random sample of size 25 from a normal distribution $N(\theta, 100)$. Find a uniformly most powerful critical region of size $\alpha=0.10$ for testing $H_{0}: \theta=75$ against $H_{1}: \theta>75$.\\
8.2.8. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a normal distribution $N(\theta, 16)$. Find the sample size $n$ and a uniformly most powerful test of $H_{0}: \theta=25$ against $H_{1}: \theta<25$ with power function $\gamma(\theta)$ so that approximately $\gamma(25)=0.10$ and $\gamma(23)=0.90$.\\
8.2.9. Consider a distribution having a pmf of the form $f(x ; \theta)=\theta^{x}(1-\theta)^{1-x}, x=$ 0,1 , zero elsewhere. Let $H_{0}: \theta=\frac{1}{20}$ and $H_{1}: \theta>\frac{1}{20}$. Use the Central Limit Theorem to determine the sample size $n$ of a random sample so that a uniformly most powerful test of $H_{0}$ against $H_{1}$ has a power function $\gamma(\theta)$, with approximately $\gamma\left(\frac{1}{20}\right)=0.05$ and $\gamma\left(\frac{1}{10}\right)=0.90$.\\
8.2.10. Illustrative Example 8.2 .1 of this section dealt with a random sample of size $n=2$ from a gamma distribution with $\alpha=1, \beta=\theta$. Thus the mgf of the distribution is $(1-\theta t)^{-1}, t<1 / \theta, \theta \geq 2$. Let $Z=X_{1}+X_{2}$. Show that $Z$ has a gamma distribution with $\alpha=2, \beta=\theta$. Express the power function $\gamma(\theta)$ of Example 8.2.1 in terms of a single integral. Generalize this for a random sample of size $n$.\\
8.2.11. Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from a distribution with pdf $f(x ; \theta)=\theta x^{\theta-1}, 0<x<1$, zero elsewhere, where $\theta>0$. Show the likelihood has mlr in the statistic $\prod_{i=1}^{n} X_{i}$. Use this to determine the UMP test for $H_{0}: \theta=\theta^{\prime}$ against $H_{1}: \theta<\theta^{\prime}$, for fixed $\theta^{\prime}>0$.\\
8.2.12. Let $X$ have the pdf $f(x ; \theta)=\theta^{x}(1-\theta)^{1-x}, x=0,1$, zero elsewhere. We test $H_{0}: \theta=\frac{1}{2}$ against $H_{1}: \theta<\frac{1}{2}$ by taking a random sample $X_{1}, X_{2}, \ldots, X_{5}$ of size $n=5$ and rejecting $H_{0}$ if $Y=\sum_{1}^{n} X_{i}$ is observed to be less than or equal to a constant $c$.\\
(a) Show that this is a uniformly most powerful test.\\
(b) Find the significance level when $c=1$.\\
(c) Find the significance level when $c=0$.\\
(d) By using a randomized test, as discussed in Example 4.6.4, modify the tests given in parts (b) and (c) to find a test with significance level $\alpha=\frac{2}{32}$.\\
8.2.13. Let $X_{1}, \ldots, X_{n}$ denote a random sample from a gamma-type distribution with $\alpha=2$ and $\beta=\theta$. Let $H_{0}: \theta=1$ and $H_{1}: \theta>1$.\\
(a) Show that there exists a uniformly most powerful test for $H_{0}$ against $H_{1}$, determine the statistic $Y$ upon which the test may be based, and indicate the nature of the best critical region.\\
(b) Find the pdf of the statistic $Y$ in part (a). If we want a significance level of 0.05 , write an equation that can be used to determine the critical region. Let $\gamma(\theta), \theta \geq 1$, be the power function of the test. Express the power function as an integral.\\
8.2.14. Show that the mlr test defined by expression (8.2.3) is an unbiased test for the hypotheses (8.2.1).

\subsection*{8.3 Likelihood Ratio Tests}
In the first section of this chapter, we presented the most powerful tests for simple versus simple hypotheses. In the second section, we extended this theory to uniformly most powerful tests for essentially one-sided alternative hypotheses and families of distributions that have a monotone likelihood ratio. What about the general case? That is, suppose the random variable $X$ has pdf or pmf $f(x ; \boldsymbol{\theta})$, where $\boldsymbol{\theta}$ is a vector of parameters in $\Omega$. Let $\omega \subset \Omega$ and consider the hypotheses


\begin{equation*}
H_{0}: \boldsymbol{\theta} \in \omega \text { versus } H_{1}: \boldsymbol{\theta} \in \Omega \cap \omega^{c} . \tag{8.3.1}
\end{equation*}


There are complications in extending the optimal theory to this general situation, which are addressed in more advanced books; see, in particular, Lehmann (1986). We illustrate some of these complications with an example. Suppose $X$ has a $N\left(\theta_{1}, \theta_{2}\right)$ distribution and that we want to test $\theta_{1}=\theta_{1}^{\prime}$, where $\theta_{1}^{\prime}$ is specified. In the notation of (8.3.1), $\boldsymbol{\theta}=\left(\theta_{1}, \theta_{2}\right), \Omega=\left\{\boldsymbol{\theta}:-\infty<\theta_{1}<\infty, \theta_{2}>0\right\}$, and $\omega=\left\{\boldsymbol{\theta}: \theta_{1}=\theta_{1}^{\prime}, \theta_{2}>0\right\}$. Notice that $H_{0}: \boldsymbol{\theta} \in \omega$ is a composite null hypothesis. Let $X_{1}, \ldots, X_{n}$ be a random sample on $X$.

Assume for the moment that $\theta_{2}$ is known. Then $H_{0}$ becomes the simple hypothesis $\theta_{1}=\theta_{1}^{\prime}$. This is essentially the situation discussed in Example 8.2.3. There\\
it was shown that no UMP test exists for this situation. If we restrict attention to the class of unbiased tests (Definition 8.1.2), then a theory of best tests can be constructed; see Lehmann (1986). For our illustrative example, as Exercise 8.3.21 shows, the test based on the critical region

$$
C_{2}=\left\{\left|\bar{X}-\theta_{1}^{\prime}\right|>\sqrt{\frac{\theta_{2}}{n}} z_{\alpha / 2}\right\}
$$

is unbiased. Then it follows from Lehmann that it is an UMP unbiased level $\alpha$ test.\\
In practice, though, the variance $\theta_{2}$ is unknown. In this case, theory for optimal tests can be constructed using the concept of what are called conditional tests. We do not pursue this any further in this text, but refer the interested reader to Lehmann (1986).

Recall from Chapter 6 that the likelihood ratio tests (6.3.3) can be used to test general hypotheses such as (8.3.1). While in general the exact null distribution of the test statistic cannot be determined, under regularity condtions the likelihood ratio test statistic is asymptotically $\chi^{2}$ under $H_{0}$. Hence we can obtain an approximate test in most situations. Although, there is no guarantee that likelihood ratio tests are optimal, similar to tests based on the Neyman-Pearson Theorem, they are based on a ratio of likelihood functions and, in many situations, are asymptotically optimal.

In the example above on testing for the mean of a normal distribution, with known variance, the likelihood ratio test is the same as the UMP unbiased test. When the variance is unknown, the likelihood ratio test results in the one-sample $t$-test as shown in Example 6.5.1 of Chapter 6. This is the same as the conditional test discussed in Lehmann (1986).

In the remainder of this section, we present likelihood ratio tests for situations when sampling from normal distributions.

\subsection*{8.3.1 Likelihood Ratio Tests for Testing Means of Normal Distributions}
In Example 6.5.1 of Chapter 6, we derived the likelihood ratio test for the onesample $t$-test to test for the mean of a normal distribution with unknown variance. In the next example, we derive the likelihood ratio test for compairing the means of two independent normal distributions. We then discuss the power functions for both of these tests.

Example 8.3.1. Let the independent random variables $X$ and $Y$ have distributions that are $N\left(\theta_{1}, \theta_{3}\right)$ and $N\left(\theta_{2}, \theta_{3}\right)$, where the means $\theta_{1}$ and $\theta_{2}$ and common variance $\theta_{3}$ are unknown. Then $\Omega=\left\{\left(\theta_{1}, \theta_{2}, \theta_{3}\right):-\infty<\theta_{1}<\infty,-\infty<\theta_{2}<\infty, 0<\theta_{3}<\infty\right\}$. Let $X_{1}, X_{2}, \ldots, X_{n}$ and $Y_{1}, Y_{2}, \ldots, Y_{m}$ denote independent random samples from these distributions. The hypothesis $H_{0}: \theta_{1}=\theta_{2}$, unspecified, and $\theta_{3}$ unspecified, is to be tested against all alternatives. Then $\omega=\left\{\left(\theta_{1}, \theta_{2}, \theta_{3}\right):-\infty<\theta_{1}=\theta_{2}<\right.$ $\left.\infty, 0<\theta_{3}<\infty\right\}$. Here $X_{1}, X_{2}, \ldots, X_{n}, Y_{1}, Y_{2}, \ldots, Y_{m}$ are $n+m>2$ mutually\\
independent random variables having the likelihood functions

$$
L(\omega)=\left(\frac{1}{2 \pi \theta_{3}}\right)^{(n+m) / 2} \exp \left\{-\frac{1}{2 \theta_{3}}\left[\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)^{2}+\sum_{1}^{m}\left(y_{i}-\theta_{1}\right)^{2}\right]\right\}
$$

and

$$
L(\Omega)=\left(\frac{1}{2 \pi \theta_{3}}\right)^{(n+m) / 2} \exp \left\{-\frac{1}{2 \theta_{3}}\left[\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)^{2}+\sum_{1}^{m}\left(y_{i}-\theta_{2}\right)^{2}\right]\right\}
$$

If $\partial \log L(\omega) / \partial \theta_{1}$ and $\partial \log L(\omega) / \partial \theta_{3}$ are equated to zero, then (Exercise 8.3.2)


\begin{align*}
\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)+\sum_{1}^{m}\left(y_{i}-\theta_{1}\right) & =0 \\
\frac{1}{\theta_{3}}\left[\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)^{2}+\sum_{1}^{m}\left(y_{i}-\theta_{1}\right)^{2}\right] & =n+m \tag{8.3.2}
\end{align*}


The solutions for $\theta_{1}$ and $\theta_{3}$ are, respectively,

$$
\begin{aligned}
& u=(n+m)^{-1}\left\{\sum_{1}^{n} x_{i}+\sum_{1}^{m} y_{i}\right\} \\
& w=(n+m)^{-1}\left\{\sum_{1}^{n}\left(x_{i}-u\right)^{2}+\sum_{1}^{m}\left(y_{i}-u\right)^{2}\right\} .
\end{aligned}
$$

Further, $u$ and $w$ maximize $L(\omega)$. The maximum is

$$
L(\hat{\omega})=\left(\frac{e^{-1}}{2 \pi w}\right)^{(n+m) / 2}
$$

In a like manner, if

$$
\frac{\partial \log L(\Omega)}{\partial \theta_{1}}, \quad \frac{\partial \log L(\Omega)}{\partial \theta_{2}}, \quad \frac{\partial \log L(\Omega)}{\partial \theta_{3}}
$$

are equated to zero, then (Exercise 8.3.3)


\begin{gather*}
\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)=0 \\
\sum_{1}^{m}\left(y_{i}-\theta_{2}\right)=0  \tag{8.3.3}\\
-(n+m)+\frac{1}{\theta_{3}}\left[\sum_{1}^{n}\left(x_{i}-\theta_{1}\right)^{2}+\sum_{1}^{m}\left(y_{i}-\theta_{2}\right)^{2}\right]=0
\end{gather*}


The solutions for $\theta_{1}, \theta_{2}$, and $\theta_{3}$ are, respectively,

$$
\begin{aligned}
& u_{1}=n^{-1} \sum_{1}^{n} x_{i} \\
& u_{2}=m^{-1} \sum_{1}^{m} y_{i} \\
& w^{\prime}=(n+m)^{-1}\left[\sum_{1}^{n}\left(x_{i}-u_{1}\right)^{2}+\sum_{1}^{m}\left(y_{i}-u_{2}\right)^{2}\right]
\end{aligned}
$$

and, further, $u_{1}, u_{2}$, and $w^{\prime}$ maximize $L(\Omega)$. The maximum is

$$
L(\hat{\Omega})=\left(\frac{e^{-1}}{2 \pi w^{\prime}}\right)^{(n+m) / 2}
$$

so that

$$
\Lambda\left(x_{1}, \ldots, x_{n}, y_{1}, \ldots, y_{m}\right)=\Lambda=\frac{L(\hat{\omega})}{L(\hat{\Omega})}=\left(\frac{w^{\prime}}{w}\right)^{(n+m) / 2}
$$

The random variable defined by $\Lambda^{2 /(n+m)}$ is

$$
\frac{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}{\sum_{1}^{n}\left\{X_{i}-[(n \bar{X}+m \bar{Y}) /(n+m)]\right\}^{2}+\sum_{1}^{n}\left\{Y_{i}-[(n \bar{X}+m \bar{Y}) /(n+m)]\right\}^{2}}
$$

Now

$$
\begin{aligned}
\sum_{1}^{n}\left(X_{i}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2} & =\sum_{1}^{n}\left[\left(X_{i}-\bar{X}\right)+\left(\bar{X}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)\right]^{2} \\
& =\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+n\left(\bar{X}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2}
\end{aligned}
$$

and

$$
\begin{aligned}
\sum_{1}^{m}\left(Y_{i}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2} & =\sum_{1}^{m}\left[\left(Y_{i}-\bar{Y}\right)+\left(\bar{Y}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)\right]^{2} \\
& =\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}+m\left(\bar{Y}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2}
\end{aligned}
$$

But

$$
n\left(\bar{X}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2}=\frac{m^{2} n}{(n+m)^{2}}(\bar{X}-\bar{Y})^{2}
$$

and

$$
m\left(\bar{Y}-\frac{n \bar{X}+m \bar{Y}}{n+m}\right)^{2}=\frac{n^{2} m}{(n+m)^{2}}(\bar{X}-\bar{Y})^{2}
$$

Hence the random variable defined by $\Lambda^{2 /(n+m)}$ may be written

$$
\begin{aligned}
& \frac{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}{} \\
& \sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}+[n m /(n+m)](\bar{X}-\bar{Y})^{2} \\
&=\frac{1}{1+\frac{[n m /(n+m)](\bar{X}-\bar{Y})^{2}}{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}}
\end{aligned}
$$

If the hypothesis $H_{0}: \theta_{1}=\theta_{2}$ is true, the random variable


\begin{equation*}
T=\sqrt{\frac{n m}{n+m}}(\bar{X}-\bar{Y})\left\{(n+m-2)^{-1}\left[\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}\right]\right\}^{-1 / 2} \tag{8.3.4}
\end{equation*}


has, in accordance with Section 3.6, a $t$-distribution with $n+m-2$ degrees of freedom. Thus the random variable defined by $\Lambda^{2 /(n+m)}$ is

$$
\frac{n+m-2}{(n+m-2)+T^{2}} .
$$

The test of $H_{0}$ against all alternatives may then be based on a $t$-distribution with $n+m-2$ degrees of freedom.

The likelihood ratio principle calls for the rejection of $H_{0}$ if and only if $\Lambda \leq \lambda_{0}<$ 1. Thus the significance level of the test is

$$
\alpha=P_{H_{0}}\left[\Lambda\left(X_{1}, \ldots, X_{n}, Y_{1}, \ldots, Y_{m}\right) \leq \lambda_{0}\right] .
$$

However, $\Lambda\left(X_{1}, \ldots, X_{n}, Y_{1}, \ldots, Y_{m}\right) \leq \lambda_{0}$ is equivalent to $|T| \geq c$, and so

$$
\alpha=P\left(|T| \geq c ; H_{0}\right)
$$

For given values of $n$ and $m$, the number $c$ is is easily computed. In $\mathrm{R}, c=\mathrm{qt}(1-$ $\alpha / 2, n+m-2)$. Then $H_{0}$ is rejected at a significance level $\alpha$ if and only if $|t| \geq c$, where $t$ is the observed value of $T$. If, for instance, $n=10, m=6$, and $\alpha=0.05$, then $c=\mathrm{qt}(0.975,14)=2.1448$.

For this last example as well as the one-sample $t$-test derived in Example 6.5.1, it was found that the likelihood ratio test could be based on a statistic that, when the hypothesis $H_{0}$ is true, has a $t$-distribution. To help us compute the power functions of these tests at parameter points other than those described by the hypothesis $H_{0}$, we turn to the following definition.

Definition 8.3.1. Let the random variable $W$ be $N(\delta, 1)$; let the random variable $V$ be $\chi^{2}(r)$, and let $W$ and $V$ be independent. The quotient

$$
T=\frac{W}{\sqrt{V / r}}
$$

is said to have a noncentral $t$-distribution with $r$ degrees of freedom and noncentrality parameter $\delta$. If $\delta=0$, we say that $T$ has a central $t$-distribution.

In the light of this definition, let us reexamine the $t$-statistics of Examples 6.5.1 and 8.3.1.

Example 8.3.2 (Power of the One Sample $t$-Test). For Example 6.5.1, consider a more general situation. Assume that $X_{1}, \ldots, X_{n}$ is a random sample on $X$ that has a $N\left(\mu, \sigma^{2}\right)$ distribution. We are interested in testing $H_{0}: \mu=\mu_{0}$ versus $H_{1}: \mu \neq \mu_{0}$, where $\mu_{0}$ is specified. Then from Example 6.5.1, the likelihood ratio test statistic is

$$
\begin{aligned}
t\left(X_{1}, \ldots, X_{n}\right) & =\frac{\sqrt{n}\left(\bar{X}-\mu_{0}\right)}{\sqrt{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2} /(n-1)}} \\
& =\frac{\sqrt{n}\left(\bar{X}-\mu_{0}\right) / \sigma}{\sqrt{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2} /\left[\sigma^{2}(n-1)\right]}}
\end{aligned}
$$

The hypothesis $H_{0}$ is rejected at level $\alpha$ if $|t| \geq t_{\alpha / 2, n-1}$. Suppose $\mu_{1} \neq \mu_{0}$ is an alternative of interest. Because $E_{\mu_{1}}[\sqrt{n} \bar{X} / \sigma \sqrt{n} \bar{X} / \sigma]=\sqrt{n}\left(\mu_{1}-\mu_{0}\right) / \sigma$, the power of the test to detect $\mu_{1}$ is


\begin{equation*}
\gamma\left(\mu_{1}\right)=P\left(|t| \geq t_{\alpha / 2, n-1}\right)=1-P\left(t \leq t_{\alpha / 2, n-1}\right)+P\left(t \leq-t_{\alpha / 2, n-1}\right) \tag{8.3.5}
\end{equation*}


where $t$ has a noncentral $t$-distribution with noncentrality parameter $\delta=\sqrt{n}\left(\mu_{1}-\right.$ $\left.\mu_{0}\right) / \sigma$ and $n-1$ degrees of freedom. This is computed in R by the call

$$
1-\mathrm{pt}(\mathrm{tc}, \mathrm{n}-1, \mathrm{ncp}=\mathrm{delta})+\mathrm{pt}(-\mathrm{tc}, \mathrm{n}-1, \mathrm{ncp}=\mathrm{delta})
$$

where tc is $t_{\alpha / 2, n-1}$ and delta is the noncentrality parameter $\delta$.\\
The following R code computes a graph of the power curve of this test. Notice that the horizontal range of the plot is the interval $\left[\mu_{0}-4 \sigma / \sqrt{n}, \mu_{0}+4 \sigma / \sqrt{n}\right]$. As indicated the parameters need to be set.

\begin{verbatim}
## Input mu0, sig, n, alpha.
fse = 4*sig/sqrt(n); maxmu = mu0 + fse; tc = qt(1-(alpha/2),n-1)
minmu = mu0 -fse; mu1 = seq(minmu,maxmu,.1)
delta = (mu1-mu0)/(sig/sqrt(n))
gs = 1 - pt(tc,n-1,ncp=delta) + pt(-tc,n-1,ncp=delta)
plot(gs ~mu1,pch=" ",xlab=expression(mu[1]),ylab=expression(gamma))
lines(gs_mu1)
\end{verbatim}

This code is the body of the function tpowerg. R. Exercise 8.3.5 discusses its use.

Example 8.3.3 (Power of the Two Sample $t$-Test). In Example 8.3.1 we had

$$
T=\frac{W_{2}}{\sqrt{V_{2} /(n+m-2)}},
$$

where

$$
W_{2}=\sqrt{\frac{n m}{n+m}}(\bar{X}-\bar{Y}) / \sigma
$$

and

$$
V_{2}=\frac{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}{\sigma^{2}}
$$

Here $W_{2}$ is $N\left[\sqrt{n m /(n+m)}\left(\theta_{1}-\theta_{2}\right) / \sigma, 1\right], V_{2}$ is $\chi^{2}(n+m-2)$, and $W_{2}$ and $V_{2}$ are independent. Accordingly, if $\theta_{1} \neq \theta_{2}, T$ has a noncentral $t$-distribution with $n+m-2$ degrees of freedom and noncentrality parameter $\delta_{2}=\sqrt{n m /(n+m)}\left(\theta_{1}-\theta_{2}\right) / \sigma$. It is interesting to note that $\delta_{1}=\sqrt{n} \theta_{1} / \sigma$ measures the deviation of $\theta_{1}$ from $\theta_{1}=0$ in units of the standard deviation $\sigma / \sqrt{n}$ of $\bar{X}$. The noncentrality parameter $\delta_{2}=$ $\sqrt{n m /(n+m)}\left(\theta_{1}-\theta_{2}\right) / \sigma$ is equal to the deviation of $\theta_{1}-\theta_{2}$ from $\theta_{1}-\theta_{2}=0$ in units of the standard deviation $\sigma / \sqrt{(n+m) / m n}$ of $\bar{X}-\bar{Y}$.

As in the last example, it is easy to write R code that evaluates power for this test. For a numerical illustration, assume that the common variance is $\theta_{3}=100$, $n=20$, and $m=15$. Suppose $\alpha=0.05$ and we want to determine the power of the test to detect $\Delta=5$, where $\Delta=\theta_{1}-\theta_{2}$. In this case the critical value is $t_{0.25,33}=\mathrm{qt}(.975,33)=2.0345$ and the noncentrality parameter is $\delta_{2}=1.4639$. The power is computed as\\
$1-\mathrm{pt}(2.0345,33, \mathrm{ncp}=1.4639)+\mathrm{pt}(-2.0345,33, \mathrm{ncp}=1.4639)=0.2954$\\
Hence, the test has a $29.4 \%$ chance of detecting a difference in means of 5 .\\
Remark 8.3.1. The one- and two-sample tests for normal means, presented in Examples 6.5.1 and 8.3.1, are the tests for normal means presented in most elementary statistics books. They are based on the assumption of normality. What if the underlying distributions are not normal? In that case, with finite variances, the $t$-test statistics for these situations are asymptotically correct. For example, consider the one-sample $t$-test. Suppose $X_{1}, \ldots, X_{n}$ are iid with a common nonnormal pdf that has mean $\theta_{1}$ and finite variance $\sigma^{2}$. The hypotheses remain the same, i.e., $H_{0}: \theta_{1}=\theta_{1}^{\prime}$ versus $H_{1}: \theta_{1} \neq \theta_{1}^{\prime}$. The $t$-test statistic, $T_{n}$, is given by


\begin{equation*}
T_{n}=\frac{\sqrt{n}\left(\bar{X}-\theta_{1}^{\prime}\right)}{S_{n}} \tag{8.3.6}
\end{equation*}


where $S_{n}$ is the sample standard deviation. Our critical region is $C_{1}=\left\{\left|T_{n}\right| \geq\right.$ $\left.t_{\alpha / 2, n-1}\right\}$. Recall that $S_{n} \rightarrow \sigma$ in probability. Hence, by the Central Limit Theorem, under $H_{0}$,


\begin{equation*}
T_{n}=\frac{\sigma}{S_{n}} \frac{\sqrt{n}\left(\bar{X}-\theta_{1}^{\prime}\right)}{\sigma} \xrightarrow{D} Z, \tag{8.3.7}
\end{equation*}


where $Z$ has a standard normal distribution. Hence the asymptotic test would use the critical region $C_{2}=\left\{\left|T_{n}\right| \geq z_{\alpha / 2}\right\}$. By (8.3.7) the critical region $C_{2}$ would have approximate size $\alpha$. In practice, we would use $C_{1}$, because $t$ critical values are generally larger than $z$ critical values and, hence, the use of $C_{1}$ would be conservative; i.e., the size of $C_{1}$ would be slightly smaller than that of $C_{2}$. As Exercise 8.3.4 shows, the two-sample $t$-test is also asymptotically correct, provided the underlying distributions have the same variance.

For nonnormal situations where the distribution is "close" to the normal distribution, the $t$-test is essentially valid; i.e., the true level of significance is close to the nominal $\alpha$. In terms of robustness, we would say that for these situations the $t$-test possesses robustness of validity. But the $t$-test may not possess robustness of power. For nonnormal situations, there are more powerful tests than the $t$-test; see Chapter 10 for discussion.

For finite sample sizes and for distributions that are decidedly not normal, very skewed for instance, the validity of the $t$-test may also be questionable, as we illustrate in the following simulation study.

Example 8.3.4 (Skewed Contaminated Normal Family of Distributions). Consider the random variable $X$ given by


\begin{equation*}
X=\left(1-I_{\epsilon}\right) Z+I_{\epsilon} Y \tag{8.3.8}
\end{equation*}


where $Z$ has a $N(0,1)$ distribution, $Y$ has a $N\left(\mu_{c}, \sigma_{c}^{2}\right)$ distribution, $I_{\epsilon}$ has a $\operatorname{bin}(1, \epsilon)$ distribution, and $Z, Y$, and $I_{\epsilon}$ are mutually independent. Assume that $\epsilon<0.5$ and $\sigma_{c}>1$, so that $Y$ is the contaminating random variable in the mixture. If $\mu_{c}=0$, then $X$ has the contaminated normal distribution discussed in Section 3.4.1, which is symmetrically distributed about 0 . For $\mu_{c} \neq 0$, the distribution of $X$, (8.3.8), is skewed and we call it the skewed contaminated normal distribution, $S C N\left(\epsilon, \sigma_{c}, \mu_{C}\right)$. Note that $E(X)=\epsilon \mu_{c}$ and in Exercise 8.3.18 the cdf and pdf of $X$ are derived. The R function rscn generates random variates from this distribution.

In this example, we show the results of a small simulation study on the validity of the $t$-test for random samples from the distribution of $X$. Consider the one-sided hypotheses

$$
H_{0}: \mu=\mu_{X} \text { versus } H_{0}: \mu<\mu_{X} .
$$

Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from the distribution of $X$. As a test statistic we consider the $t$-test discussed in Example 4.5.4, which is also given in expression (8.3.6); that is, the test statistic is $T_{n}=\left(\bar{X}-\mu_{X}\right) /\left(S_{n} / \sqrt{n}\right)$, where $\bar{X}$ and $S_{n}$ are the sample mean and standard deviation of $X_{1}, X_{2}, \ldots, X_{n}$, respectively. We set the level of significance at $\alpha=0.05$ and used the decision rule: Reject $H_{0}$ if $T_{n} \leq t_{0.05, n-1}$. For the study, we set $n=30, \epsilon=0.20$, and $\sigma_{c}=25$. For $\mu_{c}$, we selected the five values of $0,5,10,15$, and 20, as shown in Table 8.3.1. For each of these five situations, we ran 10,000 simulations and recorded $\widehat{\alpha}$, which is the number of rejections of $H_{0}$ divided by the number of simulations, i.e., the empirical $\alpha$ level.

For the test to be valid, $\widehat{\alpha}$ should be close to the nominal value of 0.05 . As Table 8.3.1 shows, though, for all cases other than $\mu_{c}=0$, the $t$-test is quite liberal; that is, its empirical significance level far exceeds the nominal 0.05 level (as Exercise

Table 8.3.1: Empirical $\alpha$ Levels for the Nominal $0.05 t$-Test of Example 8.3.4.

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 & \multicolumn{5}{|c|}{Empirical $\alpha$} \\
\hline
$\mu_{c}$ & 0 & 5 & 10 & 15 & 20 \\
\hline
$\widehat{\alpha}$ & 0.0458 & 0.0961 & 0.1238 & 0.1294 & 0.1301 \\
\hline
\end{tabular}
\end{center}

8.3.19 shows, the sampling error in the table is about 0.004 ). Note that when $\mu_{c}=0$ the distribution of $X$ is symmetric about 0 and in this case the empirical level is close to the nominal value of 0.05 .

\subsection*{8.3.2 Likelihood Ratio Tests for Testing Variances of Normal Distributions}
In this section, we discuss likelihood ratio tests for variances of normal distributions. In the next example, we begin with the two sample problem.

Example 8.3.5. In Example 8.3.1, in testing the equality of the means of two normal distributions, it was assumed that the unknown variances of the distributions were equal. Let us now consider the problem of testing the equality of these two unknown variances. We are given the independent random samples $X_{1}, \ldots, X_{n}$ and $Y_{1}, \ldots, Y_{m}$ from the distributions, which are $N\left(\theta_{1}, \theta_{3}\right)$ and $N\left(\theta_{2}, \theta_{4}\right)$, respectively. We have

$$
\Omega=\left\{\left(\theta_{1}, \theta_{2}, \theta_{3}, \theta_{4}\right):-\infty<\theta_{1}, \theta_{2}<\infty, 0<\theta_{3}, \theta_{4}<\infty\right\} .
$$

The hypothesis $H_{0}: \theta_{3}=\theta_{4}$, unspecified, with $\theta_{1}$ and $\theta_{2}$ also unspecified, is to be tested against all alternatives. Then

$$
\omega=\left\{\left(\theta_{1}, \theta_{2}, \theta_{3}, \theta_{4}\right):-\infty<\theta_{1}, \theta_{2}<\infty, 0<\theta_{3}=\theta_{4}<\infty\right\} .
$$

It is easy to show (see Exercise 8.3.11) that the statistic defined by $\Lambda=L(\hat{\omega}) / L(\hat{\Omega})$ is a function of the statistic


\begin{equation*}
F=\frac{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2} /(n-1)}{\sum_{1}^{m}\left(Y_{i}-\bar{Y}\right)^{2} /(m-1)} . \tag{8.3.9}
\end{equation*}


If $\theta_{3}=\theta_{4}$, this statistic $F$ has an $F$-distribution with $n-1$ and $m-1$ degrees of freedom. The hypothesis that $\left(\theta_{1}, \theta_{2}, \theta_{3}, \theta_{4}\right) \in \omega$ is rejected if the computed $F \leq c_{1}$ or if the computed $F \geq c_{2}$. The constants $c_{1}$ and $c_{2}$ are usually selected so that, if $\theta_{3}=\theta_{4}$,

$$
P\left(F \leq c_{1}\right)=P\left(F \geq c_{2}\right)=\frac{\alpha_{1}}{2}
$$

where $\alpha_{1}$ is the desired significance level of this test. The power function of this test is derived in Exercise 8.3.10.

Remark 8.3.2. We caution the reader on this last test for the equality of two variances. In Remark 8.3.1, we discussed that the one- and two-sample $t$-tests for means are asymptotically correct. The two-sample variance test of the last example is not, however; see, for example, page 143 of Hettmansperger and McKean (2011). If the underlying distributions are not normal, then the $F$-critical values may be far from valid critical values (unlike the $t$-critical values for the means tests as discussed in Remark 8.3.1). In a large simulation study, Conover, Johnson, and Johnson (1981) showed that instead of having the nominal size of $\alpha=0.05$, the $F$-test for variances using the $F$-critical values could have significance levels as high as 0.80 , in certain nonnormal situations. Thus the two-sample $F$-test for variances does not possess robustness of validity. It should only be used in situations where the assumption of normality can be justified. See Exercise 8.3.17 for an illustrative data set.

The corresponding likelihood ratio test for the variance of a normal distribution based on one sample is discussed in Exercise 8.3.9. The cautions raised in Remark 8.3.1, hold for this test also.

Example 8.3.6. Let the independent random variables $X$ and $Y$ have distributions that are $N\left(\theta_{1}, \theta_{3}\right)$ and $N\left(\theta_{2}, \theta_{4}\right)$. In Example 8.3.1, we derived the likelihood ratio test statistic $T$ of the hypothesis $\theta_{1}=\theta_{2}$ when $\theta_{3}=\theta_{4}$, while in Example 8.3.5 we obtained the likelihood ratio test statistic $F$ of the hypothesis $\theta_{3}=\theta_{4}$. The hypothesis that $\theta_{1}=\theta_{2}$ is rejected if the computed $|T| \geq c$, where the constant $c$ is selected so that $\alpha_{2}=P\left(|T| \geq c ; \theta_{1}=\theta_{2}, \theta_{3}=\theta_{4}\right)$ is the assigned significance level of the test. We shall show that, if $\theta_{3}=\theta_{4}$, the likelihood ratio test statistics for equality of variances and equality of means, respectively $F$ and $T$, are independent. Among other things, this means that if these two tests based on $F$ and $T$, respectively, are performed sequentially with significance levels $\alpha_{1}$ and $\alpha_{2}$, the probability of accepting both these hypotheses, when they are true, is $\left(1-\alpha_{1}\right)\left(1-\alpha_{2}\right)$. Thus the significance level of this joint test is $\alpha=1-\left(1-\alpha_{1}\right)\left(1-\alpha_{2}\right)$.

Independence of $F$ and $T$, when $\theta_{3}=\theta_{4}$, can be established using sufficiency and completeness. The statistics $\bar{X}, \bar{Y}$, and $\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}$ are joint complete sufficient statistics for the three parameters $\theta_{1}, \theta_{2}$, and $\theta_{3}=\theta_{4}$. Obviously, the distribution of $F$ does not depend upon $\theta_{1}, \theta_{2}$, or $\theta_{3}=\theta_{4}$, and hence $F$ is independent of the three joint complete sufficient statistics. However, $T$ is a function of these three joint complete sufficient statistics alone, and, accordingly, $T$ is independent of $F$. It is important to note that these two statistics are independent whether $\theta_{1}=\theta_{2}$ or $\theta_{1} \neq \theta_{2}$. This permits us to calculate probabilities other than the significance level of the test. For example, if $\theta_{3}=\theta_{4}$ and $\theta_{1} \neq \theta_{2}$, then

$$
P\left(c_{1}<F<c_{2},|T| \geq c\right)=P\left(c_{1}<F<c_{2}\right) P(|T| \geq c) .
$$

The second factor in the right-hand member is evaluated by using the probabilities of a noncentral $t$-distribution. Of course, if $\theta_{3}=\theta_{4}$ and the difference $\theta_{1}-\theta_{2}$ is large, we would want the preceding probability to be close to 1 because the event $\left\{c_{1}<F<c_{2},|T| \geq c\right\}$ leads to a correct decision, namely, accept $\theta_{3}=\theta_{4}$ and reject $\theta_{1}=\theta_{2}$.

\section*{EXERCISES}
8.3.1. Verzani (2014) discusses a data set on healthy individuals, including their temperatures by gender. The data are in the file tempbygender.rda and the variables of interest are maletemp and femaletemp. Download this file from the site listed in the Preface.\\
(a) Obtain comparison boxplots. Comment on the plots. Which, if any, gender seems to have lower temperatures? Based on the width of the boxplots, comment on the assumption of equal variances.\\
(b) As discussed in Example 8.3.3, compute the two-sample, two-sided $t$-test that there is no difference in the true mean temperatures between genders. Obtain the $p$-value of the test and conclude in terms of the problem at the nominal $\alpha$-level of 0.05 .\\
(c) Obtain a $95 \%$ confidence interval for the difference in means. What does it mean in terms of the problem?\\
8.3.2. Verify Equations (8.3.2) of Example 8.3.1 of this section.\\
8.3.3. Verify Equations (8.3.3) of Example 8.3 .1 of this section.\\
8.3.4. Let $X_{1}, \ldots, X_{n}$ and $Y_{1}, \ldots, Y_{m}$ follow the location model

$$
\begin{aligned}
X_{i} & =\theta_{1}+Z_{i}, \quad i=1, \ldots, n \\
Y_{i} & =\theta_{2}+Z_{n+i}, \quad i=1, \ldots, m
\end{aligned}
$$

where $Z_{1}, \ldots, Z_{n+m}$ are iid random variables with common pdf $f(z)$. Assume that $E\left(Z_{i}\right)=0$ and $\operatorname{Var}\left(Z_{i}\right)=\theta_{3}<\infty$.\\
(a) Show that $E\left(X_{i}\right)=\theta_{1}, E\left(Y_{i}\right)=\theta_{2}$, and $\operatorname{Var}\left(X_{i}\right)=\operatorname{Var}\left(Y_{i}\right)=\theta_{3}$.\\
(b) Consider the hypotheses of Example 8.3.1, i.e.,

$$
H_{0}: \theta_{1}=\theta_{2} \text { versus } H_{1}: \theta_{1} \neq \theta_{2}
$$

Show that under $H_{0}$, the test statistic $T$ given in expression (8.3.4) has a limiting $N(0,1)$ distribution.\\
(c) Using part (b), determine the corresponding large sample test (decision rule) of $H_{0}$ versus $H_{1}$. (This shows that the test in Example 8.3.1 is asymptotically correct.)\\
8.3.5. In Example 8.3.2, the power function for the one-sample $t$-test is discussed.\\
(a) Plot the power function for the following setup: $X$ has a $N\left(\mu, \sigma^{2}\right)$ distribution; $H_{0}: \mu=50$ versus $H_{1}: \mu \neq 50 ; \alpha=0.05 ; n=25$; and $\sigma=10$.\\
(b) Overlay the power curve in (a) with that for $\alpha=0.01$. Comment.\\
(c) Overlay the power curve in (a) with that for $n=35$. Comment.\\
(d) Determine the smallest value of $n$ so the power exceeds 0.80 to detect $\mu=53$. Hint: Modify the R function tpowerg. R so it returns the power for a specified alternative.\\
8.3.6. The effect that a certain drug (Drug A) has on increasing blood pressure is a major concern. It is thought that a modification of the drug (Drug B) will lessen the increase in blood pressure. Let $\mu_{A}$ and $\mu_{B}$ be the true mean increases in blood pressure due to Drug A and B, respectively. The hypotheses of interest are $H_{0}: \mu_{A}=\mu_{B}=0$ versus $H_{1}: \mu_{A}>\mu_{B}=0$. The two-sample $t$-test statistic discussed in Example 8.3.3 is to be used to conduct the analysis. The nominal level is set at $\alpha=0.05$ For the experimental design assume that the sample sizes are the same; i.e., $m=n$. Also, based on data from Drug A, $\sigma=30$ seems to be a reasonable selection for the common standard deviation. Determine the common sample size, so that the difference in means $\mu_{A}-\mu_{B}=12$ has an $80 \%$ detection rate. Suppose when the experiment is over, due to patients dropping out, the sample sizes for Drugs A and B are respectively $n=72$ and $m=68$. What was the actual power of the experiment to detect the difference of 12 ?\\
8.3.7. Show that the likelihood ratio principle leads to the same test when testing a simple hypothesis $H_{0}$ against an alternative simple hypothesis $H_{1}$, as that given by the Neyman-Pearson theorem. Note that there are only two points in $\Omega$.\\
8.3.8. Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from the normal distribution $N(\theta, 1)$. Show that the likelihood ratio principle for testing $H_{0}: \theta=\theta^{\prime}$, where $\theta^{\prime}$ is specified, against $H_{1}: \theta \neq \theta^{\prime}$ leads to the inequality $\left|\bar{x}-\theta^{\prime}\right| \geq c$.\\
(a) Is this a uniformly most powerful test of $H_{0}$ against $H_{1}$ ?\\
(b) Is this a uniformly most powerful unbiased test of $H_{0}$ against $H_{1}$ ?\\
8.3.9. Let $X_{1}, X_{2}, \ldots, X_{n}$ be iid $N\left(\theta_{1}, \theta_{2}\right)$. Show that the likelihood ratio principle for testing $H_{0}: \theta_{2}=\theta_{2}^{\prime}$ specified, and $\theta_{1}$ unspecified, against $H_{1}: \theta_{2} \neq \theta_{2}^{\prime}$, $\theta_{1}$ unspecified, leads to a test that rejects when $\sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2} \leq c_{1}$ or $\sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2} \geq c_{2}$, where $c_{1}<c_{2}$ are selected appropriately.\\
8.3.10. For the situation discussed in Example 8.3.5, derive the power function for the likelihood ratio test statistic given in expression (8.3.9).\\
8.3.11. Let $X_{1}, \ldots, X_{n}$ and $Y_{1}, \ldots, Y_{m}$ be independent random samples from the distributions $N\left(\theta_{1}, \theta_{3}\right)$ and $N\left(\theta_{2}, \theta_{4}\right)$, respectively.\\
(a) Show that the likelihood ratio for testing $H_{0}: \theta_{1}=\theta_{2}, \theta_{3}=\theta_{4}$ against all alternatives is given by

$$
\frac{\left[\sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2} / n\right]^{n / 2}\left[\sum_{1}^{m}\left(y_{i}-\bar{y}\right)^{2} / m\right]^{m / 2}}{\left\{\left[\sum_{1}^{n}\left(x_{i}-u\right)^{2}+\sum_{1}^{m}\left(y_{i}-u\right)^{2}\right] /(m+n)\right\}^{(n+m) / 2}}
$$

where $u=(n \bar{x}+m \bar{y}) /(n+m)$.\\
(b) Show that the likelihood ratio for testing $H_{0}: \theta_{3}=\theta_{4}$ with $\theta_{1}$ and $\theta_{2}$ unspecified can be based on the test statistic $F$ given in expression (8.3.9).\\
8.3.12. Let $Y_{1}<Y_{2}<\cdots<Y_{5}$ be the order statistics of a random sample of size $n=5$ from a distribution with pdf $f(x ; \theta)=\frac{1}{2} e^{-|x-\theta|},-\infty<x<\infty$, for all real $\theta$. Find the likelihood ratio test $\Lambda$ for testing $H_{0}: \theta=\theta_{0}$ against $H_{1}: \theta \neq \theta_{0}$.\\
8.3.13. A random sample $X_{1}, X_{2}, \ldots, X_{n}$ arises from a distribution given by

$$
H_{0}: f(x ; \theta)=\frac{1}{\theta}, \quad 0<x<\theta, \quad \text { zero elsewhere }
$$

or

$$
H_{1}: f(x ; \theta)=\frac{1}{\theta} e^{-x / \theta}, \quad 0<x<\infty, \quad \text { zero elsewhere. }
$$

Determine the likelihood ratio ( $\Lambda$ ) test associated with the test of $H_{0}$ against $H_{1}$.\\
8.3.14. Consider a random sample $X_{1}, X_{2}, \ldots, X_{n}$ from a distribution with pdf $f(x ; \theta)=\theta(1-x)^{\theta-1}, 0<x<1$, zero elsewhere, where $\theta>0$.\\
(a) Find the form of the uniformly most powerful test of $H_{0}: \theta=1$ against $H_{1}: \theta>1$.\\
(b) What is the likelihood ratio $\Lambda$ for testing $H_{0}: \theta=1$ against $H_{1}: \theta \neq 1$ ?\\
8.3.15. Let $X_{1}, X_{2}, \ldots, X_{n}$ and $Y_{1}, Y_{2}, \ldots, Y_{n}$ be independent random samples from two normal distributions $N\left(\mu_{1}, \sigma^{2}\right)$ and $N\left(\mu_{2}, \sigma^{2}\right)$, respectively, where $\sigma^{2}$ is the common but unknown variance.\\
(a) Find the likelihood ratio $\Lambda$ for testing $H_{0}: \mu_{1}=\mu_{2}=0$ against all alternatives.\\
(b) Rewrite $\Lambda$ so that it is a function of a statistic $Z$ which has a well-known distribution.\\
(c) Give the distribution of $Z$ under both null and alternative hypotheses.\\
8.3.16. Let $\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \ldots,\left(X_{n}, Y_{n}\right)$ be a random sample from a bivariate normal distribution with $\mu_{1}, \mu_{2}, \sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}, \rho=\frac{1}{2}$, where $\mu_{1}, \mu_{2}$, and $\sigma^{2}>0$ are unknown real numbers. Find the likelihood ratio $\Lambda$ for testing $H_{0}: \mu_{1}=\mu_{2}=0, \sigma^{2}$ unknown against all alternatives. The likelihood ratio $\Lambda$ is a function of what statistic that has a well-known distribution?\\
8.3.17. Let $X$ be a random variable with pdf $f_{X}(x)=\left(2 b_{X}\right)^{-1} \exp \left\{-|x| / b_{X}\right\}$, for $-\infty<x<\infty$ and $b_{X}>0$. First, show that the variance of $X$ is $\sigma_{X}^{2}=2 b_{X}^{2}$. Next, let $Y$, independent of $X$, have pdf $f_{Y}(y)=\left(2 b_{Y}\right)^{-1} \exp \left\{-|y| / b_{Y}\right\}$, for $-\infty<x<\infty$ and $b_{Y}>0$. Consider the hypotheses

$$
H_{0}: \sigma_{X}^{2}=\sigma_{Y}^{2} \text { versus } H_{1}: \sigma_{X}^{2}>\sigma_{Y}^{2} .
$$

To illustrate Remark 8.3.2 for testing these hypotheses, consider the following data set (data are also in the file exercise8316.rda). Sample 1 represents the values of a sample drawn on $X$ with $b_{X}=1$, while Sample 2 represents the values of a sample drawn on $Y$ with $b_{Y}=1$. Hence, in this case $H_{0}$ is true.

\begin{center}
\begin{tabular}{|c|rrrr|}
\hline
Sample & -0.389 & -2.177 & 0.813 & -0.001 \\
1 & -0.110 & -0.709 & 0.456 & 0.135 \\
\hline
Sample & 0.763 & -0.570 & -2.565 & -1.733 \\
1 & 0.403 & 0.778 & -0.115 &  \\
\hline
Sample & -1.067 & -0.577 & 0.361 & -0.680 \\
2 & -0.634 & -0.996 & -0.181 & 0.239 \\
\hline
Sample & -0.775 & -1.421 & -0.818 & 0.328 \\
2 & 0.213 & 1.425 & -0.165 &  \\
\hline
\end{tabular}
\end{center}

(a) Obtain comparison boxplots of these two samples. Comparison boxplots consist of boxplots of both samples drawn on the same scale. Based on these plots, in particular the interquartile ranges, what do you conclude about $H_{0}$ ?\\
(b) Obtain the $F$-test (for a one-sided hypothesis) as discussed in Remark 8.3.2 at level $\alpha=0.10$. What is your conclusion?\\
(c) The test in part (b) is not exact. Why?\\
8.3.18. For the skewed contaminated normal random variable $X$ of Example 8.3.4, derive the cdf, pdf, mean, and variance of $X$.\\
8.3.19. For Table 8.3.1 of Example 8.3.4, show that the half-width of the $95 \%$ confidence interval for a binomial proportion as given in Chapter 4 is 0.004 at the nominal value of 0.05.\\
8.3.20. If computational facilities are available, perform a Monte Carlo study of the two-sided $t$-test for the skewed contaminated normal situation of Example 8.3.4. The R function rscn. R generates variates from the distribution of $X$.\\
8.3.21. Suppose $X_{1}, \ldots, X_{n}$ is a random sample on $X$ which has a $N\left(\mu, \sigma_{0}^{2}\right)$ distribution, where $\sigma_{0}^{2}$ is known. Consider the two-sided hypotheses

$$
H_{0}: \mu=0 \text { versus } H_{1}: \mu \neq 0 .
$$

Show that the test based on the critical region $C=\left\{|\bar{X}|>\sqrt{\sigma_{0}^{2} / n} z_{\alpha / 2}\right\}$ is an unbiased level $\alpha$ test.\\
8.3.22. Assume the same situation as in the last exercise but consider the test with critical region $C^{*}=\left\{\bar{X}>\sqrt{\sigma_{0}^{2} / n} z_{\alpha}\right\}$. Show that the test based on $C^{*}$ has significance level $\alpha$ but that it is not an unbiased test.

\section*{8.4 *The Sequential Probability Ratio Test}
Theorem 8.1.1 provides us with a method for determining a best critical region for testing a simple hypothesis against an alternative simple hypothesis. Recall its statement: Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample with fixed sample size $n$ from a distribution that has pdf or $\operatorname{pmf} f(x ; \theta)$, where $\theta=\left\{\theta: \theta=\theta^{\prime}, \theta^{\prime \prime}\right\}$ and $\theta^{\prime}$ and $\theta^{\prime \prime}$\\
are known numbers. For this section, we denote the likelihood of $X_{1}, X_{2}, \ldots, X_{n}$ by

$$
L(\theta ; n)=f\left(x_{1} ; \theta\right) f\left(x_{2} ; \theta\right) \cdots f\left(x_{n} ; \theta\right)
$$

a notation that reveals both the parameter $\theta$ and the sample size $n$. If we reject $H_{0}: \theta=\theta^{\prime}$ and accept $H_{1}: \theta=\theta^{\prime \prime}$ when and only when

$$
\frac{L\left(\theta^{\prime} ; n\right)}{L\left(\theta^{\prime \prime} ; n\right)} \leq k
$$

where $k>0$, then by Theorem 8.1.1 this is a best test of $H_{0}$ against $H_{1}$.\\
Let us now suppose that the sample size $n$ is not fixed in advance. In fact, let the sample size be a random variable $N$ with sample space $\{1,2,, 3, \ldots\}$. An interesting procedure for testing the simple hypothesis $H_{0}: \theta=\theta^{\prime}$ against the simple hypothesis $H_{1}: \theta=\theta^{\prime \prime}$ is the following: Let $k_{0}$ and $k_{1}$ be two positive constants with $k_{0}<k_{1}$. Observe the independent outcomes $X_{1}, X_{2}, X_{3}, \ldots$ in a sequence, for example, $x_{1}, x_{2}, x_{3}, \ldots$, and compute

$$
\frac{L\left(\theta^{\prime} ; 1\right)}{L\left(\theta^{\prime \prime} ; 1\right)}, \frac{L\left(\theta^{\prime} ; 2\right)}{L\left(\theta^{\prime \prime} ; 2\right)}, \frac{L\left(\theta^{\prime} ; 3\right)}{L\left(\theta^{\prime \prime} ; 3\right)}, \ldots
$$

The hypothesis $H_{0}: \theta=\theta^{\prime}$ is rejected (and $H_{1}: \theta=\theta^{\prime \prime}$ is accepted) if and only if there exists a positive integer $n$ so that $\mathbf{x}_{n}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ belongs to the set


\begin{equation*}
C_{n}=\left\{\mathbf{x}_{n}: k_{0}<\frac{L\left(\theta^{\prime}, j\right)}{L\left(\theta^{\prime \prime}, j\right)}<k_{1}, j=1, \ldots, n-1, \text { and } \frac{L\left(\theta^{\prime}, n\right)}{L\left(\theta^{\prime \prime}, n\right)} \leq k_{0}\right\} \tag{8.4.1}
\end{equation*}


On the other hand, the hypothesis $H_{0}: \theta=\theta^{\prime}$ is accepted (and $H_{1}: \theta=\theta^{\prime \prime}$ is rejected) if and only if there exists a positive integer $n$ so that $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ belongs to the set


\begin{equation*}
B_{n}=\left\{\mathbf{x}_{n}: k_{0}<\frac{L\left(\theta^{\prime}, j\right)}{L\left(\theta^{\prime \prime}, j\right)}<k_{1}, j=1, \ldots, n-1, \text { and } \frac{L\left(\theta^{\prime}, n\right)}{L\left(\theta^{\prime \prime}, n\right)} \geq k_{1}\right\} \tag{8.4.2}
\end{equation*}


That is, we continue to observe sample observations as long as


\begin{equation*}
k_{0}<\frac{L\left(\theta^{\prime}, n\right)}{L\left(\theta^{\prime \prime}, n\right)}<k_{1} \tag{8.4.3}
\end{equation*}


We stop these observations in one of two ways:

\begin{enumerate}
  \item With rejection of $H_{0}: \theta=\theta^{\prime}$ as soon as
\end{enumerate}

$$
\frac{L\left(\theta^{\prime}, n\right)}{L\left(\theta^{\prime \prime}, n\right)} \leq k_{0}
$$

or\\
2. With acceptance of $H_{0}: \theta=\theta^{\prime}$ as soon as

$$
\frac{L\left(\theta^{\prime}, n\right)}{L\left(\theta^{\prime \prime}, n\right)} \geq k_{1}
$$

A test of this kind is called Wald's sequential probability ratio test. Frequently, inequality (8.4.3) can be conveniently expressed in an equivalent form:


\begin{equation*}
c_{0}(n)<u\left(x_{1}, x_{2}, \ldots, x_{n}\right)<c_{1}(n), \tag{8.4.4}
\end{equation*}


where $u\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ is a statistic and $c_{0}(n)$ and $c_{1}(n)$ depend on the constants $k_{0}, k_{1}, \theta^{\prime}, \theta^{\prime \prime}$, and on $n$. Then the observations are stopped and a decision is reached as soon as

$$
u\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq c_{0}(n) \quad \text { or } \quad u\left(x_{1}, x_{2}, \ldots, x_{n}\right) \geq c_{1}(n) .
$$

We now give an illustrative example.\\
Example 8.4.1. Let $X$ have a pmf

$$
f(x ; \theta)= \begin{cases}\theta^{x}(1-\theta)^{1-x} & x=0,1 \\ 0 & \text { elsewhere }\end{cases}
$$

In the preceding discussion of a sequential probability ratio test, let $H_{0}: \theta=\frac{1}{3}$ and $H_{1}: \theta=\frac{2}{3}$; then, with $\sum x_{i}=\sum_{i=1}^{n} x_{i}$,

$$
\frac{L\left(\frac{1}{3}, n\right)}{L\left(\frac{2}{3}, n\right)}=\frac{\left(\frac{1}{3}\right)^{\sum x_{i}}\left(\frac{2}{3}\right)^{n-\sum x_{i}}}{\left(\frac{2}{3}\right)^{\sum x_{i}}\left(\frac{1}{3}\right)^{n-\sum x_{i}}}=2^{n-2 \sum x_{i}} .
$$

If we take logarithms to the base 2 , the inequality

$$
k_{0}<\frac{L\left(\frac{1}{3}, n\right)}{L\left(\frac{2}{3}, n\right)}<k_{1},
$$

with $0<k_{0}<k_{1}$, becomes

$$
\log _{2} k_{0}<n-2 \sum_{1}^{n} x_{i}<\log _{2} k_{1}
$$

or, equivalently, in the notation of expression (8.4.4),

$$
c_{0}(n)=\frac{n}{2}-\frac{1}{2} \log _{2} k_{1}<\sum_{1}^{n} x_{i}<\frac{n}{2}-\frac{1}{2} \log _{2} k_{0}=c_{1}(n) .
$$

Note that $L\left(\frac{1}{3}, n\right) / L\left(\frac{2}{3}, n\right) \leq k_{0}$ if and only if $c_{1}(n) \leq \sum_{1}^{n} x_{i}$; and $L\left(\frac{1}{3}, n\right) / L\left(\frac{2}{3}, n\right) \geq$ $k_{1}$ if and only if $c_{0}(n) \geq \sum_{1}^{n} x_{i}$. Thus we continue to observe outcomes as long as $c_{0}(n)<\sum_{1}^{n} x_{i}<c_{1}(n)$. The observation of outcomes is discontinued with the first value of $n$ of $N$ for which either $c_{1}(n) \leq \sum_{1}^{n} x_{i}$ or $c_{0}(n) \geq \sum_{1}^{n} x_{i}$. The inequality $c_{1}(n) \leq \sum_{1}^{n} x_{i}$ leads to rejection of $H_{0}: \theta=\frac{1}{3}$ (the acceptance of $H_{1}$ ), and the inequality $c_{0}(n) \geq \sum_{1}^{n} x_{i}$ leads to the acceptance of $H_{0}: \theta=\frac{1}{3}$ (the rejection of $H_{1}$ ).

Remark 8.4.1. At this point, the reader undoubtedly sees that there are many questions that should be raised in connection with the sequential probability ratio test. Some of these questions are possibly among the following:

\begin{enumerate}
  \item What is the probability of the procedure continuing indefinitely?
  \item What is the value of the power function of this test at each of the points $\theta=\theta^{\prime}$ and $\theta=\theta^{\prime \prime}$ ?
  \item If $\theta^{\prime \prime}$ is one of several values of $\theta$ specified by an alternative composite hypothesis, say $H_{1}: \theta>\theta^{\prime}$, what is the power function at each point $\theta \geq \theta^{\prime}$ ?
  \item Since the sample size $N$ is a random variable, what are some of the properties of the distribution of $N$ ? In particular, what is the expected value $E(N)$ of $N$ ?
  \item How does this test compare with tests that have a fixed sample size $n$ ?
\end{enumerate}

A course in sequential analysis would investigate these and many other problems. However, in this book our objective is largely that of acquainting the reader with this kind of test procedure. Accordingly, we assert that the answer to question 1 is zero. Moreover, it can be proved that if $\theta=\theta^{\prime}$ or if $\theta=\theta^{\prime \prime}, E(N)$ is smaller for this sequential procedure than the sample size of a fixed-sample-size test that has the same values of the power function at those points. We now consider question 2 in some detail.

In this section we shall denote the power of the test when $H_{0}$ is true by the symbol $\alpha$ and the power of the test when $H_{1}$ is true by the symbol $1-\beta$. Thus $\alpha$ is the probability of committing a Type I error (the rejection of $H_{0}$ when $H_{0}$ is true), and $\beta$ is the probability of committing a Type II error (the acceptance of $H_{0}$ when $H_{0}$ is false). With the sets $C_{n}$ and $B_{n}$ as previously defined, and with random variables of the continuous type, we then have

$$
\alpha=\sum_{n=1}^{\infty} \int_{C_{n}} L\left(\theta^{\prime}, n\right), \quad 1-\beta=\sum_{n=1}^{\infty} \int_{C_{n}} L\left(\theta^{\prime \prime}, n\right) .
$$

Since the probability is 1 that the procedure terminates, we also have

$$
1-\alpha=\sum_{n=1}^{\infty} \int_{B_{n}} L\left(\theta^{\prime}, n\right), \quad \beta=\sum_{n=1}^{\infty} \int_{B_{n}} L\left(\theta^{\prime \prime}, n\right) .
$$

If $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C_{n}$, we have $L\left(\theta^{\prime}, n\right) \leq k_{0} L\left(\theta^{\prime \prime}, n\right)$; hence, it is clear that

$$
\alpha=\sum_{n=1}^{\infty} \int_{C_{n}} L\left(\theta^{\prime}, n\right) \leq \sum_{n=1}^{\infty} \int_{C_{n}} k_{0} L\left(\theta^{\prime \prime}, n\right)=k_{0}(1-\beta) .
$$

Because $L\left(\theta^{\prime}, n\right) \geq k_{1} L\left(\theta^{\prime \prime}, n\right)$ at each point of the set $B_{n}$, we have

$$
1-\alpha=\sum_{n=1}^{\infty} \int_{B_{n}} L\left(\theta^{\prime}, n\right) \geq \sum_{n=1}^{\infty} \int_{B_{n}} k_{1} L\left(\theta^{\prime \prime}, n\right)=k_{1} \beta .
$$

Accordingly, it follows that


\begin{equation*}
\frac{\alpha}{1-\beta} \leq k_{0}, \quad k_{1} \leq \frac{1-\alpha}{\beta}, \tag{8.4.5}
\end{equation*}


provided that $\beta$ is not equal to 0 or 1 .\\
Now let $\alpha_{a}$ and $\beta_{a}$ be preassigned proper fractions; some typical values in the applications are $0.01,0.05$, and 0.10 . If we take

$$
k_{0}=\frac{\alpha_{a}}{1-\beta_{a}}, \quad k_{1}=\frac{1-\alpha_{a}}{\beta_{a}},
$$

then inequalities (8.4.5) become


\begin{equation*}
\frac{\alpha}{1-\beta} \leq \frac{\alpha_{a}}{1-\beta_{a}}, \quad \frac{1-\alpha_{a}}{\beta_{a}} \leq \frac{1-\alpha}{\beta} \tag{8.4.6}
\end{equation*}


or, equivalently,

$$
\alpha\left(1-\beta_{a}\right) \leq(1-\beta) \alpha_{a}, \quad \beta\left(1-\alpha_{a}\right) \leq(1-\alpha) \beta_{a} .
$$

If we add corresponding members of the immediately preceding inequalities, we find that

$$
\alpha+\beta-\alpha \beta_{a}-\beta \alpha_{a} \leq \alpha_{a}+\beta_{a}-\beta \alpha_{a}-\alpha \beta_{a}
$$

and hence

$$
\alpha+\beta \leq \alpha_{a}+\beta_{a} ;
$$

that is, the sum $\alpha+\beta$ of the probabilities of the two kinds of errors is bounded above by the sum $\alpha_{a}+\beta_{a}$ of the preassigned numbers. Moreover, since $\alpha$ and $\beta$ are positive proper fractions, inequalities (8.4.6) imply that

$$
\alpha \leq \frac{\alpha_{a}}{1-\beta_{a}}, \quad \beta \leq \frac{\beta_{a}}{1-\alpha_{a}}
$$

consequently, we have an upper bound on each of $\alpha$ and $\beta$. Various investigations of the sequential probability ratio test seem to indicate that in most practical cases, the values of $\alpha$ and $\beta$ are quite close to $\alpha_{a}$ and $\beta_{a}$. This prompts us to approximate the power function at the points $\theta=\theta^{\prime}$ and $\theta=\theta^{\prime \prime}$ by $\alpha_{a}$ and $1-\beta_{a}$, respectively.

Example 8.4.2. Let $X$ be $N(\theta, 100)$. To find the sequential probability ratio test for testing $H_{0}: \theta=75$ against $H_{1}: \theta=78$ such that each of $\alpha$ and $\beta$ is approximately equal to 0.10 , take

$$
k_{0}=\frac{0.10}{1-0.10}=\frac{1}{9}, \quad k_{1}=\frac{1-0.10}{0.10}=9 .
$$

Since

$$
\frac{L(75, n)}{L(78, n)}=\frac{\exp \left[-\sum\left(x_{i}-75\right)^{2} / 2(100)\right]}{\exp \left[-\sum\left(x_{i}-78\right)^{2} / 2(100)\right]}=\exp \left(-\frac{6 \sum x_{i}-459 n}{200}\right),
$$

the inequality

$$
k_{0}=\frac{1}{9}<\frac{L(75, n)}{L(78, n)}<9=k_{1}
$$

can be rewritten, by taking logarithms, as

$$
-\log 9<\frac{6 \sum x_{i}-459 n}{200}<\log 9
$$

This inequality is equivalent to the inequality

$$
c_{0}(n)=\frac{153}{2} n-\frac{100}{3} \log 9<\sum_{1}^{n} x_{i}<\frac{153}{2} n+\frac{100}{3} \log 9=c_{1}(n)
$$

Moreover, $L(75, n) / L(78, n) \leq k_{0}$ and $L(75, n) / L(78, n) \geq k_{1}$ are equivalent to the inequalities $\sum_{1}^{n} x_{i} \geq c_{1}(n)$ and $\sum_{1}^{n} x_{i} \leq c_{0}(n)$, respectively. Thus the observation of outcomes is discontinued with the first value of $n$ of $N$ for which either $\sum_{1}^{n} x_{i} \geq$ $c_{1}(n)$ or $\sum_{1}^{n} x_{i} \leq c_{0}(n)$. The inequality $\sum_{1}^{n} x_{i} \geq c_{1}(n)$ leads to the rejection of $H_{0}: \theta=75$, and the inequality $\sum_{1}^{n} x_{i} \leq c_{0}(n)$ leads to the acceptance of $H_{0}: \theta=75$. The power of the test is approximately 0.10 when $H_{0}$ is true, and approximately 0.90 when $H_{1}$ is true.

Remark 8.4.2. It is interesting to note that a sequential probability ratio test can be thought of as a random-walk procedure. To illustrate, the final inequalities of Examples 8.4.1 and 8.4.2 can be written as

$$
-\log _{2} k_{1}<\sum_{1}^{n} 2\left(x_{i}-0.5\right)<-\log _{2} k_{0}
$$

and

$$
-\frac{100}{3} \log 9<\sum_{1}^{n}\left(x_{i}-76.5\right)<\frac{100}{3} \log 9
$$

respectively. In each instance, think of starting at the point zero and taking random steps until one of the boundaries is reached. In the first situation the random steps are $2\left(X_{1}-0.5\right), 2\left(X_{2}-0.5\right), 2\left(X_{3}-0.5\right), \ldots$, which have the same length, 1 , but with random directions. In the second instance, both the length and the direction of the steps are random variables, $X_{1}-76.5, X_{2}-76.5, X_{3}-76.5, \ldots$.

In recent years, there has been much attention devoted to improving quality of products using statistical methods. One such simple method was developed by Walter Shewhart in which a sample of size $n$ of the items being produced is taken and they are measured, resulting in $n$ values. The mean $\bar{X}$ of these $n$ measurements has an approximate normal distribution with mean $\mu$ and variance $\sigma^{2} / n$. In practice, $\mu$ and $\sigma^{2}$ must be estimated, but in this discussion, we assume that they are known. From theory we know that the probability is 0.997 that $\bar{x}$ is between

$$
\mathrm{LCL}=\mu-\frac{3 \sigma}{\sqrt{n}} \quad \text { and } \quad \mathrm{UCL}=\mu+\frac{3 \sigma}{\sqrt{n}}
$$

These two values are called the lower (LCL) and upper (UCL) control limits, respectively. Samples like these are taken periodically, resulting in a sequence of means,\\
say $\bar{x}_{1}, \bar{x}_{2}, \bar{x}_{3}, \ldots$. These are usually plotted; and if they are between the LCL and UCL, we say that the process is in control. If one falls outside the limits, this would suggest that the mean $\mu$ has shifted, and the process would be investigated.

It was recognized by some that there could be a shift in the mean, say from $\mu$ to $\mu+(\sigma / \sqrt{n})$; and it would still be difficult to detect that shift with a single sample mean, for now the probability of a single $\bar{x}$ exceeding UCL is only about 0.023 . This means that we would need about $1 / 0.023 \approx 43$ samples, each of size $n$, on the average before detecting such a shift. This seems too long; so statisticians recognized that they should be cumulating experience as the sequence $\bar{X}_{1}, \bar{X}_{2}, \bar{X}_{3}, \ldots$ is observed in order to help them detect the shift sooner. It is the practice to compute the standardized variable $Z=(\bar{X}-\mu) /(\sigma / \sqrt{n})$; thus, we state the problem in these terms and provide the solution given by a sequential probability ratio test.

Here $Z$ is $N(\theta, 1)$, and we wish to test $H_{0}: \theta=0$ against $H_{1}: \theta=1$ using the sequence of iid random variables $Z_{1}, Z_{2}, \ldots, Z_{m}, \ldots$. We use $m$ rather than $n$, as the latter is the size of the samples taken periodically. We have

$$
\frac{L(0, m)}{L(1, m)}=\frac{\exp \left[-\sum z_{i}^{2} / 2\right]}{\exp \left[-\sum\left(z_{i}-1\right)^{2} / 2\right]}=\exp \left[-\sum_{i=1}^{m}\left(z_{i}-0.5\right)\right]
$$

Thus

$$
k_{0}<\exp \left[-\sum_{i=1}^{m}\left(z_{i}-0.5\right)\right]<k_{1}
$$

can be written as

$$
h=-\log k_{0}>\sum_{i=1}^{m}\left(z_{i}-0.5\right)>-\log k_{1}=-h .
$$

It is true that $-\log k_{0}=\log k_{1}$ when $\alpha_{a}=\beta_{a}$. Often, $h=-\log k_{0}$ is taken to be about 4 or 5 , suggesting that $\alpha_{a}=\beta_{a}$ is small, like 0.01 . As $\sum\left(z_{i}-0.5\right)$ is cumulating the sum of $z_{i}-0.5, i=1,2,3, \ldots$, these procedures are often called CUSUMS. If the CUSUM $=\sum\left(z_{i}-0.5\right)$ exceeds $h$, we would investigate the process, as it seems that the mean has shifted upward. If this shift is to $\theta=1$, the theory associated with these procedures shows that we need only eight or nine samples on the average, rather than 43, to detect this shift. For more information about these methods, the reader is referred to one of the many books on quality improvement through statistical methods. What we would like to emphasize here is that through sequential methods (not only the sequential probability ratio test), we should take advantage of all past experience that we can gather in making inferences.

\section*{EXERCISES}
8.4.1. Let $X$ be $N(0, \theta)$ and, in the notation of this section, let $\theta^{\prime}=4, \theta^{\prime \prime}=9$, $\alpha_{a}=0.05$, and $\beta_{a}=0.10$. Show that the sequential probability ratio test can be based upon the statistic $\sum_{1}^{n} X_{i}^{2}$. Determine $c_{0}(n)$ and $c_{1}(n)$.\\
8.4.2. Let $X$ have a Poisson distribution with mean $\theta$. Find the sequential probability ratio test for testing $H_{0}: \theta=0.02$ against $H_{1}: \theta=0.07$. Show that this test can be based upon the statistic $\sum_{1}^{n} X_{i}$. If $\alpha_{a}=0.20$ and $\beta_{a}=0.10$, find $c_{0}(n)$ and $c_{1}(n)$.\\
8.4.3. Let the independent random variables $Y$ and $Z$ be $N\left(\mu_{1}, 1\right)$ and $N\left(\mu_{2}, 1\right)$, respectively. Let $\theta=\mu_{1}-\mu_{2}$. Let us observe independent observations from each distribution, say $Y_{1}, Y_{2}, \ldots$ and $Z_{1}, Z_{2}, \ldots$ To test sequentially the hypothesis $H_{0}: \theta=0$ against $H_{1}: \theta=\frac{1}{2}$, use the sequence $X_{i}=Y_{i}-Z_{i}, i=1,2, \ldots$ If $\alpha_{a}=\beta_{a}=0.05$, show that the test can be based upon $\bar{X}=\bar{Y}-\bar{Z}$. Find $c_{0}(n)$ and $c_{1}(n)$.\\
8.4.4. Suppose that a manufacturing process makes about $3 \%$ defective items, which is considered satisfactory for this particular product. The managers would like to decrease this to about $1 \%$ and clearly want to guard against a substantial increase, say to $5 \%$. To monitor the process, periodically $n=100$ items are taken and the number $X$ of defectives counted. Assume that $X$ is $b(n=100, p=\theta)$. Based on a sequence $X_{1}, X_{2}, \ldots, X_{m}, \ldots$, determine a sequential probability ratio test that tests $H_{0}: \theta=0.01$ against $H_{1}: \theta=0.05$. (Note that $\theta=0.03$, the present level, is in between these two values.) Write this test in the form

$$
h_{0}>\sum_{i=1}^{m}\left(x_{i}-n d\right)>h_{1}
$$

and determine $d, h_{0}$, and $h_{1}$ if $\alpha_{a}=\beta_{a}=0.02$.\\
8.4.5. Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from a distribution with pdf $f(x ; \theta)=$ $\theta x^{\theta-1}, 0<x<1$, zero elsewhere.\\
(a) Find a complete sufficient statistic for $\theta$.\\
(b) If $\alpha_{a}=\beta_{a}=\frac{1}{10}$, find the sequential probability ratio test of $H_{0}: \theta=2$ against $H_{1}: \theta=3$.

\section*{8.5 *Minimax and Classification Procedures}
We have considered several procedures that may be used in problems of point estimation. Among these were decision function procedures (in particular, minimax decisions). In this section, we apply minimax procedures to the problem of testing a simple hypothesis $H_{0}$ against an alternative simple hypothesis $H_{1}$. It is important to observe that these procedures yield, in accordance with the Neyman-Pearson theorem, a best test of $H_{0}$ against $H_{1}$. We end this section with a discussion on an application of these procedures to a classification problem.

\subsection*{8.5.1 Minimax Procedures}
We first investigate the decision function approach to the problem of testing a simple null hypothesis against a simple alternative hypothesis. Let the joint pdf of the $n$\\
random variables $X_{1}, X_{2}, \ldots, X_{n}$ depend upon the parameter $\theta$. Here $n$ is a fixed positive integer. This pdf is denoted by $L\left(\theta ; x_{1}, x_{2}, \ldots, x_{n}\right)$ or, for brevity, by $L(\theta)$. Let $\theta^{\prime}$ and $\theta^{\prime \prime}$ be distinct and fixed values of $\theta$. We wish to test the simple hypothesis $H_{0}: \theta=\theta^{\prime}$ against the simple hypothesis $H_{1}: \theta=\theta^{\prime \prime}$. Thus the parameter space is $\Omega=\left\{\theta: \theta=\theta^{\prime}, \theta^{\prime \prime}\right\}$. In accordance with the decision function procedure, we need a function $\delta$ of the observed values of $X_{1}, \ldots, X_{n}$ (or, of the observed value of a statistic $Y$ ) that decides which of the two values of $\theta, \theta^{\prime}$ or $\theta^{\prime \prime}$, to accept. That is, the function $\delta$ selects either $H_{0}: \theta=\theta^{\prime}$ or $H_{1}: \theta=\theta^{\prime \prime}$. We denote these decisions by $\delta=\theta^{\prime}$ and $\delta=\theta^{\prime \prime}$, respectively. Let $\mathcal{L}(\theta, \delta)$ represent the loss function associated with this decision problem. Because the pairs $\left(\theta=\theta^{\prime}, \delta=\theta^{\prime}\right)$ and $\left(\theta=\theta^{\prime \prime}, \delta=\theta^{\prime \prime}\right)$ represent correct decisions, we shall always take $\mathcal{L}\left(\theta^{\prime}, \theta^{\prime}\right)=\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime \prime}\right)=0$. On the other hand, if either $\delta=\theta^{\prime \prime}$ when $\theta=\theta^{\prime}$ or $\delta=\theta^{\prime}$ when $\theta=\theta^{\prime \prime}$, then a positive value should be assigned to the loss function; that is, $\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right)>0$ and $\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right)>0$.

It has previously been emphasized that a test of $H_{0}: \theta=\theta^{\prime}$ against $H_{1}: \theta=\theta^{\prime \prime}$ can be described in terms of a critical region in the sample space. We can do the same kind of thing with the decision function. That is, we can choose a subset of $C$ of the sample space and if $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C$, we can make the decision $\delta=\theta^{\prime \prime}$; whereas if $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in C^{c}$, the complement of $C$, we make the decision $\delta=\theta^{\prime}$. Thus a given critical region $C$ determines the decision function. In this sense, we may denote the risk function by $R(\theta, C)$ instead of $R(\theta, \delta)$. That is, in a notation used in Section 7.1,

$$
R(\theta, C)=R(\theta, \delta)=\int_{C \cup C^{c}} \mathcal{L}(\theta, \delta) L(\theta)
$$

Since $\delta=\theta^{\prime \prime}$ if $\left(x_{1}, \ldots, x_{n}\right) \in C$ and $\delta=\theta^{\prime}$ if $\left(x_{1}, \ldots, x_{n}\right) \in C^{c}$, we have


\begin{equation*}
R(\theta, C)=\int_{C} \mathcal{L}\left(\theta, \theta^{\prime \prime}\right) L(\theta)+\int_{C^{c}} \mathcal{L}\left(\theta, \theta^{\prime}\right) L(\theta) \tag{8.5.1}
\end{equation*}


If, in Equation (8.5.1), we take $\theta=\theta^{\prime}$, then $\mathcal{L}\left(\theta^{\prime}, \theta^{\prime}\right)=0$ and hence

$$
R\left(\theta^{\prime}, C\right)=\int_{C} \mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) L\left(\theta^{\prime}\right)=\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \int_{C} L\left(\theta^{\prime}\right)
$$

On the other hand, if in Equation (8.5.1) we let $\theta=\theta^{\prime \prime}$, then $\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime \prime}\right)=0$ and, accordingly,

$$
R\left(\theta^{\prime \prime}, C\right)=\int_{C^{c}} \mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) L\left(\theta^{\prime \prime}\right)=\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) \int_{C^{c}} L\left(\theta^{\prime \prime}\right)
$$

It is enlightening to note that if $\gamma(\theta)$ is the power function of the test associated with the critical region $C$, then

$$
R\left(\theta^{\prime}, C\right)=\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \gamma\left(\theta^{\prime}\right)=\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \alpha
$$

where $\alpha=\gamma\left(\theta^{\prime}\right)$ is the significance level; and

$$
R\left(\theta^{\prime \prime}, C\right)=\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right)\left[1-\gamma\left(\theta^{\prime \prime}\right)\right]=\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) \beta
$$

where $\beta=1-\gamma\left(\theta^{\prime \prime}\right)$ is the probability of the type II error.\\
Let us now see if we can find a minimax solution to our problem. That is, we want to find a critical region $C$ so that

$$
\max \left[R\left(\theta^{\prime}, C\right), R\left(\theta^{\prime \prime}, C\right)\right]
$$

is minimized. We shall show that the solution is the region

$$
C=\left\{\left(x_{1}, \ldots, x_{n}\right): \frac{L\left(\theta^{\prime} ; x_{1}, \ldots, x_{n}\right)}{L\left(\theta^{\prime \prime} ; x_{1}, \ldots, x_{n}\right)} \leq k\right\}
$$

provided the positive constant $k$ is selected so that $R\left(\theta^{\prime}, C\right)=R\left(\theta^{\prime \prime}, C\right)$. That is, if $k$ is chosen so that

$$
\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \int_{C} L\left(\theta^{\prime}\right)=\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) \int_{C^{c}} L\left(\theta^{\prime \prime}\right)
$$

then the critical region $C$ provides a minimax solution. In the case of random variables of the continuous type, $k$ can always be selected so that $R\left(\theta^{\prime}, C\right)=R\left(\theta^{\prime \prime}, C\right)$. However, with random variables of the discrete type, we may need to consider an auxiliary random experiment when $L\left(\theta^{\prime}\right) / L\left(\theta^{\prime \prime}\right)=k$ in order to achieve the exact equality $R\left(\theta^{\prime}, C\right)=R\left(\theta^{\prime \prime}, C\right)$.

To see that $C$ is the minimax solution, consider every other region $A$ for which $R\left(\theta^{\prime}, C\right) \geq R\left(\theta^{\prime}, A\right)$. A region $A$ for which $R\left(\theta^{\prime}, C\right)<R\left(\theta^{\prime}, A\right)$ is not a candidate for a minimax solution, for then $R\left(\theta^{\prime}, C\right)=R\left(\theta^{\prime \prime}, C\right)<\max \left[R\left(\theta^{\prime}, A\right), R\left(\theta^{\prime \prime}, A\right)\right]$. Since $R\left(\theta^{\prime}, C\right) \geq R\left(\theta^{\prime}, A\right)$ means that

$$
\mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \int_{C} L\left(\theta^{\prime}\right) \geq \mathcal{L}\left(\theta^{\prime}, \theta^{\prime \prime}\right) \int_{A} L\left(\theta^{\prime}\right)
$$

we have

$$
\alpha=\int_{C} L\left(\theta^{\prime}\right) \geq \int_{A} L\left(\theta^{\prime}\right)
$$

that is, the significance level of the test associated with the critical region $A$ is less than or equal to $\alpha$. But $C$, in accordance with the Neyman-Pearson theorem, is a best critical region of size $\alpha$. Thus

$$
\int_{C} L\left(\theta^{\prime \prime}\right) \geq \int_{A} L\left(\theta^{\prime \prime}\right)
$$

and

$$
\int_{C^{c}} L\left(\theta^{\prime \prime}\right) \leq \int_{A^{c}} L\left(\theta^{\prime \prime}\right)
$$

Accordingly,

$$
\mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) \int_{C^{c}} L\left(\theta^{\prime \prime}\right) \leq \mathcal{L}\left(\theta^{\prime \prime}, \theta^{\prime}\right) \int_{A^{c}} L\left(\theta^{\prime \prime}\right)
$$

or, equivalently,

$$
R\left(\theta^{\prime \prime}, C\right) \leq R\left(\theta^{\prime \prime}, A\right)
$$

That is,

$$
R\left(\theta^{\prime}, C\right)=R\left(\theta^{\prime \prime}, C\right) \leq R\left(\theta^{\prime \prime}, A\right)
$$

This means that

$$
\max \left[R\left(\theta^{\prime}, C\right), R\left(\theta^{\prime \prime}, C\right)\right] \leq R\left(\theta^{\prime \prime}, A\right)
$$

Then certainly,

$$
\max \left[R\left(\theta^{\prime}, C\right), R\left(\theta^{\prime \prime}, C\right)\right] \leq \max \left[R\left(\theta^{\prime}, A\right), R\left(\theta^{\prime \prime}, A\right)\right]
$$

and the critical region $C$ provides a minimax solution, as we wanted to show.\\
Example 8.5.1. Let $X_{1}, X_{2}, \ldots, X_{100}$ denote a random sample of size 100 from a distribution that is $N(\theta, 100)$. We again consider the problem of testing $H_{0}$ : $\theta=75$ against $H_{1}: \theta=78$. We seek a minimax solution with $\mathcal{L}(75,78)=3$ and $\mathcal{L}(78,75)=1$. Since $L(75) / L(78) \leq k$ is equivalent to $\bar{x} \geq c$, we want to determine $c$, and thus $k$, so that


\begin{equation*}
3 P(\bar{X} \geq c ; \theta=75)=P(\bar{X}<c ; \theta=78) \tag{8.5.2}
\end{equation*}


Because $\bar{X}$ is $N(\theta, 1)$, the preceding equation can be rewritten as

$$
3[1-\Phi(c-75)]=\Phi(c-78) .
$$

As requested in Exercise 8.5.4, the reader can show by using Newton's algorithm that the solution to one place is $c=76.8$. The significance level of the test is $1-\Phi(1.8)=0.036$, approximately, and the power of the test when $H_{1}$ is true is $1-\Phi(-1.2)=0.885$, approximately.

\subsection*{8.5.2 Classification}
The summary above has an interesting application to the problem of classification, which can be described as follows. An investigator makes a number of measurements on an item and wants to place it into one of several categories (or classify it). For convenience in our discussion, we assume that only two measurements, say $X$ and $Y$, are made on the item to be classified. Moreover, let $X$ and $Y$ have a joint pdf $f(x, y ; \theta)$, where the parameter $\theta$ represents one or more parameters. In our simplification, suppose that there are only two possible joint distributions (categories) for $X$ and $Y$, which are indexed by the parameter values $\theta^{\prime}$ and $\theta^{\prime \prime}$, respectively. In this case, the problem then reduces to one of observing $X=x$ and $Y=y$ and then testing the hypothesis $\theta=\theta^{\prime}$ against the hypothesis $\theta=\theta^{\prime \prime}$, with the classification of $X$ and $Y$ being in accord with which hypothesis is accepted. From the Neyman-Pearson theorem, we know that a best decision of this sort is of the following form: If

$$
\frac{f\left(x, y ; \theta^{\prime}\right)}{f\left(x, y ; \theta^{\prime \prime}\right)} \leq k
$$

choose the distribution indexed by $\theta^{\prime \prime}$; that is, we classify $(x, y)$ as coming from the distribution indexed by $\theta^{\prime \prime}$. Otherwise, choose the distribution indexed by $\theta^{\prime}$; that is, we classify $(x, y)$ as coming from the distribution indexed by $\theta^{\prime}$. Some discussion on the choice of $k$ follows in the next remark.

Remark 8.5.1 (On the Choice of $k$ ). Consider the following probabilities:

$$
\begin{aligned}
\pi^{\prime} & =P\left[(X, Y) \text { is drawn from the distribution with pdf } f\left(x, y ; \theta^{\prime}\right)\right] \\
\pi^{\prime \prime} & =P\left[(X, Y) \text { is drawn from the distribution with pdf } f\left(x, y ; \theta^{\prime \prime}\right)\right]
\end{aligned}
$$

Note that $\pi^{\prime}+\pi^{\prime \prime}=1$. Then it can be shown that the optimal classification rule is determined by taking $k=\pi^{\prime \prime} / \pi^{\prime}$; see, for instance, Seber (1984). Hence, if we have prior information on how likely the item is drawn from the distribution with parameter $\theta^{\prime}$, then we can obtain the classification rule. In practice, it is common for each distribution to be equilikely, in which case, $\pi^{\prime}=\pi^{\prime \prime}=1 / 2$ and, hence, $k=1$.

Example 8.5.2. Let $(x, y)$ be an observation of the random pair $(X, Y)$, which has a bivariate normal distribution with parameters $\mu_{1}, \mu_{2}, \sigma_{1}^{2}, \sigma_{2}^{2}$, and $\rho$. In Section 3.5 that joint pdf is given by

$$
f\left(x, y ; \mu_{1}, \mu_{2}, \sigma_{1}^{2}, \sigma_{2}^{2}\right)=\frac{1}{2 \pi \sigma_{1} \sigma_{2} \sqrt{1-\rho^{2}}} e^{-q\left(x, y ; \mu_{1}, \mu_{2}\right) / 2}
$$

for $-\infty<x<\infty$ and $-\infty<y<\infty$, where $\sigma_{1}>0, \sigma_{2}>0,-1<\rho<1$, and

$$
q\left(x, y ; \mu_{1}, \mu_{2}\right)=\frac{1}{1-\rho^{2}}\left[\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)^{2}-2 \rho\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)\left(\frac{y-\mu_{2}}{\sigma_{2}}\right)+\left(\frac{y-\mu_{2}}{\sigma_{2}}\right)^{2}\right]
$$

Assume that $\sigma_{1}^{2}, \sigma_{2}^{2}$, and $\rho$ are known but that we do not know whether the respective means of $(X, Y)$ are $\left(\mu_{1}^{\prime}, \mu_{2}^{\prime}\right)$ or $\left(\mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}\right)$. The inequality

$$
\frac{f\left(x, y ; \mu_{1}^{\prime}, \mu_{2}^{\prime}, \sigma_{1}^{2}, \sigma_{2}^{2}, \rho\right)}{f\left(x, y ; \mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}, \sigma_{1}^{2}, \sigma_{2}^{2}, \rho\right)} \leq k
$$

is equivalent to

$$
\frac{1}{2}\left[q\left(x, y ; \mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}\right)-q\left(x, y ; \mu_{1}^{\prime}, \mu_{2}^{\prime}\right)\right] \leq \log k
$$

Moreover, it is clear that the difference in the left-hand member of this inequality does not contain terms involving $x^{2}, x y$, and $y^{2}$. In particular, this inequality is the same as

$$
\begin{aligned}
\frac{1}{1-\rho^{2}}\left\{\left[\frac{\mu_{1}^{\prime}-\mu_{1}^{\prime \prime}}{\sigma_{1}^{2}}-\frac{\rho\left(\mu_{2}^{\prime}-\mu_{2}^{\prime \prime}\right)}{\sigma_{1} \sigma_{2}}\right] x\right. & \left.+\left[\frac{\mu_{2}^{\prime}-\mu_{2}^{\prime \prime}}{\sigma_{2}^{2}}-\frac{\rho\left(\mu_{1}^{\prime}-\mu_{1}^{\prime \prime}\right)}{\sigma_{1} \sigma_{2}}\right] y\right\} \\
& \leq \log k+\frac{1}{2}\left[q\left(0,0 ; \mu_{1}^{\prime}, \mu_{2}^{\prime}\right)-q\left(0,0 ; \mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}\right)\right]
\end{aligned}
$$

or, for brevity,


\begin{equation*}
a x+b y \leq c \tag{8.5.3}
\end{equation*}


That is, if this linear function of $x$ and $y$ in the left-hand member of inequality (8.5.3) is less than or equal to a constant, we classify $(x, y)$ as coming from the bivariate normal distribution with means $\mu_{1}^{\prime \prime}$ and $\mu_{2}^{\prime \prime}$. Otherwise, we classify $(x, y)$ as arising from the bivariate normal distribution with means $\mu_{1}^{\prime}$ and $\mu_{2}^{\prime}$. Of course, if the prior probabilities can be assigned as discussed in Remark 8.5.1 then $k$ and thus $c$ can be found easily; see Exercise 8.5.3.

Once the rule for classification is established, the statistician might be interested in the two probabilities of misclassifications using that rule. The first of these two is associated with the classification of $(x, y)$ as arising from the distribution indexed by $\theta^{\prime \prime}$ if, in fact, it comes from that index by $\theta^{\prime}$. The second misclassification is similar, but with the interchange of $\theta^{\prime}$ and $\theta^{\prime \prime}$. In the preceding example, the probabilities of these respective misclassifications are

$$
P\left(a X+b Y \leq c ; \mu_{1}^{\prime}, \mu_{2}^{\prime}\right) \quad \text { and } \quad P\left(a X+b Y>c ; \mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}\right) .
$$

The distribution of $Z=a X+b Y$ is obtained from Theorem 3.5.2. It follows that the distribution of $Z=a X+b Y$ is given by

$$
N\left(a \mu_{1}+b \mu_{2}, a^{2} \sigma_{1}^{2}+2 a b \rho \sigma_{1} \sigma_{2}+b^{2} \sigma_{2}^{2}\right)
$$

With this information, it is easy to compute the probabilities of misclassifications; see Exercise 8.5.3.

One final remark must be made with respect to the use of the important classification rule established in Example 8.5.2. In most instances the parameter values $\mu_{1}^{\prime}, \mu_{2}^{\prime}$ and $\mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}$ as well as $\sigma_{1}^{2}, \sigma_{2}^{2}$, and $\rho$ are unknown. In such cases the statistician has usually observed a random sample (frequently called a training sample) from each of the two distributions. Let us say the samples have sizes $n^{\prime}$ and $n^{\prime \prime}$, respectively, with sample characteristics

$$
\bar{x}^{\prime}, \bar{y}^{\prime},\left(s_{x}^{\prime}\right)^{2},\left(s_{y}^{\prime}\right)^{2}, r^{\prime} \quad \text { and } \quad \bar{x}^{\prime \prime}, \bar{y}^{\prime \prime},\left(s_{x}^{\prime \prime}\right)^{2},\left(s_{y}^{\prime \prime}\right)^{2}, r^{\prime \prime}
$$

The statistics $r^{\prime}$ and $r^{\prime \prime}$ are the sample correlation coefficients, as defined in expression (9.7.1) of Section 9.7. The sample correlation coefficient is the mle for the correlation parameter $\rho$ of a bivariate normal distribution; see Section 9.7. If in inequality (8.5.3) the parameters $\mu_{1}^{\prime}, \mu_{2}^{\prime}, \mu_{1}^{\prime \prime}, \mu_{2}^{\prime \prime}, \sigma_{1}^{2}, \sigma_{2}^{2}$, and $\rho \sigma_{1} \sigma_{2}$ are replaced by the unbiased estimates

$$
\begin{gathered}
\bar{x}^{\prime}, \bar{y}^{\prime}, \bar{x}^{\prime \prime}, \bar{y}^{\prime \prime}, \frac{\left(n^{\prime}-1\right)\left(s_{x}^{\prime}\right)^{2}+\left(n^{\prime \prime}-1\right)\left(s_{x}^{\prime \prime}\right)^{2}}{n^{\prime}+n^{\prime \prime}-2}, \frac{\left(n^{\prime}-1\right)\left(s_{y}^{\prime}\right)^{2}+\left(n^{\prime \prime}-1\right)\left(s_{y}^{\prime \prime}\right)^{2}}{n^{\prime}+n^{\prime \prime}-2} \\
\frac{\left(n^{\prime}-1\right) r^{\prime} s_{x}^{\prime} s_{y}^{\prime}+\left(n^{\prime \prime}-1\right) r^{\prime \prime} s_{x}^{\prime \prime} s_{y}^{\prime \prime}}{n^{\prime}+n^{\prime \prime}-2}
\end{gathered}
$$

the resulting expression in the left-hand member is frequently called Fisher's linear discriminant function. Since those parameters have been estimated, the distribution theory associated with $a X+b Y$ does provide an approximation.

Although we have considered only bivariate distributions in this section, the results can easily be extended to multivariate normal distributions using the results of Section 3.5; see also Chapter 6 of Seber (1984).

\section*{EXERCISES}
8.5.1. Let $X_{1}, X_{2}, \ldots, X_{20}$ be a random sample of size 20 from a distribution that is $N(\theta, 5)$. Let $L(\theta)$ represent the joint pdf of $X_{1}, X_{2}, \ldots, X_{20}$. The problem is to test $H_{0}: \theta=1$ against $H_{1}: \theta=0$. Thus $\Omega=\{\theta: \theta=0,1\}$.\\
(a) Show that $L(1) / L(0) \leq k$ is equivalent to $\bar{x} \leq c$.\\
(b) Find $c$ so that the significance level is $\alpha=0.05$. Compute the power of this test if $H_{1}$ is true.\\
(c) If the loss function is such that $\mathcal{L}(1,1)=\mathcal{L}(0,0)=0$ and $\mathcal{L}(1,0)=\mathcal{L}(0,1)>0$, find the minimax test. Evaluate the power function of this test at the points $\theta=1$ and $\theta=0$.\\
8.5.2. Let $X_{1}, X_{2}, \ldots, X_{10}$ be a random sample of size 10 from a Poisson distribution with parameter $\theta$. Let $L(\theta)$ be the joint pdf of $X_{1}, X_{2}, \ldots, X_{10}$. The problem is to test $H_{0}: \theta=\frac{1}{2}$ against $H_{1}: \theta=1$.\\
(a) Show that $L\left(\frac{1}{2}\right) / L(1) \leq k$ is equivalent to $y=\sum_{1}^{n} x_{i} \geq c$.\\
(b) In order to make $\alpha=0.05$, show that $H_{0}$ is rejected if $y>9$ and, if $y=9$, reject $H_{0}$ with probability $\frac{1}{2}$ (using some auxiliary random experiment).\\
(c) If the loss function is such that $\mathcal{L}\left(\frac{1}{2}, \frac{1}{2}\right)=\mathcal{L}(1,1)=0$ and $\mathcal{L}\left(\frac{1}{2}, 1\right)=1$ and $\mathcal{L}\left(1, \frac{1}{2}\right)=2$, show that the minimax procedure is to reject $H_{0}$ if $y>6$ and, if $y=6$, reject $H_{0}$ with probability 0.08 (using some auxiliary random experiment).\\
8.5.3. In Example 8.5.2 let $\mu_{1}^{\prime}=\mu_{2}^{\prime}=0, \mu_{1}^{\prime \prime}=\mu_{2}^{\prime \prime}=1, \sigma_{1}^{2}=1, \sigma_{2}^{2}=1$, and $\rho=\frac{1}{2}$.\\
(a) Find the distribution of the linear function $a X+b Y$.\\
(b) With $k=1$, compute $P\left(a X+b Y \leq c ; \mu_{1}^{\prime}=\mu_{2}^{\prime}=0\right)$ and $P\left(a X+b Y>c ; \mu_{1}^{\prime \prime}=\right.$ $\mu_{2}^{\prime \prime}=1$ ).\\
8.5.4. Determine Newton's algorithm to find the solution of Equation (8.5.2). If software is available, write a program that performs your algorithm and then show that the solution is $c=76.8$. If software is not available, solve (8.5.2) by "trial and error."\\
8.5.5. Let $X$ and $Y$ have the joint pdf

$$
f\left(x, y ; \theta_{1}, \theta_{2}\right)=\frac{1}{\theta_{1} \theta_{2}} \exp \left(-\frac{x}{\theta_{1}}-\frac{y}{\theta_{2}}\right), \quad 0<x<\infty, \quad 0<y<\infty
$$

zero elsewhere, where $0<\theta_{1}, 0<\theta_{2}$. An observation $(x, y)$ arises from the joint distribution with parameters equal to either $\left(\theta_{1}^{\prime}=1, \theta_{2}^{\prime}=5\right)$ or $\left(\theta_{1}^{\prime \prime}=3, \theta_{2}^{\prime \prime}=2\right)$. Determine the form of the classification rule.\\
8.5.6. Let $X$ and $Y$ have a joint bivariate normal distribution. An observation $(x, y)$ arises from the joint distribution with parameters equal to either

$$
\mu_{1}^{\prime}=\mu_{2}^{\prime}=0, \quad\left(\sigma_{1}^{2}\right)^{\prime}=\left(\sigma_{2}^{2}\right)^{\prime}=1, \quad \rho^{\prime}=\frac{1}{2}
$$

or

$$
\mu_{1}^{\prime \prime}=\mu_{2}^{\prime \prime}=1, \quad\left(\sigma_{1}^{2}\right)^{\prime \prime}=4, \quad\left(\sigma_{2}^{2}\right)^{\prime \prime}=9, \quad \rho^{\prime \prime}=\frac{1}{2}
$$

Show that the classification rule involves a second-degree polynomial in $x$ and $y$.\\
8.5.7. Let $\boldsymbol{W}^{\prime}=\left(W_{1}, W_{2}\right)$ be an observation from one of two bivariate normal distributions, I and II, each with $\mu_{1}=\mu_{2}=0$ but with the respective variancecovariance matrices

$$
\boldsymbol{V}_{1}=\left(\begin{array}{ll}
1 & 0 \\
0 & 4
\end{array}\right) \quad \text { and } \quad \boldsymbol{V}_{2}=\left(\begin{array}{cc}
3 & 0 \\
0 & 12
\end{array}\right) .
$$

How would you classify $\boldsymbol{W}$ into I or II?

\section*{Chapter 9}
\section*{Inferences About Normal Linear Models}
\subsection*{9.1 Introduction}
In this chapter, we consider analyses of some of the most widely used linear models. These models include one- and two-way analysis of variance (ANOVA) models and regression and correlation models. We generally assume normally distributed random errors for these models. The inference procedures that we discuss are, for the most part, based on maximum likelihood procedures. The theory requires some discussion of quadratic forms which we briefly introduce next.

Consider polynomials of degree 2 in $n$ variables, $X_{1}, \ldots, X_{n}$, of the form

$$
q\left(X_{1}, \ldots, X_{n}\right)=\sum_{i=1}^{n} \sum_{j=1}^{n} X_{i} a_{i j} X_{j},
$$

for $n^{2}$ constants $a_{i j}$. We call this form a quadratic form in the variables $X_{1}, \ldots, X_{n}$. If both the variables and the coefficients are real, it is called a real quadratic form. Only real quadratic forms are considered in this book. To illustrate, the form $X_{1}^{2}+X_{1} X_{2}+X_{2}^{2}$ is a quadratic form in the two variables $X_{1}$ and $X_{2}$; the form $X_{1}^{2}+X_{2}^{2}+X_{3}^{2}-2 X_{1} X_{2}$ is a quadratic form in the three variables $X_{1}, X_{2}$, and $X_{3}$; but the form $\left(X_{1}-1\right)^{2}+\left(X_{2}-2\right)^{2}=X_{1}^{2}+X_{2}^{2}-2 X_{1}-4 X_{2}+5$ is not a quadratic form in $X_{1}$ and $X_{2}$, although it is a quadratic form in the variables $X_{1}-1$ and $X_{2}-2$.

Let $\bar{X}$ and $S^{2}$ denote, respectively, the mean and variance of a random sample\\
$X_{1}, X_{2}, \ldots, X_{n}$ from an arbitrary distribution. Thus

$$
\begin{aligned}
(n-1) S^{2} & =\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2} \\
& =\sum_{i=1}^{n} X_{i}^{2}-\frac{n}{n^{2}}\left(\sum_{i=1}^{n} X_{i}\right)^{2} \\
& =\sum_{i=1}^{n} X_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} X_{i} \sum_{j=1}^{n} X_{j}\right) \\
& =\sum_{i=1}^{n} X_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} X_{i}^{2}+2 \sum_{i<j} X_{i} X_{j}\right) \\
& =\frac{n-1}{n} \sum_{i=1}^{n} X_{i}^{2}-\frac{2}{n} \sum_{i<j} X_{i} X_{j} .
\end{aligned}
$$

So the sample variance is a quadratic form in the variables $X_{1}, \ldots, X_{n}$.

\subsection*{9.2 One-Way ANOVA}
Consider $b$ independent random variables that have normal distributions with unknown means $\mu_{1}, \mu_{2}, \ldots, \mu_{b}$, respectively, and unknown but common variance $\sigma^{2}$. For each $j=1,2, \ldots, b$, let $X_{1 j}, X_{2 j}, \ldots, X_{n_{j} j}$ represent a random sample of size $n_{j}$ from the normal distribution with mean $\mu_{j}$ and variance $\sigma^{2}$. The appropriate model for the observations is


\begin{equation*}
X_{i j}=\mu_{j}+e_{i j} ; \quad i=1, \ldots, n_{j}, j=1, \ldots, b, \tag{9.2.1}
\end{equation*}


where $e_{i j}$ are iid $N\left(0, \sigma^{2}\right)$. Let $n=\sum_{j=1}^{b} n_{j}$ denote the total sample size. Suppose that it is desired to test the composite hypothesis


\begin{equation*}
H_{0}: \mu_{1}=\mu_{2}=\cdots=\mu_{b} \text { versus } H_{1}: \mu_{j} \neq \mu_{j^{\prime}}, \text { for some } j \neq j^{\prime} \tag{9.2.2}
\end{equation*}


We derive the likelihood ratio test for these hypotheses.\\
Such problems often arise in practice. For example, suppose for a certain type of disease there are $b$ drugs that can be used to treat it and we are interested in determining which drug is best in terms of a certain response. Let $X_{j}$ denote this response when drug $j$ is applied and let $\mu_{j}=E\left(X_{j}\right)$. If we assume that $X_{j}$ is $N\left(\mu_{j}, \sigma^{2}\right)$, then the above null hypothesis says that all the drugs are equally effective; see Exercise 9.2.6 for a numerical illustration of this situation involving drugs that are intended to lower cholesterol. In general, we often summarize this problem by saying that we have one factor at $b$ levels. In this case the factor is the treatment of the disease and each level corresponds to one of the treatment drugs.

Model (9.2.1) is called a one-way model. As shown, the likelihood ratio test can be thought of in terms of estimates of variance. Hence, this is an example of an\\
analysis of variance (ANOVA). In short, we say that this example is a one-way ANOVA problem.

Here the full model parameter space is

$$
\Omega=\left\{\left(\mu_{1}, \mu_{2}, \ldots, \mu_{b}, \sigma^{2}\right):-\infty<\mu_{j}<\infty, 0<\sigma^{2}<\infty\right\}
$$

while the reduced model (full model under $H_{0}$ ) parameter space is

$$
\omega=\left\{\left(\mu_{1}, \mu_{2}, \ldots, \mu_{b}, \sigma^{2}\right):-\infty<\mu_{1}=\mu_{2}=\cdots=\mu_{b}=\mu<\infty, 0<\sigma^{2}<\infty\right\} .
$$

The likelihood functions, denoted by $L(\Omega)$ and $L(\omega)$ are, respectively,

$$
L(\Omega)=\left(\frac{1}{2 \pi \sigma^{2}}\right)^{a b / 2} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\mu_{j}\right)^{2}\right] .
$$

and

$$
L(\omega)=\left(\frac{1}{2 \pi \sigma^{2}}\right)^{a b / 2} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\mu\right)^{2}\right]
$$

We first consider the reduced model. Notice that it is just a one sample model with sample size $n$ from a $N\left(\mu, \sigma^{2}\right)$ distribution. We have derived the mles in Example 4.1.3 of Chapter 4, which, in this notation, are given by


\begin{equation*}
\hat{\mu}_{\omega}=\frac{1}{n} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}} x_{i j}=\bar{x} . . \text { and } \hat{\sigma}_{\omega}^{2}=\frac{1}{n} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{. .}\right)^{2} . \tag{9.2.3}
\end{equation*}


The notation $\bar{x}$. denotes that the mean is taken over both subscripts. This is often called the grand mean. Evaluating $L(\omega)$ at the mles, we obtain after simplification:


\begin{equation*}
L(\hat{\omega})=\left(\frac{1}{2 \pi}\right)^{n / 2}\left(\frac{1}{\hat{\sigma}_{\omega}^{2}}\right)^{n / 2} e^{-n / 2} \tag{9.2.4}
\end{equation*}


Next, we consider the full model. The log of its likelihood is


\begin{equation*}
\log L(\Omega)=-(n / 2) \log (2 \pi)-(n / 2) \log \left(\sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\mu_{j}\right)^{2} . \tag{9.2.5}
\end{equation*}


For $j=1, \ldots, b$, the partial of the $\log$ of $L(\Omega)$ with respect to $\mu_{j}$ results in

$$
\frac{\partial \log L(\Omega}{\partial \mu_{j}}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n_{j}}\left(x_{i j}-\mu_{j}\right) .
$$

Setting this partial to 0 and solving for $\mu_{j}$, we obtain the mle of $\mu_{j}$ which we denote by


\begin{equation*}
\hat{\mu}_{j}=\frac{1}{n_{j}} \sum_{i=1}^{n_{j}} x_{i j}=\bar{x} \cdot j, \quad j=1, \ldots, b \tag{9.2.6}
\end{equation*}


Since this derivation did not depend on $\sigma$, to find the mle of $\sigma$, we substitute $\bar{x}_{\cdot j}$ for $\mu_{j}$ in the $\log L(\Omega)$. Taking the partial derivative with respect to $\sigma$ we then get

$$
\frac{\partial \log L(\Omega}{\partial \sigma}=-(n / 2) \frac{2 \sigma}{\sigma^{2}}+\frac{1}{\sigma^{3}} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{\cdot j}\right)^{2} .
$$

Solving this for $\sigma^{2}$, we obtain ${ }^{1}$ the mle


\begin{equation*}
\hat{\sigma}_{\Omega}^{2}=\frac{1}{n} \sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{\cdot j}\right)^{2} . \tag{9.2.7}
\end{equation*}


Substituting these mles for their respective parameters in $L(\Omega)$, after some simplification, leads to


\begin{equation*}
L(\hat{\Omega})=\left(\frac{1}{2 \pi}\right)^{n / 2}\left(\frac{1}{\hat{\sigma}_{\Omega}^{2}}\right)^{n / 2} e^{-n / 2} \tag{9.2.8}
\end{equation*}


Hence, the likelihood ratio test rejects $H_{0}$ in favor of $H_{1}$ for small values of the statistic $\hat{\Lambda}=L(\hat{\omega}) / L(\hat{\Omega})$ or equivalently, for large values of $\hat{\Lambda}^{-2 / n}$. We can express this test statistic as a ratio of two quadratic forms $Q_{3}$ and $Q$ as


\begin{align*}
\hat{\Lambda}^{n / 2} & =\frac{\hat{\sigma}_{\Omega}^{2}}{\hat{\sigma}_{\omega}^{2}}=\frac{\sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{. j}\right)^{2}}{\sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x} . .\right)^{2}} \\
& =\mathrm{dfn} \frac{Q_{3}}{Q} . \tag{9.2.9}
\end{align*}


In order to rewrite the test statistic in terms of an $F$-statistic, consider the identity involving $Q, Q_{3}$, and another quadratic form $Q_{4}$ given by:


\begin{align*}
Q & =\sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x} . .\right)^{2}=\sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left[\left(x_{i j}-\bar{x}_{. j}\right)+\left(\bar{x}_{. j}-\bar{x}_{. .}\right)\right]^{2} \\
& =\sum_{j=1}^{b} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{. j}\right)^{2}+\sum_{j=1}^{b} n_{j}\left(\bar{x}_{. j}-\bar{x}_{. .}\right)^{2} \\
& =\operatorname{dfn} \quad Q_{3}+Q_{4} . \tag{9.2.10}
\end{align*}


This derivation follows because the cross product term in the second line is 0 . Using this identity, the test statistic $\hat{\Lambda}^{-2 / n}$ can be expressed as

$$
\hat{\Lambda}^{-2 / n}=\frac{Q_{3}+Q_{4}}{Q_{3}}=1+\frac{Q_{4}}{Q_{3}} .
$$

As the final version, note that the test rejects $H_{0}$ if $F$ is too large where


\begin{equation*}
F=\frac{Q_{4} /(b-1)}{Q_{3} /(n-b)} . \tag{9.2.11}
\end{equation*}


\footnotetext{${ }^{1}$ We are using the fact that the mle of $\sigma^{2}$ is the square of the mle of $\sigma$.
}To complete the test, we need to determine the distribution of $F$ under $H_{0}$. First consider the sum of squares in the denominator, $Q_{3}$, which we write as:

$$
Q_{3} / \sigma^{2}=\sum_{j=1}^{b}\left\{\frac{1}{\sigma^{2}} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)^{2}\right\} .
$$

Notice, since we are discussing distributions, we are now using random variable notation. By Part (c) of Theorem 3.6.1, for $j=1, \ldots, b$, the term within the braces has a $\chi^{2}$-distribution with $n_{j}-1$ degrees of freedom. Further, the samples are independent so these $\chi^{2}$ random variables are independent. Hence, by Corollary 3.3.1, $Q_{3} / \sigma^{2}$ has a $\chi^{2}$-distribution with $\sum_{j=1}^{b}\left(n_{j}-1\right)=n-b$ degrees of freedom. By Part (b) of Theorem 3.6.1, the random variable $\bar{X}_{\cdot j}$ is independent of the sum of squares within the braces and further, by the independence of the samples, it is independent of $Q_{3}$. Thus, all $b$ sample means are independent of $Q_{3}$. Because $\bar{X} . .=\sum_{j=1}^{b} n_{j} \bar{X}_{. j}$, the grand mean $\bar{X} .$. is a function of the $b$ sample means, it must be independent of $Q_{3}$, also. Therefore, $Q_{4}$ is independent of $Q_{3}$. For the distribution of the numerator sum of squares, write the identity (9.2.10) as

$$
Q / \sigma^{2}=Q_{3} / \sigma^{2}+Q_{4} / \sigma^{2} .
$$

For the left side, under $H_{0}, Q / \sigma^{2}$ has a $\chi^{2}$-distribution with $n-1$ degrees of freedom. On the right side $Q_{3} / \sigma^{2}$ has a $\chi^{2}$-distribution with $n-b$ degrees of freedom and it is also independent of $Q_{4} / \sigma^{2}$. By equating the mgfs of both sides, it follows that $Q_{4} / \sigma^{2}$ has a $\chi^{2}$-distribution with $(n-1)-(n-b)=b-1$ degrees of freedom. Therefore, under $H_{0}$, the $F$ test statistic, (9.2.11), has a $F$-distribution with $b-1$ and $n-b$ degrees of freedom.

Suppose now that we wish to compute the power of the test of $H_{0}$ against $H_{1}$ when $H_{0}$ is false, that is, when we do not have $\mu_{1}=\mu_{2}=\cdots=\mu_{b}$. In Section 9.3 we show that under $H_{1}, Q_{4} / \sigma^{2}$ no longer has a $\chi^{2}(b-1)$ distribution. Thus we cannot use an $F$-statistic to compute the power of the test when $H_{1}$ is true. The problem is discussed in Section 9.3.

Next, based on a simple example, we illustrate the computation of the $F$-test using R.

Example 9.2.1. Devore (2012), page 412, presents a data set where the response is the elastic modulus for an alloy that is cast by one of three different casting processes. The null hypothesis is that the mean of the elastic modulus is not affected by the casting process. The data are:

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
Cast Method & \multicolumn{8}{|c|}{Elastic Modulus} \\
\hline
Permanent mold & 45.5 & 45.3 & 45.4 & 44.4 & 44.6 & 43.9 & 44.6 & 44.0 \\
\hline
Die cast & 44.2 & 43.9 & 44.7 & 44.2 & 44.0 & 43.8 & 44.6 & 43.1 \\
\hline
Plaster mold & 46.0 & 45.9 & 44.8 & 46.2 & 45.1 & 45.5 &  &  \\
\hline
\end{tabular}
\end{center}

The data are in the file elasticmod.rda. The variable elasticmod contains the response while the variable ind contains the casting method ( 1,2 , or 3 ). The R code and results (test statistic $F$ and the $p$-value) are:

\begin{verbatim}
oneway.test(elasticmod~ind,var.equal=T)
F = 12.565, num df = 2, denom df = 19, p-value = 0.0003336
\end{verbatim}

With such a low $p$-value, the null hypothesis would be rejected and we would conclude that the casting method does have an effect on the elastic modulus.

In this example, the experimenter would also be interested in the pairwise comparisons of the casting methods. We consider this in Section 9.4.

\section*{EXERCISES}
9.2.1. Consider the $T$-statistic that was derived through a likelihood ratio for testing the equality of the means of two normal distributions having common variance in Example 8.3.1. Show that $T^{2}$ is exactly the $F$-statistic of expression (9.2.11).\\
9.2.2. Under Model (9.2.1), show that the linear functions $X_{i j}-\bar{X}_{. j}$ and $\bar{X}_{. j}-\bar{X}_{\text {.. }}$ are uncorrelated.\\
Hint: Recall the definition of $\bar{X}_{. j}$ and $\bar{X}_{. .}$and, without loss of generality, we can let $E\left(X_{i j}\right)=0$ for all $i, j$.\\
9.2.3. The following are observations associated with independent random samples from three normal distributions having equal variances and respective means $\mu_{1}, \mu_{2}, \mu_{3}$.

\begin{center}
\begin{tabular}{rrr}
\hline
I & II & III \\
\hline
0.5 & 2.1 & 3.0 \\
1.3 & 3.3 & 5.1 \\
-1.0 & 0.0 & 1.9 \\
1.8 & 2.3 & 2.4 \\
 & 2.5 & 4.2 \\
 &  & 4.1 \\
\hline
\end{tabular}
\end{center}

Using R or another statistical package, compute the $F$-statistic that is used to test $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$.\\
9.2.4. Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from a normal distribution $N\left(\mu, \sigma^{2}\right)$. Show that

$$
\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\sum_{i=2}^{n}\left(X_{i}-\bar{X}^{\prime}\right)^{2}+\frac{n-1}{n}\left(X_{1}-\bar{X}^{\prime}\right)^{2},
$$

where $\bar{X}=\sum_{i=1}^{n} X_{i} / n$ and $\bar{X}^{\prime}=\sum_{i=2}^{n} X_{i} /(n-1)$.\\
Hint: Replace $X_{i}-\bar{X}$ by $\left(X_{i}-\bar{X}^{\prime}\right)-\left(X_{1}-\bar{X}^{\prime}\right) / n$. Show that $\sum_{i=2}^{n}\left(X_{i}-\bar{X}^{\prime}\right)^{2} / \sigma^{2}$ has a chi-square distribution with $n-2$ degrees of freedom. Prove that the two terms in the right-hand member are independent. What then is the distribution of

$$
\frac{[(n-1) / n]\left(X_{1}-\bar{X}^{\prime}\right)^{2}}{\sigma^{2}} ?
$$

9.2.5. Using the notation of this section, assume that the means satisfy the condition that $\mu=\mu_{1}+(b-1) d=\mu_{2}-d=\mu_{3}-d=\cdots=\mu_{b}-d$. That is, the last $b-1$ means are equal but differ from the first mean $\mu_{1}$, provided that $d \neq 0$. Let independent random samples of size $a$ be taken from the $b$ normal distributions with common unknown variance $\sigma^{2}$.\\
(a) Show that the maximum likelihood estimators of $\mu$ and $d$ are $\hat{\mu}=\bar{X}$.. and

$$
\hat{d}=\frac{\sum_{j=2}^{b} \bar{X}_{. j} /(b-1)-\bar{X}_{.1}}{b} .
$$

(b) Using Exercise 9.2.4, find $Q_{6}$ and $Q_{7}=c \hat{d}^{2}$ so that, when $d=0, Q_{7} / \sigma^{2}$ is $\chi^{2}(1)$ and

$$
\sum_{i=1}^{a} \sum_{j=1}^{b}\left(X_{i j}-\bar{X}_{. .}\right)^{2}=Q_{3}+Q_{6}+Q_{7}
$$

(c) Argue that the three terms in the right-hand member of part (b), once divided by $\sigma^{2}$, are independent random variables with chi-square distributions, provided that $d=0$.\\
(d) The ratio $Q_{7} /\left(Q_{3}+Q_{6}\right)$ times what constant has an $F$-distribution, provided that $d=0$ ? Note that this $F$ is really the square of the two-sample $T$ used to test the equality of the mean of the first distribution and the common mean of the other distributions, in which the last $b-1$ samples are combined into one.\\
9.2.6. On page 123 of their text, Kloke and McKean (2014) present the results of an experiment investigating 4 drugs (treatments) for their effect on lowering LDL (low density lipids) cholesterol. For the experimental design, 39 quail were randomly assigned to one of the 4 drugs. The drug was mixed in their food, but, other than this, the quail were all treated in the same way. After a specified period of time, the LDL level of each quail was determined. The first drug was a placebo, so the interest is to see if any other of the drugs resulted in lower LDL than the placebo. The data are in the file quailldl.rda. The first column of this matrix contains the drug indicator ( 1 through 4) for the quail while the second column contains the ldl level of that quail.\\
(a) Obtain comparison boxplots of LDL levels. Which drugs seem to result in lower LDL levels? Identify, by observation number, the outliers in the data.\\
(b) Compute the $F$-test that all mean levels of LDL are the same for all 4 drugs. Report the $F$-test statistic and $p$-value. Conclude in terms of the problem using the nominal significance level of 0.05. Use the R code in Example 9.2.1.\\
(c) Does your conclusion in Part (b) agree with the boxplots of Part (a)?\\
(d) Note that one assumption for the $F$-test is that the random errors $e_{i j}$ in Model (9.2.1) are normally distributed. An estimate of $e_{i j}$ is $x_{i j}-\bar{x}_{\cdot j}$. These are called residuals, i.e., what is left after the full model fit. Compute these residuals and then obtain a histogram, a boxplot, and a normal $q-q$ plot of them. Comment on the normality assumption. Use the code:

\begin{verbatim}
resd <- lm(quailmat[,2] ^factor(quailmat[,1]))$resid
par(mfrow=c(2,2));hist(resd); boxplot(resd); qqnorm(resd)
\end{verbatim}

9.2.7. Let $\mu_{1}, \mu_{2}, \mu_{3}$ be, respectively, the means of three normal distributions with a common but unknown variance $\sigma^{2}$. In order to test, at the $\alpha=5 \%$ significance level, the hypothesis $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$ against all possible alternative hypotheses, we take an independent random sample of size 4 from each of these distributions. Determine whether we accept or reject $H_{0}$ if the observed values from these three distributions are, respectively,

\begin{center}
\begin{tabular}{rrrrr}
$X_{1}:$ & 5 & 9 & 6 & 8 \\
$X_{2}:$ & 11 & 13 & 10 & 12 \\
$X_{3}:$ & 10 & 6 & 9 & 9 \\
\end{tabular}
\end{center}

9.2.8. The driver of a diesel-powered automobile decided to test the quality of three types of diesel fuel sold in the area based on mpg. Test the null hypothesis that the three means are equal using the following data. Make the usual assumptions and take $\alpha=0.05$.

\begin{center}
\begin{tabular}{llllll}
Brand A: & 38.7 & 39.2 & 40.1 & 38.9 &  \\
Brand B: & 41.9 & 42.3 & 41.3 &  &  \\
Brand C: & 40.8 & 41.2 & 39.5 & 38.9 & 40.3 \\
\end{tabular}
\end{center}

\subsection*{9.3 Noncentral $\chi^{2}$ and $F$-Distributions}
Let $X_{1}, X_{2}, \ldots, X_{n}$ denote independent random variables that are $N\left(\mu_{i}, \sigma^{2}\right), i=$ $1,2, \ldots, n$, and consider the quadratic form $Y=\sum_{1}^{n} X_{i}^{2} / \sigma^{2}$. If each $\mu_{i}$ is zero, we know that $Y$ is $\chi^{2}(n)$. We shall now investigate the distribution of $Y$ when each $\mu_{i}$ is not zero. The mgf of $Y$ is given by

$$
\begin{aligned}
M(t) & =E\left[\exp \left(t \sum_{i=1}^{n} \frac{X_{i}^{2}}{\sigma^{2}}\right)\right] \\
& =\prod_{i=1}^{n} E\left[\exp \left(t \frac{X_{i}^{2}}{\sigma^{2}}\right)\right] .
\end{aligned}
$$

Consider

$$
E\left[\exp \left(\frac{t X_{i}^{2}}{\sigma^{2}}\right)\right]=\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[\frac{t x_{i}^{2}}{\sigma^{2}}-\frac{\left(x_{i}-\mu_{i}\right)^{2}}{2 \sigma^{2}}\right] d x_{i} .
$$

The integral exists if $t<\frac{1}{2}$. To evaluate the integral, note that

$$
\begin{aligned}
\frac{t x_{i}^{2}}{\sigma^{2}}-\frac{\left(x_{i}-\mu_{i}\right)^{2}}{2 \sigma^{2}} & =-\frac{x_{i}^{2}(1-2 t)}{2 \sigma^{2}}+\frac{2 \mu_{i} x_{i}}{2 \sigma^{2}}-\frac{\mu_{i}^{2}}{2 \sigma^{2}} \\
& =\frac{t \mu_{i}^{2}}{\sigma^{2}(1-2 t)}-\frac{1-2 t}{2 \sigma^{2}}\left(x_{i}-\frac{\mu_{i}}{1-2 t}\right)^{2}
\end{aligned}
$$

Accordingly, with $t<\frac{1}{2}$, we have\\
$E\left[\exp \left(\frac{t X_{i}^{2}}{\sigma^{2}}\right)\right]=\exp \left[\frac{t \mu_{i}^{2}}{\sigma^{2}(1-2 t)}\right] \int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1-2 t}{2 \sigma^{2}}\left(x_{i}-\frac{\mu_{i}}{1-2 t}\right)^{2}\right] d x_{i}$.\\
If we multiply the integrand by $\sqrt{1-2 t}, t<\frac{1}{2}$, we have the integral of a normal pdf with mean $\mu_{i} /(1-2 t)$ and variance $\sigma^{2} /(1-2 t)$. Thus

$$
E\left[\exp \left(\frac{t X_{i}^{2}}{\sigma^{2}}\right)\right]=\frac{1}{\sqrt{1-2 t}} \exp \left[\frac{t \mu_{i}^{2}}{\sigma^{2}(1-2 t)}\right]
$$

and the mgf of $Y=\sum_{1}^{n} X_{i}^{2} / \sigma^{2}$ is given by


\begin{equation*}
M(t)=\frac{1}{(1-2 t)^{n / 2}} \exp \left[\frac{t \sum_{1}^{n} \mu_{i}^{2}}{\sigma^{2}(1-2 t)}\right], \quad t<\frac{1}{2} \tag{9.3.1}
\end{equation*}


A random variable that has the mgf


\begin{equation*}
M(t)=\frac{1}{(1-2 t)^{r / 2}} e^{t \theta /(1-2 t)} \tag{9.3.2}
\end{equation*}


where $t<\frac{1}{2}, 0<\theta$, and $r$ is a positive integer, is said to have a noncentral chi-square distribution with $r$ degrees of freedom and noncentrality parameter $\theta$. If one sets the noncentrality parameter $\theta=0$, one has $M(t)=(1-2 t)^{-r / 2}$, which is the mgf of a random variable that is $\chi^{2}(r)$. Such a random variable can appropriately be called a central chi-square variable. We shall use the symbol $\chi^{2}(r, \theta)$ to denote a noncentral chi-square distribution that has the parameters $r$ and $\theta$; and we shall say that a random variable is $\chi^{2}(r, \theta)$ when that random variable has this kind of distribution. The symbol $\chi^{2}(r, 0)$ is equivalent to $\chi^{2}(r)$. Thus our random variable $Y=\sum_{1}^{n} X_{i}^{2} / \sigma^{2}$ of this section is $\chi^{2}\left(n, \sum_{1}^{n} \mu_{i}^{2} / \sigma^{2}\right)$. The mean of $Y$ is given by


\begin{equation*}
E(Y)=\frac{1}{\sigma^{2}} \sum_{i=1}^{n} E\left(X_{i}^{2}\right)=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(\sigma^{2}+\mu_{i}^{2}\right)=n+\theta \tag{9.3.3}
\end{equation*}


i.e., the mean of the central $\chi^{2}$ plus the noncentrality parameter. If each $\mu_{i}$ is equal to zero, then $Y$ is $\chi^{2}(n, 0)$ or, more simply, $Y$ is $\chi^{2}(n)$ with mean $n$.

The noncentral $\chi^{2}$-variables, in which we have interest, are certain quadratic forms in normally distributed variables divided by a variance $\sigma^{2}$. In our example it is worth noting that the noncentrality parameter of $\sum_{1}^{n} X_{i}^{2} / \sigma^{2}$, which is\\
$\sum_{1}^{n} \mu_{i}^{2} / \sigma^{2}$, may be computed by replacing each $X_{i}$ in the quadratic form by its mean $\mu_{i}, i=1,2, \ldots, n$. This is no fortuitous circumstance; any quadratic form $Q=Q\left(X_{1}, \ldots, X_{n}\right)$ in normally distributed variables, which is such that $Q / \sigma^{2}$ is $\chi^{2}(r, \theta)$, has $\theta=Q\left(\mu_{1}, \mu_{2}, \ldots, \mu_{n}\right) / \sigma^{2}$; and if $Q / \sigma^{2}$ is a chi-square variable (central or noncentral) for certain real values of $\mu_{1}, \mu_{2}, \ldots, \mu_{n}$, it is chi-square (central or noncentral) for all real values of these means.

We next discuss the noncentral $F$-distribution. If $U$ and $V$ are independent and are, respectively, $\chi^{2}\left(r_{1}\right)$ and $\chi^{2}\left(r_{2}\right)$, the random variable $F$ has been defined by $F=r_{2} U / r_{1} V$. Now suppose, in particular, that $U$ is $\chi^{2}\left(r_{1}, \theta\right), V$ is $\chi^{2}\left(r_{2}\right)$, and $U$ and $V$ are independent. The distribution of the random variable $r_{2} U / r_{1} V$ is called a noncentral $F$-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with noncentrality parameter $\theta$. Note that the noncentrality parameter of $F$ is precisely the noncentrality parameter of the random variable $U$, which is $\chi^{2}\left(r_{1}, \theta\right)$. To obtain the expectation of $F$, use the $E(U)$ in expression (9.3.3) and the derivation of the expected value of a central $F$ given in expression (3.6.8). These together immediately imply that


\begin{equation*}
E(F)=\frac{r_{2}}{r_{2}-2}\left[\frac{r_{1}+\theta}{r_{1}}\right], \tag{9.3.4}
\end{equation*}


provided, of course, that $r_{2}>2$. If $\theta>0$ then the quantity in brackets exceeds one and, hence, the mean of the noncentral $F$ exceeds the mean of the corresponding central $F$.

We next discuss the noncentral $F$ distribution for the one-way ANOVA of the last section.

Example 9.3.1 (Noncentrality Parameter for One-way ANOVA). Consider the one-way model with $b$ levels, expression (9.2.1), with the hypotheses $H_{0}: \mu_{1}=$ $\cdots=\mu_{b}$ versus $H_{1}: \mu_{j} \neq \mu_{j^{\prime}}$ for some $j \neq j^{\prime}$. From expression (9.2.11), the $F$ test statistic is $F=\left[Q_{4} /(b-1)\right] /\left[Q_{3} /(n-b)\right]$. In the denominator, the random variable $Q_{3} / \sigma^{2}$ is $\chi^{2}(n-b)$ under the full model and, hence, in particular, under $H_{1}$. It follows from Remark 9.8.3 of Section 9.8, though, that the distribution of $Q_{4} / \sigma^{2}$ is noncentral $\chi^{2}(b-1, \theta)$ under the full model. Recall that

$$
Q_{4} / \sigma^{2}=\frac{1}{\sigma^{2}} \sum_{j=1}^{b} n_{j}\left(\bar{X}_{\cdot j}-\bar{X}_{. .}\right)^{2} .
$$

Under the full model, $E\left(\bar{X}_{. j}\right)=\mu_{j}$ and $E\left(\bar{X}_{. .}\right)=\sum_{j=1}^{b}\left(n_{j} / n\right) \mu_{j}$. Calling this last expectation $\bar{\mu}$, we have from the above discussion that


\begin{equation*}
\theta=\frac{1}{\sigma^{2}} \sum_{j=1}^{b} n_{j}\left(\mu_{j}-\bar{\mu}\right)^{2} . \tag{9.3.5}
\end{equation*}


If $H_{0}$ is true then $\mu_{j} \equiv \mu$, for some $\mu$, and, hence, $\bar{\mu}=\mu$. Thus, under $H_{0}, \theta=0$. Under $H_{1}$, there are distinct $j$ and $j^{\prime}$ such that $\mu_{j} \neq \mu_{j^{\prime}}$. In particular, then both $\mu_{j}$ and $\mu_{j^{\prime}}$ cannot equal $\bar{\mu}$, so $\theta>0$. Therefore, under $H_{1}$ the expectation of $F$ exceeds the null expectation.

There are R commands that compute the cdf of noncentral $\chi^{2}$ and $F$ random variables. For example, suppose we want to compute $P(Y \leq y)$, where $Y$ has a $\chi^{2}$-distribution with d degrees of freedom and noncentrality parameter b . This probability is returned with the command $\operatorname{pchisq}(\mathrm{y}, \mathrm{d}, \mathrm{b})$. The corresponding value of the pdf at $y$ is computed by the command dchisq( $\mathrm{y}, \mathrm{d}, \mathrm{b}$ ). As another example, suppose we want $P(W \geq w)$, where $W$ has an $F$-distribution with n1 and n2 degrees of freedom and noncentrality parameter theta. This is computed by the command 1-pf (w, n1, n2, theta), while the command $\mathrm{df}(\mathrm{w}, \mathrm{n} 1, \mathrm{n} 2$, theta) computes the value of the density of $W$ at $w$. Tables of the noncentral chi-square and noncentral $F$-distributions are available in the literature also.

\section*{EXERCISES}
9.3.1. Let $Y_{i}, i=1,2, \ldots, n$, denote independent random variables that are, respectively, $\chi^{2}\left(r_{i}, \theta_{i}\right), i=1,2, \ldots, n$. Prove that $Z=\sum_{1}^{n} Y_{i}$ is $\chi^{2}\left(\sum_{1}^{n} r_{i}, \sum_{1}^{n} \theta_{i}\right)$.\\
9.3.2. Compute the variance of a random variable that is $\chi^{2}(r, \theta)$.\\
9.3.3. Three different medical procedures (A, B, and C) for a certain disease are under investigation. For the study, $3 m$ patients having this disease are to be selected and $m$ are to be assigned to each procedure. This common sample size $m$ must be determined. Let $\mu_{1}, \mu_{2}$, and $\mu_{3}$, be the means of the response of interest under treatments $\mathrm{A}, \mathrm{B}$, and C , respectively. The hypotheses are: $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$ versus $H_{1}: \mu_{j} \neq \mu_{j^{\prime}}$ for some $j \neq j^{\prime}$. To determine $m$, from a pilot study the experimenters use a guess of 30 of $\sigma^{2}$ and they select the significance level of 0.05 . They are interested in detecting the pattern of means: $\mu_{2}=\mu_{1}+5$ and $\mu_{3}=\mu_{1}+10$.\\
(a) Determine the noncentrality parameter under the above pattern of means.\\
(b) Use the R function pf to determine the powers of the $F$-test to detect the above pattern of means for $m=5$ and $m=10$.\\
(c) Determine the smallest value of $m$ so that the power of detection is at least 0.80 .\\
(d) Answer (a)-(c) if $\sigma^{2}=40$.\\
9.3.4. Show that the square of a noncentral $T$ random variable is a noncentral $F$ random variable.\\
9.3.5. Let $X_{1}$ and $X_{2}$ be two independent random variables. Let $X_{1}$ and $Y=$ $X_{1}+X_{2}$ be $\chi^{2}\left(r_{1}, \theta_{1}\right)$ and $\chi^{2}(r, \theta)$, respectively. Here $r_{1}<r$ and $\theta_{1} \leq \theta$. Show that $X_{2}$ is $\chi^{2}\left(r-r_{1}, \theta-\theta_{1}\right)$.

\subsection*{9.4 Multiple Comparisons}
For this section, consider the one-way ANONA model with $b$ treatments as described in expression (9.2.1) of Section 9.2. In that section, we developed the $F$-test\\
of the hypotheses of equal means, (9.2.2). In practice, besides this test, statisticians usually want to make pairwise comparisons of the form $\mu_{j}-\mu_{j^{\prime}}$. This is often called the Second Stage Analysis, while the $F$-test is consider the First Stage Analysis. The analysis for such comparisons usually consists of confidence intervals for the differences $\mu_{j}-\mu_{j^{\prime}}$ and $\mu_{j}$ is declared different from $\mu_{j^{\prime}}$ if 0 is not in the confidence interval. The random samples for treatments $j$ and $j^{\prime}$ are: $X_{1 j}, \ldots, X_{n_{j} j}$ from the $N\left(\mu_{j}, \sigma^{2}\right)$ distribution and $X_{1 j^{\prime}}, \ldots, X_{n_{j^{\prime}} j^{\prime}}$ from the $N\left(\mu_{j^{\prime}}, \sigma^{2}\right)$ distribution, which are independent random samples. Based on these samples the estimator of $\mu_{j}-\mu_{j^{\prime}}$ is $\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}}$. Further in the one-way analysis, an estimator of $\sigma^{2}$ is the full model estimator $\hat{\sigma}^{2} \Omega$ defined in expression (9.2.7). As discussed in Section 9.2, $(n-b) \hat{\sigma^{2}} \Omega / \sigma^{2}$ has a $\chi^{2}(n-b)$ distribution which is independent of all the sample means $\bar{X}_{. j}$. Hence, for a specified $\alpha$ it follows as in (4.2.13) of Chapter 4 that


\begin{equation*}
\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}} \pm t_{\alpha / 2, n-b} \hat{\sigma}_{\Omega} \sqrt{\frac{1}{n_{j}}+\frac{1}{n_{j^{\prime}}}} \tag{9.4.1}
\end{equation*}


is a $(1-\alpha) 100 \%$ confidence interval for $\mu_{j}-\mu_{j^{\prime}}$.\\
We often want to make many pairwise comparisons, though. For example, the first treatment might be a placebo or represent the standard treatment. In this case, there are $b-1$ pairwise comparisons of interest. On the other hand, we may want to make all $\binom{b}{2}$ pairwise comparisons. In making so many comparisons, while each confidence interval, (9.4.1), has confidence $(1-\alpha)$, it would seem that the overall confidence diminishes. As we next show, this slippage of overall confidence is true. These problems are often called Multiple Comparison Problems (MCP). In this section, we present several MCP procedures.

\section*{Bonferroni Multiple Comparison Procedure}
It is easy to motivate the Bonferroni Procedure while, at the same time, showing the slippage of confidence. This procedure is quite general and can be used in many settings not just the one-way design. So suppose we have $k$ parameters $\theta_{i}$ with $(1-\alpha) 100 \%$ confidence intervals $I_{i}, i=1, \ldots, k$, where $0<\alpha<1$ is given. Then the overall confidence is $P\left(\theta_{1} \in I_{1}, \ldots, \theta_{k} \in I_{k}\right)$. Using the method of complements, DeMorgan's Laws, and Boole's inequality, expression (1.3.7) of Chapter 1, we have


\begin{align*}
P\left(\theta_{1} \in I_{1}, \ldots, \theta_{k} \in I_{k}\right) & =1-P\left(\cup_{i=1}^{k} \theta_{i} \notin I_{i}\right) \\
& \geq 1-\sum_{i=1}^{k} P\left(\theta_{i} \notin I_{i}\right)=1-k \alpha \tag{9.4.2}
\end{align*}


The quantity $1-k \alpha$ is the lower bound on the slippage of confidence. For example, if $k=20$ and $\alpha=0.05$ then the overall confidence may be 0 . The Bonferroni procedure follows from expression (9.4.2). Simply change the confidence level of each confidence interval to $[1-(\alpha / k)]$. Then the overall confidence is at least $1-\alpha$.

For our one-way analysis, suppose we have $k$ differences of interest. Then the

Bonferroni confidence interval for $\mu_{j}-\mu_{j^{\prime}}$ is


\begin{equation*}
\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}} \pm t_{\alpha /(2 k), n-b} \hat{\sigma}_{\Omega} \sqrt{\frac{1}{n_{j}}+\frac{1}{n_{j^{\prime}}}} \tag{9.4.3}
\end{equation*}


While the overall confidence of the Bonferroni procedure is at least $(1-\alpha)$, for a large number of comparisons, the lengths of its intervals are wide; i.e., a loss in precision. We offer two other procedures that, generally, lessen this effect.

The R function mcpbon. $\mathrm{R}^{2}$ computes the Bonferroni procedure for all pairwise comparisons for a one-way design. The call is mcpbon( y , ind, alpha=0.05) where y is the vector of the combined samples and ind is the corresponding treatment vector. See Example 9.4.1 below.

\section*{Tukey's Multiple Comparison Procedure}
To state Tukey's procedure, we first need to define the Studentized range distribution.

Definition 9.4.1. Let $Y_{1}, \ldots, Y_{k}$ be iid $N\left(\mu, \sigma^{2}\right)$. Denote the range of these variables by $R=\max \left\{Y_{i}\right\}-\min \left\{Y_{i}\right\}$. Suppose $m S^{2} / \sigma^{2}$ has a $\chi^{2}(m)$ distribution which is independent of $Y_{1}, \ldots, Y_{k}$. Then we say that $Q=R / S$ has a Studentized range distribution with parameters $k$ and $m$.

The distribution of $Q$ cannot be obtained in close form but packages such as R have functions that compute the cdf and quantiles. In $R$, the call ptukey ( $x, k, m$ ) computes the cdf of $Q$ at $x$, while the call qtukey ( $\mathrm{p}, \mathrm{k}, \mathrm{m}$ ) returns the $p$ th quantile.

Consider the one-way design. First, assume that all the sample sizes are the same; i.e., for some positive integer $a, n_{\underline{j}}=a$, for all $j=1, \ldots, b$. Let $R=$ Range $\left\{\bar{X}_{\cdot}-\mu_{1}, \ldots, \bar{X}_{\cdot b}-\mu_{b}\right\}$. Then since $\bar{X}_{\cdot 1}-\mu_{1}, \ldots, \bar{X}_{\cdot b}-\mu_{b}$ are iid $N\left(0, \sigma^{2} / a\right)$, the random variable $Q=R /\left(\hat{\sigma}_{\Omega} / \sqrt{a}\right)$ has a Studentized range distribution with parameters $b$ and $n-b$. Let $q_{c}=q_{1-\alpha, b, n-b}$.

$$
\begin{aligned}
1-\alpha & =P\left(Q \leq q_{c}\right)=P\left(\max \left\{\bar{X}_{\cdot j}-\mu_{j}\right\}-\min \left\{\bar{X}_{\cdot j}-\mu_{j}\right\} \leq q_{c} \hat{\sigma}_{\Omega} / \sqrt{a}\right) \\
& =P\left(\left|\left(\mu_{j}-\mu_{j^{\prime}}\right)-\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}}\right)\right| \leq q_{c} \hat{\sigma}_{\Omega} / \sqrt{a}, \text { for all } j, j^{\prime}\right)
\end{aligned}
$$

If we expand the inequality in the last statement, we obtain the $(1-\alpha) 100 \%$ simultaneous confidence intervals for all pairwise differences given by


\begin{equation*}
\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}} \pm q_{1-\alpha, b, n-b} \frac{\hat{\sigma}_{\Omega}}{\sqrt{a}}, \quad \text { for all } j, j^{\prime} \text { in } 1, \ldots b \tag{9.4.4}
\end{equation*}


The statistician John Tukey developed these simultaneous confidence intervals for the balanced case. For the unbalanced case, first write the error term in (9.4.4) as

$$
\frac{q_{1-\alpha, b, n-b}}{\sqrt{2}} \hat{\sigma}_{\Omega} \sqrt{\frac{1}{a}+\frac{1}{a}}
$$

\footnotetext{${ }^{2}$ Downloadable at the site listed in the Preface.
}For the unbalanced case, this suggests the following intervals


\begin{equation*}
\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}} \pm \frac{q_{1-\alpha, b, n-b}}{\sqrt{2}} \hat{\sigma}_{\Omega} \sqrt{\frac{1}{n_{j}}+\frac{1}{n_{j^{\prime}}}} \text {, for all } j, j^{\prime} \text { in } 1, \ldots b \tag{9.4.5}
\end{equation*}


This correction is due to Kramer and these intervals are often referred to as the Tukey-Kramer multiple comparison procedure; see Miller (1981) for discussion. These intervals do not have exact confidence $(1-\alpha)$ but studies have indicated that if the unbalance is not severe the confidence is close to $(1-\alpha)$; see Dunnett (1980). Corresponding R code is shown in Example 9.4.1.

\section*{Fisher's PLSD Multiple Comparison Procedure}
The final procedure we discuss is Fisher's Protected Least Significance Difference (PLSD). The setting is the general (unbalanced) one-way design (9.2.1). This procedure is a two-stage procedure. It can be used for an arbitrary umber of comparisons but we state it for all comparisons. For a specified level of significance $\alpha$, Stage 1 consists of the $F$-test of the hypotheses of equal means, (9.2.2). If the test rejects at level $\alpha$ then Stage 2 consists of the usual pairwise ( $1-\alpha$ ) $100 \%$ confidence intervals, i.e.,


\begin{equation*}
\bar{X}_{\cdot j}-\bar{X}_{\cdot j^{\prime}} \pm t_{\alpha / 2, n-b} \hat{\sigma}_{\Omega} \sqrt{\frac{1}{n_{j}}+\frac{1}{n_{j^{\prime}}}} \text {, for all } j, j^{\prime} \text { in } 1, \ldots, b \tag{9.4.6}
\end{equation*}


If the test in Stage 1 fails to reject, users sometimes perform Stage 2 using the Bonferroni procedure. Fisher's procedure does not have overall coverage $1-\alpha$, but the initial $F$-test offers protection. Simulation studies have shown that Fisher's procedure performs well in terms of power and level; see, for instance, Carmer and Swanson (1973) and McKean et al. (1989). The R function ${ }^{3}$ mcpf isher .R computes this procedure as discussed in the next example.

Example 9.4.1 (Fast Cars). Kitchens (1997) discusses an experiment concerning the speed of cars. Five cars are considered: Acura (1), Ferrari (2), Lotus (3), Porsche (4), and Viper (5). For each car, 6 runs were made, 3 in each direction. For each run, the speed recorded is the maximum speed on the run achieved without exceeding the engine's redline. The data are in the file fastcars.rda. Figure 9.4.1 displays the comparison boxplots of the speeds versus the cars, which shows clearly that there are differences in speed due to the car. Ferrari and Porsche seem to be the fastest but are the differences significant? We assume the one-way design (9.2.1) and use R to do the computations. Key commands and corresponding results are given next. The overall $F$-test of the hypotheses of equal means, (9.2.2), is quite significant: $F=25.15$ with the $p$-value 0.0000 . We selected the Tukey MCP at level 0.05. The command below returns all $\binom{5}{2}=10$ pairwise comparisons, but in our summary we only list two.

\footnotetext{\#\#\# Code assumes that fastcars.rda has been loaded in R
}

\footnotetext{${ }^{3}$ Down loadable at the site listed in the Preface.
}\begin{verbatim}
> fit <- lm(speed~factor(car))
> anova(fit)
### F-Stat and p-value 25.145 1.903e-08
> aovfit <- aov(speed~factor(car))
> TukeyHSD(aovfit)
## Tukey's procedures of all pairwise comparisons are computed.
## Summary of a pertinent few
## Cars Mean-diff LB CI UB CI Sig??
## Porsche - Ferrari -2.6166667 -9.0690855 3.835752 NS
## Viper - Porsche -7.7333333 -14.1857522 -1.280914 Sig.
## Bonferroni
> mcpbon(speed,car)
## Porsche - Ferrari -2.6166667 -9.3795891 4.1462558 NS
## Viper - Porsche -7.7333333 -14.496255 -0.9704109 Sig.
2.197038 6.762922 0.9704109 14.49625578
## Fisher
> mcpfisher(speed,car)
## ftest 2.514542e+01 1.903360e-08
## Porsche - Ferrari -2.6166667 -7.141552 1.908219 NS
## Viper - Porsche -7.7333333 -12.258219 -3.208448 Sig.
\end{verbatim}

For discussion, we cite only two of Tukey's confidence intervals. As the second interval in the above printout shows, the mean speeds of both the Ferrari and Porsche are significantly faster than the mean speeds of the other cars. The difference between the Ferrari's and Porsche's mean speeds, though, is insignificant. Below the two Tukey confidence intervals, we display the results based on the Bonferroni and Fisher procedures. Note that all three procedures result in the same conclusions for these comparisons. The Bonferroni intervals are slightly larger than those of the Tukey procedure. The Fisher procedure gives the shortest intervals as expected.

In practice, the Tukey-Kramer procedure is often used, but there are many other multiple comparison procedures. A classical monograph on MCPs is Miller (1981) while Hus (1996) offers a more recent discussion.

\section*{EXERCISES}
9.4.1. For the study discussed in Exercise 9.2.8, obtain the results of Bonferroni multiple comparison procedure using $\alpha=0.10$. Based on this procedure, which brand of fuel if any is significantly best?\\
9.4.2. For the study discussed in Exercise 9.2.6, compute the Tukey-Kramer procedure. Are there any significant differences?\\
9.4.3. Suppose $X$ and $Y$ are discrete random variables that have the common range $\{1,2, \ldots, k\}$. Let $p_{1 j}$ and $p_{2 j}$ be the respective probabilities $P(X=j)$ and\\
\includegraphics[max width=\textwidth, center]{2025_05_06_e40e4b0ff5c2cb8edd3bg-546}

Figure 9.4.1: Boxplot of car speeds cited in Example 9.4.1.\\
$P(Y=j)$. Let $X_{1}, \ldots, X_{n_{1}}$ and $Y_{1}, \ldots, Y_{n_{2}}$ be respective independent random samples on $X$ and $Y$. The samples are recorded in a $2 \times k$ contingency table of counts $O_{i j}$, where $O_{1 j}=\#\left\{X_{i}=j\right\}$ and $O_{2 j}=\#\left\{Y_{i}=j\right\}$. In Example 4.7.3, based on this table, we discussed a test that the distributions of $X$ and $Y$ are the same. Here we want to consider all the differences $p_{1 j}-p_{2 j}$ for $j=1, \ldots, k$. Let $\hat{p}_{i j}=O_{i j} / n_{i}$.\\
(a) Determine the Bonferroni method for performing all these comparisons.\\
(b) Determine the Fisher method for performing all these comparisons.\\
9.4.4. Suppose the samples in Exercise 9.4.3 resulted in the contingency table:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
$x$ & 20 & 31 & 56 & 18 & 45 & 55 & 47 & 78 & 56 & 81 \\
\hline
$y$ & 36 & 41 & 65 & 15 & 38 & 78 & 18 & 72 & 59 & 85 \\
\hline
\end{tabular}
\end{center}

To compute (in R) the confidence intervals below, use the command prop.test as in Example 4.2.5.\\
(a) Based on the Bonferroni procedure for all 10 comparisons, compute the confidence interval for $p_{16}-p_{26}$.\\
(b) Based on the Fisher procedure for all 10 comparisons, compute the confidence interval for $p_{16}-p_{26}$.\\
9.4.5. Write an R function that computes the Fisher procedure of Exercise 9.4.3. Validate it using the data of Exercise 9.4.4.\\
9.4.6. Extend the Bonferroni procedure to simultaneous testing. That is, suppose we have $m$ hypotheses of interest: $H_{0 i}$ versus $H_{1 i}, i=1, \ldots, m$. For testing $H_{0 i}$ versus $H_{1 i}$, let $C_{i, \alpha}$ be a critical region of size $\alpha$ and assume $H_{0 i}$ is rejected if $\mathbf{X}_{i} \in C_{i, \alpha}$, for a sample $\mathbf{X}_{i}$. Determine a rule so that we can simultaneously test these $m$ hypotheses with a Type I error rate less than or equal to $\alpha$.

\subsection*{9.5 Two-Way ANOVA}
Recall the one-way analysis of variance (ANOVA) problem considered in Section 9.2 which was concerned with one factor at $b$ levels. In this section, we are concerned with the situation where we have two factors $A$ and $B$ with levels $a$ and $b$, respectively. This is called a two-way analysis of variance (ANOVA). Let $X_{i j}, i=1,2, \ldots, a$ and $j=1,2, \ldots, b$, denote the response for factor $A$ at level $i$ and factor B at level $j$. Denote the total sample size by $n=a b$. We shall assume that the $X_{i j} \mathrm{~s}$ are independent normally distributed random variables with common variance $\sigma^{2}$. Denote the mean of $X_{i j}$ by $\mu_{i j}$. The mean $\mu_{i j}$ is often referred to as the mean of the $(i, j)$ th cell. For our first model, we consider the additive model where


\begin{equation*}
\mu_{i j}=\bar{\mu}+\left(\bar{\mu}_{i .}-\bar{\mu}\right)+\left(\bar{\mu}_{\cdot j}-\bar{\mu}\right) ; \tag{9.5.1}
\end{equation*}


that is, the mean in the $(i, j)$ th cell is due to additive effects of the levels, $i$ of factor A and $j$ of factor $B$, over the average (constant) $\bar{\mu}$. Let $\alpha_{i}=\bar{\mu}_{i} .-\bar{\mu}, i=1, \ldots, a$; $\beta_{j}=\bar{\mu}_{. j}-\bar{\mu}, j=1, \ldots, b$; and $\mu=\bar{\mu}$. Then the model can be written more simply as


\begin{equation*}
\mu_{i j}=\mu+\alpha_{i}+\beta_{j}, \tag{9.5.2}
\end{equation*}


where $\sum_{i=1}^{a} \alpha_{i}=0$ and $\sum_{j=1}^{b} \beta_{j}=0$. We refer to this model as being a two-way additive ANOVA model.

For example, take $a=2, b=3, \mu=5, \alpha_{1}=1, \alpha_{2}=-1, \beta_{1}=1, \beta_{2}=0$, and $\beta_{3}=-1$. Then the cell means are

\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
 &  & \multicolumn{3}{|c|}{Factor B} \\
 &  & 1 & 2 & 3 \\
\hline
Factor A & 1 & $\mu_{11}=7$ & $\mu_{12}=6$ & $\mu_{13}=5$ \\
 & 2 & $\mu_{21}=5$ & $\mu_{22}=4$ & $\mu_{23}=3$ \\
\hline
\end{tabular}
\end{center}

Note that for each $i$, the plots of $\mu_{i j}$ versus $j$ are parallel. This is true for additive models in general; see Exercise 9.5.9. We call these plots mean profile plots.

Had we taken $\beta_{1}=\beta_{2}=\beta_{3}=0$, then the cell means would be

\begin{center}
\begin{tabular}{|lc|ccc|}
\hline
 &  & \multicolumn{3}{|c|}{Factor B} \\
 &  & 1 & 2 & 3 \\
\hline
Factor A & 1 & $\mu_{11}=6$ & $\mu_{12}=6$ & $\mu_{13}=6$ \\
 & 2 & $\mu_{21}=4$ & $\mu_{22}=4$ & $\mu_{23}=4$ \\
\hline
\end{tabular}
\end{center}

The hypotheses of interest are


\begin{equation*}
H_{0 A}: \alpha_{1}=\cdots=\alpha_{a}=0 \text { versus } H_{1 A}: \alpha_{i} \neq 0 \text {, for some } i, \tag{9.5.3}
\end{equation*}


and


\begin{equation*}
H_{0 B}: \beta_{1}=\cdots=\beta_{b}=0 \text { versus } H_{1 B}: \beta_{j} \neq 0, \text { for some } j \text {. } \tag{9.5.4}
\end{equation*}


If $H_{0 A}$ is true, then by (9.5.2) the mean of the $(i, j)$ th cell does not depend on the level of $A$. The second example above is under $H_{0 B}$. The cell means remain the same from column to column for a specified row. We call these hypotheses main effect hypotheses.

Remark 9.5.1. The model just described, and others similar to it, are widely used in statistical applications. Consider a situation in which it is desirable to investigate the effects of two factors that influence an outcome. Thus the variety of a grain and the type of fertilizer used influence the yield; or the teacher and the size of the class may influence the score on a standardized test. Let $X_{i j}$ denote the yield from the use of variety $i$ of a grain and type $j$ of fertilizer. A test of the hypothesis that $\beta_{1}=\beta_{2}=\cdots=\beta_{b}=0$ would then be a test of the hypothesis that the mean yield of each variety of grain is the same regardless of the type of fertilizer used.

Call the model described around expression (9.5.2) the full model. We want to determine the mles. If we write out the likelihood function, the summation in the exponent of $e$ is

$$
S S=\sum_{i=1}^{a} \sum_{j=1}^{b}\left(x_{i j}-\bar{\mu}-\alpha_{i}-\beta_{j}\right)^{2} .
$$

The mles of $\alpha_{i}, \beta_{j}$, and $\bar{\mu}$ minimize $S S$. By adding in and subtracting out, we obtain:\\
$S S=\sum_{i=1}^{a} \sum_{j=1}^{b}\left\{\left[\bar{x}_{. .}-\bar{\mu}\right]-\left[\alpha_{i}-\left(\bar{x}_{i .}-\bar{x}_{. .}\right)\right]-\left[\beta_{j}-\left(\bar{x}_{. j}-\bar{x}_{. .}\right)\right]+\left[x_{i j}-\bar{x}_{i .}-\bar{x}_{. j}+\bar{x}_{. .}\right]\right\}^{2}$.\\
From expression (9.5.2), we have $\sum_{i} \alpha_{i}=\sum_{j} \beta_{j}=0$. Further,

$$
\sum_{i=1}^{a}\left(\bar{x}_{i .}-\bar{x}_{. .}\right)=\sum_{j=1}^{b}\left(\bar{x}_{. j}-\bar{x}_{. .}\right)=0
$$

and

$$
\sum_{i=1}^{a}\left(x_{i j}-\bar{x}_{i .}-\bar{x}_{. j}+\bar{x}_{. .}\right)=\sum_{j=1}^{b}\left(x_{i j}-\bar{x}_{i .}-\bar{x}_{\cdot j}+\bar{x}_{. .}\right)=0
$$

Therefore, in the expansion of the sum of squares, (9.5.5), all cross product terms are 0 . Hence, we have the identity


\begin{align*}
S S= & a b[\bar{x} . .-\bar{\mu}]^{2}+b \sum_{i=1}^{a}\left[\alpha_{i}-\left(\bar{x}_{i} .-\bar{x}_{. .}\right)\right]^{2}+a \sum_{j=1}^{b}\left[\beta_{j}-\left(\bar{x}_{. j}-\bar{x}_{. .}\right)\right]^{2} \\
& +\sum_{i=1}^{a} \sum_{j=1}^{b}\left[x_{i j}-\bar{x}_{i .}-\bar{x}_{\cdot j}+\bar{x}_{. .}\right]^{2} . \tag{9.5.6}
\end{align*}


Since these are sums of squares, the minimizing values, (mles), must be


\begin{equation*}
\hat{\bar{\mu}}=\bar{X}_{. .}, \hat{\alpha}_{i}=\bar{X}_{i .}-\bar{X}_{. .}, \text {and } \hat{\beta}_{j}=\bar{X}_{. j}-\bar{X}_{\ldots} \tag{9.5.7}
\end{equation*}


Note that we have used random variable notation. So these are the maximum likelihood estimators. It then follows that the maximum likelihood estimator of $\sigma^{2}$ is


\begin{equation*}
\hat{\sigma}_{\Omega}^{2}=\frac{\sum_{i=1}^{a} \sum_{j=1}^{b}\left[X_{i j}-\bar{X}_{i .}-\bar{X}_{. j}+\bar{X}_{. .}\right]^{2}}{a b}=\operatorname{dfn} \frac{Q_{3}^{\prime}}{a b}, \tag{9.5.8}
\end{equation*}


where we have defined the numerator of $\hat{\sigma}_{\Omega}^{2}$ as the quadratic form $Q_{3}^{\prime}$. It follows from an advanced course in linear models that $a b \hat{\sigma}_{\Omega}^{2} / \sigma^{2}$ has a $\chi^{2}((a-1)(b-1))$ distribution.

Next we construct the likelihood ratio test for $H_{0 B}$. Under the reduced model (full model constrained by $H_{0 B}$ ), $\beta_{j}=0$ for all $j=1, \ldots, b$. To obtain the mles for the reduced model, the identity (9.5.6) becomes


\begin{align*}
S S= & a b[\bar{x} . .-\bar{\mu}]^{2}+b \sum_{i=1}^{a}\left[\alpha_{i}-\left(\bar{x}_{i .}-\bar{x}_{. .}\right)\right]^{2} \\
& +a \sum_{j=1}^{b}\left[\bar{x}_{\cdot j}-\bar{x}_{. .}\right]^{2}+\sum_{i=1}^{a} \sum_{j=1}^{b}\left[x_{i j}-\bar{x}_{i .}-\bar{x}_{. j}+\bar{x}_{. .}\right]^{2} . \tag{9.5.9}
\end{align*}


Thus the mles for $\alpha_{i}$ and $\bar{\mu}$ remain the same as in the full model and the reduced model maximum likelihood estimator of $\sigma^{2}$ is


\begin{equation*}
\hat{\sigma}_{\omega}^{2}=\frac{\left\{a \sum_{j=1}^{b}\left[\bar{X}_{. j}-\bar{X}_{. .}\right]^{2}+\sum_{i=1}^{a} \sum_{j=1}^{b}\left[X_{i j}-\bar{X}_{i .}-\bar{X}_{. j}+\bar{X}_{. .}\right]^{2}\right\}}{a b} . \tag{9.5.10}
\end{equation*}


Denote the numerator of $\hat{\sigma}_{\omega}^{2}$ by $Q^{\prime}$. Note that it is the residual variation left after fitting the reduced model.

Let $\Lambda$ denote the likelihood ratio test statistic for $H_{0 B}$. Our derivation is similar to the derivation for the likelihood ratio test statistic for one-way ANOVA of Section 9.2. Hence, similar to equation (9.2.9), our likelihood ratio test statistic simplifies to

$$
\Lambda^{a b / 2}=\frac{\hat{\sigma}_{\Omega}^{2}}{\hat{\sigma}_{\omega}^{2}}=\frac{Q_{3}^{\prime}}{Q^{\prime}} .
$$

Then, similar to the one-way derivation, the likelihood ratio test rejects $H_{0 B}$ for large values of $Q_{4}^{\prime} / Q_{3}^{\prime}$, where in this case,


\begin{equation*}
Q_{4}^{\prime}=a \sum_{j=1}^{b}\left[\bar{x}_{. j}-\bar{x}_{. .}\right]^{2} . \tag{9.5.11}
\end{equation*}


Note that $Q_{4}^{\prime}=Q^{\prime}-Q_{3}^{\prime}$; i.e., it is the incremental increase in residual variation if we use the reduced model instead of the full model.

To obtain the null distribution of $Q_{4}^{\prime}$, notice that it is the numerator of the sample variance of the random variables $\sqrt{a} \bar{X}_{\cdot 1}, \ldots, \sqrt{a} \bar{X}_{. b}$. These random variables are\\
independent with the common $N\left(\sqrt{a} \bar{\mu}, \sigma^{2}\right)$ distribution; see Exercise 9.5.2. Hence, by Theorem 3.6.1, $Q_{4}^{\prime} / \sigma^{2}$ has $\chi^{2}(b-1)$ distribution. In a more advanced course, it can be further shown that $Q_{4}^{\prime}$ and $Q_{3}^{\prime}$ are independent. Hence, the statistic


\begin{equation*}
F_{B}=\frac{a \sum_{j=1}^{b}\left[\bar{X}_{. j}-\bar{X}_{. .}\right]^{2} /(b-1)}{\sum_{i=1}^{a} \sum_{j=1}^{b}\left[X_{i j}-\bar{X}_{i .}-\bar{X}_{. j}+\bar{X}_{. .}\right]^{2} /(a-1)(b-1)} \tag{9.5.12}
\end{equation*}


has an $F(b-1,(a-1)(b-1))$ under $H_{0 B}$. Thus, a level $\alpha$ test is to reject $H_{0 B}$ in favor of $H_{1 B}$ if


\begin{equation*}
F_{B} \geq F(\alpha, b-1,(a-1)(b-1)) \tag{9.5.13}
\end{equation*}


If we are to compute the power function of the test, we need the distribution of $F_{B}$ when $H_{0 B}$ is not true. As we have stated above, $Q_{3}^{\prime} / \sigma^{2},(9.5 .8)$, has a central $\chi^{2}$-distribution with $(a-1)(b-1)$ degrees of freedom under the full model, and, hence, under $H_{1 B}$. Further, it can be shown that $Q_{4}^{\prime},(9.5 .11)$, has a noncentral $\chi^{2}$ distribution with $b-1$ degrees of freedom under $H_{1 B}$. To compute the noncentrality parameters of $Q_{4}^{\prime} / \sigma^{2}$ when $H_{1 B}$ is true, we have $E\left(X_{i j}\right)=\mu+\alpha_{i}+\beta_{j}, E\left(\bar{X}_{i .}\right)=$ $\mu+\alpha_{i}, E\left(\bar{X}_{. j}\right)=\mu+\beta_{j}$, and $E\left(\bar{X}_{. .}\right)=\mu$. Using the general rule discussed in Section 9.4, we replace the variables in $Q_{4}^{\prime} / \sigma^{2}$ with their means. Accordingly, the noncentrality parameter $Q_{4}^{\prime} / \sigma^{2}$ is

$$
\frac{a}{\sigma^{2}} \sum_{j=1}^{b}\left(\mu+\beta_{j}-\mu\right)^{2}=\frac{a}{\sigma^{2}} \sum_{j=1}^{b} \beta_{j}^{2} .
$$

Thus, if the hypothesis $H_{0 B}$ is not true, $F$ has a noncentral $F$-distribution with $b-1$ and $(a-1)(b-1)$ degrees of freedom and noncentrality parameter $a \sum_{j=1}^{b} \beta_{j}^{2} / \sigma^{2}$.

A similar argument can be used to construct the likelihood ratio test statistics $F_{A}$ to test $H_{0 A}$ versus $H_{1 A},(9.5 .3)$. The numerator of the $F$ test statistic is the sum of squares among rows. The test statistic is


\begin{equation*}
F_{A}=\frac{b \sum_{i=1}^{a}\left[\bar{X}_{i .}-\bar{X}_{. .}\right]^{2} /(a-1)}{\sum_{i=1}^{a} \sum_{j=1}^{b}\left[X_{i j}-\bar{X}_{i .}-\bar{X}_{. j}+\bar{X} . .\right]^{2} /(a-1)(b-1)} \tag{9.5.14}
\end{equation*}


and it has an $F(a-1,(a-1)(b-1))$ distribution under $H_{0 A}$.

\subsection*{9.5.1 Interaction between Factors}
The analysis of variance problem that has just been discussed is usually referred to as a two-way classification with one observation per cell. Each combination of $i$ and $j$ determines a cell; thus, there is a total of $a b$ cells in this model. Let us now investigate another two-way classification problem, but in this case we take $c>1$ independent observations per cell.

Let $X_{i j k}, i=1,2, \ldots, a, j=1,2, \ldots, b$, and $k=1,2, \ldots, c$, denote $n=a b c$ random variables that are independent and have normal distributions with common, but unknown, variance $\sigma^{2}$. Denote the mean of each $X_{i j k}, k=1,2, \ldots, c$, by $\mu_{i j}$.

Under the additive model, (9.5.1), the mean of each cell depended on its row and column, but often the mean is cell-specific. To allow this, consider the parameters

$$
\begin{aligned}
\gamma_{i j} & =\mu_{i j}-\left\{\mu+\left(\bar{\mu}_{i .}-\mu\right)+\left(\bar{\mu}_{\cdot j}-\mu\right)\right\} \\
& =\mu_{i j}-\bar{\mu}_{i \cdot}-\bar{\mu}_{\cdot j}+\mu,
\end{aligned}
$$

for $i=1, \ldots a, j=1, \ldots, b$. Hence $\gamma_{i j}$ reflects the specific contribution to the cell mean over and above the additive model. These parameters are called interaction parameters. Using the second form (9.5.2), we can write the cell means as


\begin{equation*}
\mu_{i j}=\mu+\alpha_{i}+\beta_{j}+\gamma_{i j}, \tag{9.5.15}
\end{equation*}


where $\sum_{i=1}^{a} \alpha_{i}=0, \sum_{j=1}^{b} \beta_{j}=0$, and $\sum_{i=1}^{a} \gamma_{i j}=\sum_{j=1}^{b} \gamma_{i j}=0$. This model is called a two-way model with interaction.

For example, take $a=2, b=3, \mu=5, \alpha_{1}=1, \alpha_{2}=-1, \beta_{1}=1, \beta_{2}=0$, $\beta_{3}=-1, \gamma_{11}=1, \gamma_{12}=1, \gamma_{13}=-2, \gamma_{21}=-1, \gamma_{22}=-1$, and $\gamma_{23}=2$. Then the cell means are

\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
 &  & \multicolumn{3}{|c|}{Factor B} \\
 &  & 1 & 2 & 3 \\
\hline
Factor A & 1 & $\mu_{11}=8$ & $\mu_{12}=7$ & $\mu_{13}=3$ \\
 & 2 & $\mu_{21}=4$ & $\mu_{22}=3$ & $\mu_{23}=5$ \\
\hline
\end{tabular}
\end{center}

If each $\gamma_{i j}=0$, then the cell means are

\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
 &  & \multicolumn{3}{|c|}{Factor B} \\
 &  & 1 & 2 & 3 \\
\hline
Factor A & 1 & $\mu_{11}=7$ & $\mu_{12}=6$ & $\mu_{13}=5$ \\
 & 2 & $\mu_{21}=5$ & $\mu_{22}=4$ & $\mu_{23}=3$ \\
\hline
\end{tabular}
\end{center}

Note that the mean profile plots for this second example are parallel, but those in the first example (where interaction is present) are not.

The derivation of the mles under the full model, (9.5.15), is quite similar to the derivation for the additive model. Letting $S S$ denote the sums of squares in the exponent of $e$ in the likelihood function, we obtain the following identity by adding in and subtracting out (we have omitted subscripts on the sums):


\begin{align*}
S S= & \sum \sum \sum\left(x_{i j k}-\mu-\alpha_{i}-\beta_{j}-\gamma_{i j k}\right)^{2} \\
= & \sum \sum \sum\left\{\left[x_{i j k}-\bar{x}_{i j} .\right]-\left[\mu-\bar{x}_{\ldots . .}\right]-\left[\alpha_{i}-\left(\bar{x}_{i . .}-\bar{x}_{\ldots}\right)\right]-\left[\beta_{j}-\left(\bar{x}_{. j} .-\bar{x}_{\ldots .}\right)\right]\right. \\
& -\left[\gamma_{i j}-\left(\bar{x}_{i j .}-\bar{x}_{i . .}-\bar{x}_{. j .}+\bar{x}_{\ldots . .}\right\}^{2}\right. \\
= & \sum \sum \sum\left[x_{i j k}-\bar{x}_{i j .}\right]^{2}+a b c\left[\mu-\bar{x}_{\ldots . .}\right]^{2}+b c \sum\left[\alpha_{i}-\left(\bar{x}_{i . .}-\bar{x}_{\ldots}\right)\right]^{2}+ \\
& a c \sum\left[\beta_{j}-\left(\bar{x}_{. j .}-\bar{x}_{\ldots .}\right)\right]^{2}+c \sum \sum\left[\gamma_{i j}-\left(\bar{x}_{i j .}-\bar{x}_{i . .}-\bar{x}_{. j .}+\bar{x}_{\ldots .}\right)\right]^{2} \tag{9.5.16}
\end{align*}


where, as in the additive model, the cross product terms in the expansion are 0 . Thus, the mles of $\mu, \alpha_{i}$ and $\beta_{j}$ are the same as in the additive model; the mle of $\gamma_{i j}$ is $\hat{\gamma}_{i j}=\bar{X}_{i j .}-\bar{X}_{i . .}-\bar{X}_{. j}+\bar{X}_{\ldots} .$. ; and the mle of $\sigma^{2}$ is


\begin{equation*}
\hat{\sigma}_{\Omega}^{2}=\frac{\sum \sum \sum\left[X_{i j k}-\bar{X}_{i j}\right]^{2}}{a b c} . \tag{9.5.17}
\end{equation*}


Let $Q_{3}^{\prime \prime}$ denote the numerator of $\hat{\sigma}^{2}$.\\
The major hypotheses of interest for the interaction model are


\begin{equation*}
H_{0 A B}: \gamma_{i j}=0 \text { for all } i, j \text { versus } H_{1 A B}: \gamma_{i j} \neq 0, \text { for some } i, j \tag{9.5.18}
\end{equation*}


Substituting $\gamma_{i j}=0$ in $S S$, it is clear that the reduced model mle of $\sigma^{2}$ is


\begin{equation*}
\hat{\sigma}_{\omega}^{2}=\frac{\sum \sum \sum\left[X_{i j k}-\bar{X}_{i j .}\right]^{2}+c \sum \sum\left[\bar{X}_{i j .}-\bar{X}_{i . .}-\bar{X}_{. j .}+\bar{X}_{\ldots . .}\right]^{2}}{a b c} \tag{9.5.19}
\end{equation*}


Let $Q^{\prime \prime}$ denote the numerator of $\hat{\sigma}_{\omega}^{2}$ and let $Q_{4}^{\prime \prime}=Q^{\prime \prime}-Q_{3}^{\prime \prime}$. Then it follows as in the additive model that the likelihood ratio test statistic rejects $H_{0 A B}$ for large values of $Q_{4}^{\prime \prime} / Q_{3}^{\prime \prime}$. In a more advanced class, it is shown that the standardized test statistic


\begin{equation*}
F_{A B}=\frac{Q_{4}^{\prime \prime} /[(a-1)(b-1)]}{Q_{3}^{\prime \prime} /[a b(c-1)]} \tag{9.5.20}
\end{equation*}


has under $H_{0 A B}$ an $F$-distribution with $(a-1)(b-1)$ and $a b(c-1)$ degrees of freedom.

If $H_{0 A B}: \gamma_{i j}=0$ is accepted, then one usually continues to test $\alpha_{i}=0, i=$ $1,2, \ldots, a$, by using the test statistic

$$
F=\frac{b c \sum_{i=1}^{a}\left(\bar{X}_{i . .}-\bar{X}_{\ldots}\right)^{2} /(a-1)}{\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{c}\left(X_{i j k}-\bar{X}_{i j} .\right)^{2} /[a b(c-1)]}
$$

which has a null $F$-distribution with $a-1$ and $a b(c-1)$ degrees of freedom. Similarly, the test of $\beta_{j}=0, j=1,2, \ldots, b$, proceeds by using the test statistic

$$
F=\frac{a c \sum_{j=1}^{b}\left(\bar{X}_{\cdot j \cdot}-\bar{X}_{\ldots .}\right)^{2} /(b-1)}{\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{c}\left(X_{i j k}-\bar{X}_{i j .}\right)^{2} /[a b(c-1)]}
$$

which has a null $F$-distribution with $b-1$ and $a b(c-1)$ degrees of freedom.\\
We conclude this section with an example that serves as an illustration of twoway ANOVA along with its associated R code.\\
Example 9.5.1. Devore (2012), page 435, presents a study concerning the effects to the thermal conductivity of an asphalt mix due to two factors: Binder Grade at three different levels (PG58, PG64, and PG70) and Coarseness of Aggregate Content at three levels ( $38 \%, 41 \%$, and $44 \%$ ). Hence, there are $3 \times 3=9$ different treatments. The responses are the thermal conductivities of the mixes of asphalt at these crossed levels. Two replications were performed at each treatment. The data are:

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
 & \multicolumn{3}{|c|}{Coarse Aggregate Content} \\
\hline
Binder-Grade & $38 \%$ & $41 \%$ & $44 \%$ \\
\hline
PG58 & 0.835 & 0.822 & 0.785 \\
 & 0.845 & 0.826 & 0.795 \\
\hline
PG64 & 0.855 & 0.832 & 0.790 \\
 & 0.865 & 0.836 & 0.800 \\
\hline
PG70 & 0.815 & 0.800 & 0.770 \\
 & 0.825 & 0.820 & 0.790 \\
\hline
\end{tabular}
\end{center}

The data are also in the file conductivity.rda. Assuming this file has been loaded into the R work area, the mean profile plot is computed by

\begin{verbatim}
interaction.plot(Binder,Aggregate,Conductivity,legend=T)
\end{verbatim}

and it is displayed in Figure 9.5.1. Note that the mean profiles are almost parallel, a graphical indication of little interaction between the factors. The ANOVA for the study is computed by the following two commands. It yields the tabled results (which we have abbreviated). The next to last column shows the $F$-test statistics discussed in this section.

\begin{verbatim}
fit=lm(Conductivity ~ factor(Binder) + factor(Aggregate) +
factor(Binder)*factor(Aggregate))
anova(fit)
Analysis of Variance Table
    Df Sum Sq F value Pr (>F)
factor(Binder) 2 0.0020893 14.1171 0.001678
factor(Aggregate) 2 0.0082973 56.0631 8.308e-06
factor(Binder):factor(Aggregate) 4 0.0003253 1.0991 0.413558
\end{verbatim}

As the interaction plot suggests, interaction is not significant ( $p=0.4135$ ). In practice, we would accept the additive (no interaction) model. The main effects are both highly significant. So both factors have an effect on conductivity. See Devore (2012) for more discussion.

\section*{EXERCISES}
9.5.1. For the two-way interaction model, (9.5.15), show that the following decomposition of sums of squares is true:

$$
\begin{aligned}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{c}\left(X_{i j k}-\bar{X}_{\ldots} \ldots\right)^{2}= & b c \sum_{i=1}^{a}\left(\bar{X}_{i . .}-\bar{X}_{\ldots .}\right)^{2}+a c \sum_{j=1}^{b}\left(\bar{X}_{. j .}-\bar{X}_{\ldots . .}\right)^{2} \\
& +c \sum_{i=1}^{a} \sum_{j=1}^{b}\left(\bar{X}_{i j .}-\bar{X}_{i . .}-\bar{X}_{. j .}+\bar{X}_{\ldots}\right)^{2} \\
& +\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{c}\left(X_{i j k}-\bar{X}_{i j .}\right)^{2} ;
\end{aligned}
$$

that is, the total sum of squares is decomposed into that due to row differences, that due to column differences, that due to interaction, and that within cells.\\
\includegraphics[max width=\textwidth, center]{2025_05_06_e40e4b0ff5c2cb8edd3bg-554}

Figure 9.5.1: Mean profile plot for the study discussed in Example 9.5.1. The profiles are nearly parallel, indicating little interaction between the factors.\\
9.5.2. Consider the discussion above expression (9.5.14). Show that the random variables $\sqrt{a} \bar{X}_{\cdot 1}, \ldots, \sqrt{a} \bar{X}_{\cdot b}$ are independent with the common $N\left(\sqrt{a} \bar{\mu}, \sigma^{2}\right)$ distribution.\\
9.5.3. For the two-way interaction model, (9.5.15), show that the noncentrality parameter of the test statistic $F_{A B}$ is equal to $c \sum_{j=1}^{b} \sum_{i=1}^{a} \gamma_{i j}^{2} / \sigma^{2}$.\\
9.5.4. Using the background of the two-way classification with one observation per cell, determine the distribution of the maximum likelihood estimators of $\alpha_{i}, \beta_{j}$, and $\mu$.\\
9.5.5. Prove that the linear functions $X_{i j}-\bar{X}_{i .}-\bar{X}_{. j}+\bar{X}_{. .}$and $\bar{X}_{. j}-\bar{X}_{. .}$are uncorrelated, under the assumptions of this section.\\
9.5.6. Given the following observations associated with a two-way classification with $a=3$ and $b=4$, use R or another statistical package to compute the $F$ statistic used to test the equality of the column means ( $\beta_{1}=\beta_{2}=\beta_{3}=\beta_{4}=0$ ) and the equality of the row means ( $\alpha_{1}=\alpha_{2}=\alpha_{3}=0$ ), respectively.

\begin{center}
\begin{tabular}{ccccc}
\hline
Row/Column & 1 & 2 & 3 & 4 \\
\hline
1 & 3.1 & 4.2 & 2.7 & 4.9 \\
2 & 2.7 & 2.9 & 1.8 & 3.0 \\
3 & 4.0 & 4.6 & 3.0 & 3.9 \\
\hline
\end{tabular}
\end{center}

9.5.7. With the background of the two-way classification with $c>1$ observations per cell, determine the distribution of the mles of $\alpha_{i}, \beta_{j}$, and $\gamma_{i j}$.\\
9.5.8. Given the following observations in a two-way classification with $a=3$, $b=4$, and $c=2$, compute the $F$-statistics used to test that all interactions are equal to zero $\left(\gamma_{i j}=0\right)$, all column means are equal $\left(\beta_{j}=0\right)$, and all row means are equal $\left(\alpha_{i}=0\right)$, respectively. Data are in the form $x_{i j k}, i, j$ in the data set sec951.rda.

\begin{center}
\begin{tabular}{ccccc}
\hline
Row/Column & 1 & 2 & 3 & 4 \\
\hline
1 & 3.1 & 4.2 & 2.7 & 4.9 \\
 & 2.9 & 4.9 & 3.2 & 4.5 \\
2 & 2.7 & 2.9 & 1.8 & 3.0 \\
 & 2.9 & 2.3 & 2.4 & 3.7 \\
3 & 4.0 & 4.6 & 3.0 & 3.9 \\
 & 4.4 & 5.0 & 2.5 & 4.2 \\
\hline
\end{tabular}
\end{center}

9.5.9. For the additive model (9.5.1), show that the mean profile plots are parallel. The sample mean profile plots are given by plotting $\bar{X}_{i j}$. versus $j$, for each $i$. These offer a graphical diagnostic for interaction detection. Obtain these plots for the last exercise.\\
9.5.10. We wish to compare compressive strengths of concrete corresponding to $a=3$ different drying methods (treatments). Concrete is mixed in batches that are just large enough to produce three cylinders. Although care is taken to achieve uniformity, we expect some variability among the $b=5$ batches used to obtain the following compressive strengths. (There is little reason to suspect interaction, and hence only one observation is taken in each cell.) Data are also in the data set sec95set2.rda.

\begin{center}
\begin{tabular}{cccccc}
\hline
 & \multicolumn{5}{c}{Batch} \\
\cline { 2 - 6 }
Treatment & $B_{1}$ & $B_{2}$ & $B_{3}$ & $B_{4}$ & $B_{5}$ \\
\hline
$A_{1}$ & 52 & 47 & 44 & 51 & 42 \\
$A_{2}$ & 60 & 55 & 49 & 52 & 43 \\
$A_{3}$ & 56 & 48 & 45 & 44 & 38 \\
\hline
\end{tabular}
\end{center}

(a) Use the $5 \%$ significance level and test $H_{A}: \alpha_{1}=\alpha_{2}=\alpha_{3}=0$ against all alternatives.\\
(b) Use the $5 \%$ significance level and test $H_{B}: \beta_{1}=\beta_{2}=\beta_{3}=\beta_{4}=\beta_{5}=0$ against all alternatives.\\
9.5.11. With $a=3$ and $b=4$, find $\mu, \alpha_{i}, \beta_{j}$ and $\gamma_{i j}$ if $\mu_{i j}$, for $i=1,2,3$ and $j=1,2,3,4$, are given by

\begin{center}
\begin{tabular}{rrrr}
6 & 7 & 7 & 12 \\
10 & 3 & 11 & 8 \\
8 & 5 & 9 & 10 \\
\end{tabular}
\end{center}

\subsection*{9.6 A Regression Problem}
There is often interest in the relationship between two variables, for example, a student's scholastic aptitude test score in mathematics and this same student's\\
grade in calculus. Frequently, one of these variables, say $x$, is known in advance of the other and there is interest in predicting a future random variable $Y$. Since $Y$ is a random variable, we cannot predict its future observed value $Y=y$ with certainty. Thus let us first concentrate on the problem of estimating the mean of $Y$, that is, $E(Y)$. Now $E(Y)$ is usually a function of $x$; for example, in our illustration with the calculus grade, say $Y$, we would expect $E(Y)$ to increase with increasing mathematics aptitude score $x$. Sometimes $E(Y)=\mu(x)$ is assumed to be of a given form, such as a linear or quadratic or exponential function; that is, $\mu(x)$ could be assumed to be equal to $\alpha+\beta x$ or $\alpha+\beta x+\gamma x^{2}$ or $\alpha e^{\beta x}$. To estimate $E(Y)=\mu(x)$, or equivalently the parameters $\alpha, \beta$, and $\gamma$, we observe the random variable $Y$ for each of $n$ possible different values of $x$, say $x_{1}, x_{2}, \ldots, x_{n}$, which are not all equal. Once the $n$ independent experiments have been performed, we have $n$ pairs of known numbers $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)$. These pairs are then used to estimate the mean $E(Y)$. Problems like this are often classified under regression because $E(Y)=\mu(x)$ is frequently called a regression curve.\\
Remark 9.6.1. A model for the mean such as $\alpha+\beta x+\gamma x^{2}$ is called a linear model because it is linear in the parameters $\alpha, \beta$, and $\gamma$. Thus $\alpha e^{\beta x}$ is not a linear model because it is not linear in $\alpha$ and $\beta$. Note that, in Sections 9.2 to 9.5, all the means were linear in the parameters and hence are linear models.

For the most part in this section, we consider the case in which $E(Y)=\mu(x)$ is a linear function. Denote by $Y_{i}$ the response at $x_{i}$ and consider the model


\begin{equation*}
Y_{i}=\alpha+\beta\left(x_{i}-\bar{x}\right)+e_{i}, \quad i=1, \ldots, n, \tag{9.6.1}
\end{equation*}


where $\bar{x}=n^{-1} \sum_{i=1}^{n} x_{i}$ and $e_{1}, \ldots, e_{n}$ are iid random variables with a common $N\left(0, \sigma^{2}\right)$ distribution. Hence $E\left(Y_{i}\right)=\alpha+\beta\left(x_{i}-\bar{x}\right), \operatorname{Var}\left(Y_{i}\right)=\sigma^{2}$, and $Y_{i}$ has $N\left(\alpha+\beta\left(x_{i}-\bar{x}\right), \sigma^{2}\right)$ distribution. The major assumption is that the random errors, $e_{i}$, are iid. In particular, this means that the errors are not a function of the $x_{i}$ 's. This is discussed in Remark 9.6.3. First, we discuss the maximum likelihood estimates of the parameters $\alpha, \beta$, and $\sigma$.

\subsection*{9.6.1 Maximum Likelihood Estimates}
Assume that the $n$ points $\left(x_{1}, Y_{1}\right),\left(x_{2}, Y_{2}\right), \ldots,\left(x_{n}, Y_{n}\right)$ follow Model 9.6.1. So the first problem is that of fitting a straight line to the set of points; i.e., estimating $\alpha$ and $\beta$. As an aid to our discussion, Figure 9.6 .1 shows a scatterplot of 60 observations $\left(x_{1}, y_{1}\right), \ldots,\left(x_{60}, y_{60}\right)$ simulated from a linear model of the form (9.6.1). Our method of estimation in this section is that of maximum likelihood (mle). The joint pdf of $Y_{1}, \ldots, Y_{n}$ is the product of the individual probability density functions; that is, the likelihood function equals

$$
\begin{aligned}
L\left(\alpha, \beta, \sigma^{2}\right) & =\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left\{-\frac{\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}}{2 \sigma^{2}}\right\} \\
& =\left(\frac{1}{2 \pi \sigma^{2}}\right)^{n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}\right\} .
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2025_05_06_e40e4b0ff5c2cb8edd3bg-557}
\end{center}

Figure 9.6.1: The plot shows the least squares fitted line (solid line) to a set of data. The dashed-line segment from $\left(x_{i}, \hat{y}_{i}\right)$ to $\left(x_{i}, y_{i}\right)$ shows the deviation of $\left(x_{i}, y_{i}\right)$ from its fit.

To maximize $L\left(\alpha, \beta, \sigma^{2}\right)$, or, equivalently, to minimize

$$
-\log L\left(\alpha, \beta, \sigma^{2}\right)=\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)+\frac{\sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}}{2 \sigma^{2}},
$$

we must select $\alpha$ and $\beta$ to minimize

$$
H(\alpha, \beta)=\sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2} .
$$

Since $\left|y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right|=\left|y_{i}-\mu\left(x_{i}\right)\right|$ is the vertical distance from the point $\left(x_{i}, y_{i}\right)$ to the line $y=\mu(x)$ (see the dashed-line segment in Figure 9.6.1), we note that $H(\alpha, \beta)$ represents the sum of the squares of those distances. Thus, selecting $\alpha$ and $\beta$ so that the sum of the squares is minimized means that we are fitting the straight line to the data by the method of least squares (LS).

To minimize $H(\alpha, \beta)$, we find the two first partial derivatives,

$$
\frac{\partial H(\alpha, \beta)}{\partial \alpha}=2 \sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right](-1)
$$

and

$$
\frac{\partial H(\alpha, \beta)}{\partial \beta}=2 \sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]\left[-\left(x_{i}-\bar{x}\right)\right] .
$$

Setting $\partial H(\alpha, \beta) / \partial \alpha=0$, we obtain


\begin{equation*}
\sum_{i=1}^{n} y_{i}-n \alpha-\beta \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=0 \tag{9.6.2}
\end{equation*}


Since $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=0$, the equation becomes $\sum_{i=1}^{n} y_{i}-n \alpha=0$; hence, the mle of $\alpha$ is


\begin{equation*}
\hat{\alpha}=\bar{Y} . \tag{9.6.3}
\end{equation*}


The equation $\partial H(\alpha, \beta) / \partial \beta=0$ yields, with $\alpha$ replaced by $\bar{y}$,


\begin{equation*}
\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)\left(x_{i}-\bar{x}\right)-\beta \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=0 \tag{9.6.4}
\end{equation*}


and, hence, the mle of $\beta$ is


\begin{equation*}
\hat{\beta}=\frac{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{\sum_{i=1}^{n} Y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} . \tag{9.6.5}
\end{equation*}


Equations (9.6.2) and (9.6.4) are the estimating equations for the LS solutions for this simple linear model.

The fitted value at the point $\left(x_{i}, y_{i}\right)$ is given by


\begin{equation*}
\hat{y}_{i}=\hat{\alpha}+\hat{\beta}\left(x_{i}-\bar{x}\right), \tag{9.6.6}
\end{equation*}


which is shown on Figure 9.6.1. The fitted value $\hat{y}_{i}$ is also called the predicted value of $y_{i}$ at $x_{i}$. The residual at the point $\left(x_{i}, y_{i}\right)$ is given by


\begin{equation*}
\hat{e}_{i}=y_{i}-\hat{y}_{i}, \tag{9.6.7}
\end{equation*}


which is also shown on Figure 9.6.1. Residual means "what is left" and the residual in regression is exactly that, i.e., what is left over after the fit. The relationship between the fitted values and the residuals are explored in Remark 9.6.3 and in Exercise 9.6.13.

To find the maximum likelihood estimator of $\sigma^{2}$, consider the partial derivative

$$
\frac{\partial\left[-\log L\left(\alpha, \beta, \sigma^{2}\right)\right]}{\partial\left(\sigma^{2}\right)}=\frac{n}{2 \sigma^{2}}-\frac{\sum_{i=1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}}{2\left(\sigma^{2}\right)^{2}} .
$$

Setting this equal to zero and replacing $\alpha$ and $\beta$ by their solutions $\hat{\alpha}$ and $\hat{\beta}$, we obtain


\begin{equation*}
\hat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left[Y_{i}-\hat{\alpha}-\hat{\beta}\left(x_{i}-\bar{x}\right)\right]^{2} . \tag{9.6.8}
\end{equation*}


Of course, due to the invariance of mles, $\hat{\sigma}=\sqrt{\hat{\sigma}^{2}}$. Note that in terms of the residuals, $\hat{\sigma}^{2}=n^{-1} \sum_{i=1}^{n} \hat{e}_{i}^{2}$. As shown in Exercise 9.6.13, the average of the residuals is 0 .

Since $\hat{\alpha}$ is a linear function of independent and normally distributed random variables, $\hat{\alpha}$ has a normal distribution with mean

$$
E(\hat{\alpha})=E\left(\frac{1}{n} \sum_{i=1}^{n} Y_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E\left(Y_{i}\right)=\frac{1}{n} \sum_{i=1}^{n}\left[\alpha+\beta\left(x_{i}-\bar{x}\right)\right]=\alpha
$$

and variance

$$
\operatorname{var}(\hat{\alpha})=\sum_{i=1}^{n}\left(\frac{1}{n}\right)^{2} \operatorname{var}\left(Y_{i}\right)=\frac{\sigma^{2}}{n} .
$$

The estimator $\hat{\beta}$ is also a linear function of $Y_{1}, Y_{2}, \ldots, Y_{n}$ and hence has a normal distribution with mean

$$
\begin{aligned}
E(\hat{\beta}) & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left[\alpha+\beta\left(x_{i}-\bar{x}\right)\right]}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
& =\frac{\alpha \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)+\beta \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\beta
\end{aligned}
$$

and variance

$$
\begin{aligned}
\operatorname{var}(\hat{\beta}) & =\sum_{i=1}^{n}\left[\frac{x_{i}-\bar{x}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right]^{2} \operatorname{var}\left(Y_{i}\right) \\
& =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]^{2}} \sigma^{2}=\frac{\sigma^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} .
\end{aligned}
$$

In summary, the estimators $\hat{\alpha}$ and $\hat{\beta}$ are linear functions of the independent normal random variables $Y_{1}, \ldots, Y_{n}$. In Exercise 9.6.4 it is further shown that the covariance between $\hat{\alpha}$ and $\hat{\beta}$ is zero. It follows that $\hat{\alpha}$ and $\hat{\beta}$ are independent random variables with a bivariate normal distribution; that is,

\[
\binom{\hat{\alpha}}{\hat{\beta}} \text { has a } N_{2}\left(\binom{\alpha}{\beta}, \sigma^{2}\left[\begin{array}{cc}
\frac{1}{n} & 0  \tag{9.6.9}\\
0 & \frac{1}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\end{array}\right]\right) \text { distribution. }
\]

Next, we consider the estimator of $\sigma^{2}$. It can be shown (Exercise 9.6.9) that

$$
\begin{aligned}
\sum_{i=1}^{n}\left[Y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}= & \sum_{i=1}^{n}\left\{(\hat{\alpha}-\alpha)+(\hat{\beta}-\beta)\left(x_{i}-\bar{x}\right)\right. \\
& \left.+\left[Y_{i}-\hat{\alpha}-\hat{\beta}\left(x_{i}-\bar{x}\right)\right]\right\}^{2} \\
= & n(\hat{\alpha}-\alpha)^{2}+(\hat{\beta}-\beta)^{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n \hat{\sigma^{2}}
\end{aligned}
$$

or for brevity,

$$
Q=Q_{1}+Q_{2}+Q_{3} .
$$

Here $Q, Q_{1}, Q_{2}$, and $Q_{3}$ are real quadratic forms in the variables

$$
Y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right), \quad i=1,2, \ldots, n .
$$

In this equation, $Q$ represents the sum of the squares of $n$ independent random variables that have normal distributions with means zero and variances $\sigma^{2}$. Thus $Q / \sigma^{2}$ has a $\chi^{2}$ distribution with $n$ degrees of freedom. Each of the random variables $\sqrt{n}(\hat{\alpha}-\alpha) / \sigma$ and $\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}(\hat{\beta}-\beta) / \sigma$ has a normal distribution with zero mean and unit variance; thus, each of $Q_{1} / \sigma^{2}$ and $Q_{2} / \sigma^{2}$ has a $\chi^{2}$ distribution with 1 degree of freedom. In accordance with Theorem 9.9.2 (proved in Section 9.9), because $Q_{3}$ is nonnegative, we have that $Q_{1}, Q_{2}$, and $Q_{3}$ are independent and that $Q_{3} / \sigma^{2}$ has a $\chi^{2}$ distribution with $n-1-1=n-2$ degrees of freedom. That is, $n \hat{\sigma}^{2} / \sigma^{2}$ has a $\chi^{2}$ distribution with $n-2$ degrees of freedom.

We now extend this discussion to obtain inference for the parameters $\alpha$ and $\beta$. It follows from the above derivations that both the random variable $T_{1}$

$$
T_{1}=\frac{[\sqrt{n}(\hat{\alpha}-\alpha)] / \sigma}{\sqrt{Q_{3} /\left[\sigma^{2}(n-2)\right]}}=\frac{\hat{\alpha}-\alpha}{\sqrt{\hat{\sigma}^{2} /(n-2)}}
$$

and the random variable $T_{2}$


\begin{equation*}
T_{2}=\frac{\left[\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}(\hat{\beta}-\beta)\right] / \sigma}{\sqrt{Q_{3} /\left[\sigma^{2}(n-2)\right]}}=\frac{\hat{\beta}-\beta}{\sqrt{n \hat{\sigma}^{2} /\left[(n-2) \sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]}} \tag{9.6.10}
\end{equation*}


have a $t$-distribution with $n-2$ degrees of freedom. These facts enable us to obtain confidence intervals for $\alpha$ and $\beta$; see Exercise 9.6.5. The fact that $n \hat{\sigma}^{2} / \sigma^{2}$ has a $\chi^{2}$ distribution with $n-2$ degrees of freedom provides a means of determining a confidence interval for $\sigma^{2}$. These are some of the statistical inferences about the parameters to which reference was made in the introductory remarks of this section.

Remark 9.6.2. The more discerning reader should quite properly question our construction of $T_{1}$ and $T_{2}$ immediately above. We know that the squares of the linear forms are independent of $Q_{3}=n \hat{\sigma}^{2}$, but we do not know, at this time, that the linear forms themselves enjoy this independence. A more general result is obtained in Theorem 9.9.1 of Section 9.9 and the present case is a special instance.

Before considering a numerical example, we discuss a diagnostic plot for the major assumption of Model 9.6.1.

Remark 9.6.3 (Diagnostic Plot Based on Fitted Values and Residuals). The major assumption in the model is that the random errors $e_{1}, \ldots, e_{n}$ are iid. In particular, this means that the errors are not a function of the $x_{i}$ 's so that a plot of $e_{i}$ versus $\alpha+$ $\beta\left(x_{i}-\bar{x}\right)$ should result in a random scatter. Since the errors and the parameters are unknown this plot is not possible. We have estimates, though, of these quantities, namely the residuals $\hat{e}_{i}$ and the fitted values $\hat{y}_{i}$. A diagnostic for the assumption is to plot the residuals versus the fitted values. This is called the residual plot. If the plot results in a random scatter, it is an indication that the model is appropriate. Patterns in the plot, though, are indicative of a poor model. Often in this later case, the patterns in the plot lead to better models.

As a final note, in Model 9.6 .1 we have centered the $x$ 's; i.e., subtracted $\bar{x}$ from $x_{i}$. In practice, usually we do not precenter the $x$ 's. Instead, we fit the model $y_{i}=\alpha^{*}+\beta x_{i}+e_{i}$. In this case, the least squares, and hence, mles minimize the sum of squares


\begin{equation*}
\sum_{i=1}^{n}\left(y_{i}-\alpha^{*}-\beta x_{i}\right)^{2} . \tag{9.6.11}
\end{equation*}


In Exercise 9.6.1, the reader is asked to show that the estimate of $\beta$ remains the same as in expression (9.6.5), while $\hat{\alpha}^{*}=\bar{y}-\hat{\beta} \bar{x}$. We use this noncentered model in the following example.

Example 9.6.1 (Men's 1500 meters). As a numerical illustration, consider data drawn from the Olympics. The response of interest is the winning time of the men's 1500 meters, while the predictor is the year of the olympics. The data were taken from Wikipedia and can be found in olym1500mara.rda. Assume the R vectors for the winning times and year are time and year, respectively. There are $n=27$ data points. The top panel of Figure 9.6.2 shows a scatterplot of the data that is computed by the R command\\
par(mfrow=c (2,1));plot(time ${ }^{\text {year, } x l a b=" Y e a r ", y l a b=" W i n n i n g ~ t i m e ") ~}$\\
The winning times are steadily decreasing over time and, based on this plot, a simple linear model seems reasonable. Obviously the time for 2016 is an outlier but it is the correct time. Before proceeding to inference, though, we check the quality of the fit of the model. The following R commands obtain the least squares fit, overlaying it on the scatterplot in Figure 9.6.2, the fitted values, and the residuals. These are used to obtain the residual plot that is displayed in the bottom panel of 9.6.2.\\
fit <- lm(time\~{}year) ; abline(fit)\\
ehat <- fit\$resid; yhat <- fit\$fitted.values\\
plot (ehat\~{}yhat, xlab="Fitted values", ylab="Residuals")\\
Recall a "good" fit is indicated by a random scatter in the residual plot. This does not appear to be the case. There is a dependence ${ }^{4}$ between adjacent points over time. This dependence is apparent from the scatterplot too. In a time series course, this dependence would be investigated.

Based on the dependence, the following inference is approximate. The command summary (fit) produces the table of coefficients:

\begin{center}
\begin{tabular}{lllll}
 & Estimate Std. Error t value $\operatorname{Pr}(>\mathrm{t} \mid) \mid$ &  &  &  \\
(Intercept) & 12.325411 & 1.039402 & 11.858 & $9.26 \mathrm{e}-12$ \\
year & -0.004376 & 0.000530 & -8.257 & $1.31 \mathrm{e}-08$ \\
\end{tabular}
\end{center}

Hence, the prediction equation is $\hat{y}=12.33-.0044 y$ yar. Based on the slope estimate, we predict the winning time to drop by 0.004 minutes every year. For a $95 \%$ confidence interval for the slope, the $t$-critical value via R is $\mathrm{qt}(.975,25)$ which computes to 2.060. Using the standard error in the summary table, the following R commands compute confidence interval for the slope parameter:\\
err $=0.000530 * 2.060 ; 1 b=-0.004376$-err ; ub=-0.004376+err; ci=c (lb, ub)

\footnotetext{${ }^{4}$ This dependence is not surprising. The runners race against each other but they also try to beat the Olympic record.
}
ci; -0.0054678-0.0032842\\
So with approximate confidence $95 \%$, we estimate the drop in winning time to between 0.0032 to 0.0055 minutes per year.

Based on the fit, the predicted winning time for the men's 1500 meters in the 2020 Olympics is


\begin{equation*}
\hat{y}=12.325411-0.004376(2020)=3.486 \tag{9.6.12}
\end{equation*}


Exercise 9.6.8 provides an estimate (predictive interval) of error for this prediction.\\
\includegraphics[max width=\textwidth, center]{2025_05_06_e40e4b0ff5c2cb8edd3bg-562}

Figure 9.6.2: The top panel is the scatterplot of winning times in the men's 1500 meters versus the year of the Olympics. The least squares fit is overlaid. The bottom panel is the residual plot of the fit.

\subsection*{9.6.2 ${ }^{*}$ Geometry of the Least Squares Fit}
In the modern literature, linear models are usually expressed in terms of matrices and vectors, which we briefly introduce in this example. Furthermore, this allows us to discuss the simple geometry behind the least squares fit. Consider then Model (9.6.1). Write the vectors $\mathbf{Y}=\left(Y_{1}, \ldots, Y_{n}\right)^{\prime}, \mathbf{e}=\left(e_{1}, \ldots, e_{n}\right)^{\prime}$, and $\mathbf{x}_{c}=\left(x_{1}-\right.$ $\left.\bar{x}, \ldots, x_{n}-\bar{x}\right)^{\prime}$. Let 1 denote the $n \times 1$ vector whose components are all 1 . Then

Model (9.6.1) can be expressed equivalently as


\begin{align*}
\mathbf{Y} & =\alpha \mathbf{1}+\beta \mathbf{x}_{c}+\mathbf{e} \\
& =\left[\mathbf{1} \mathbf{x}_{c}\right]\binom{\alpha}{\beta}+\mathbf{e} \\
& =\mathbf{X} \boldsymbol{\beta}+\mathbf{e} \tag{9.6.13}
\end{align*}


where $\mathbf{X}$ is the $n \times 2$ matrix with columns $\mathbf{1}$ and $\mathbf{x}_{c}$ and $\boldsymbol{\beta}=(\alpha, \beta)^{\prime}$. Next, let $\boldsymbol{\theta}=E(\mathbf{Y})=\mathbf{X} \boldsymbol{\beta}$. Finally, let $V$ be the two-dimensional subspace of $R^{n}$ spanned by the columns of $\mathbf{X}$; i.e., $V$ is the range of the matrix $\mathbf{X}$. Hence we can also express the model succinctly as


\begin{equation*}
\mathbf{Y}=\boldsymbol{\theta}+\mathbf{e}, \quad \boldsymbol{\theta} \in V . \tag{9.6.14}
\end{equation*}


Hence, except for the random error vector $\mathbf{e}$, $\mathbf{Y}$ would lie in $V$. It makes sense intuitively then, as suggested by Figure 9.6.3, to estimate $\boldsymbol{\theta}$ by the vector in $V$ that is "closest" (in Euclidean distance) to $\mathbf{Y}$, that is, by $\hat{\boldsymbol{\theta}}$, where


\begin{equation*}
\hat{\boldsymbol{\theta}}=\operatorname{Argmin}_{\boldsymbol{\theta} \in V}\|\mathbf{Y}-\boldsymbol{\theta}\|^{2}, \tag{9.6.15}
\end{equation*}


where the square of the Euclidean norm is given by $\|\mathbf{u}\|^{2}=\sum_{i=1}^{n} u_{i}^{2}$, for $\mathbf{u} \in R^{n}$. As shown in Exercise 9.6.13 and depicted on the plot in Figure 9.6.3, $\hat{\boldsymbol{\theta}}=\hat{\alpha} \mathbf{1}+\hat{\beta} \mathbf{x}_{c}$, where $\hat{\alpha}$ and $\hat{\beta}$ are the least squares estimates given above. Also, the vector $\hat{\mathbf{e}}=$ $\mathbf{Y}-\hat{\boldsymbol{\theta}}$ is the vector of residuals and $n \hat{\sigma}^{2}=\|\hat{\mathbf{e}}\|^{2}$. Also, just as depicted in Figure 9.6.3, the angle between the vectors $\hat{\boldsymbol{\theta}}$ and $\hat{\mathbf{e}}$ is a right angle. In linear models, we say that $\hat{\boldsymbol{\theta}}$ is the projection of $\mathbf{Y}$ onto the subspace $V$.\\
\includegraphics[max width=\textwidth, center]{2025_05_06_e40e4b0ff5c2cb8edd3bg-563}

Figure 9.6.3: The sketch shows the geometry of least squares. The vector of responses is $\mathbf{Y}$, the fit is $\widehat{\boldsymbol{\theta}}$, and the vector of residuals is $\hat{\mathbf{e}}$.

\section*{EXERCISES}
9.6.1. Obtain the least squares estimates for the model $y_{i}=\alpha^{*}+\beta x_{i}+e_{i}$ by minimizing the sum of squares given in expression (9.6.11). Determine the distribution of $\hat{\alpha}^{*}$.\\
9.6.2. Students' scores on the mathematics portion of the ACT examination, $x$, and on the final examination in the first-semester calculus (200 points possible), $y$, are:

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
$x$ & 25 & 20 & 26 & 26 & 28 & 28 & 29 & 32 & 20 & 25 \\
\hline
$y$ & 138 & 84 & 104 & 112 & 88 & 132 & 90 & 183 & 100 & 143 \\
\hline
$x$ & 26 & 28 & 25 & 31 & 30 & \multicolumn{5}{|c|}{\multirow{2}{*}{}} \\
\hline
$y$ & 141 & 161 & 124 & 118 & 168 &  &  &  &  &  \\
\hline
\end{tabular}
\end{center}

The data are also in the rda file regr1.rda. Use $R$ or another statistical package for computation and plotting.\\
(a) Calculate the least squares regression line for these data.\\
(b) Plot the points and the least squares regression line on the same graph.\\
(c) Obtain the residual plot and comment on the appropriateness of the model.\\
(d) Find $95 \%$ confidence interval for $\beta$ under the usual assumptions. Comment in terms of the problem.\\
9.6.3 (Telephone Data). Consider the data presented below. The responses $(y)$ for this data set are the numbers of telephone calls (tens of millions) made in Belgium for the years 1950 through 1973. Time, the years, serves as the predictor variable $(x)$. The data are discussed on page 172 of Hettmansperger and McKean (2011) and are in the file telephone.rda.

\begin{center}
\begin{tabular}{|l|rrrrrr|}
\hline
Year & 50 & 51 & 52 & 53 & 54 & 55 \\
No. Calls & 0.44 & 0.47 & 0.47 & 0.59 & 0.66 & 0.73 \\
\hline
Year & 56 & 57 & 58 & 59 & 60 & 61 \\
No. Calls & 0.81 & 0.88 & 1.06 & 1.20 & 1.35 & 1.49 \\
\hline
Year & 62 & 63 & 64 & 65 & 66 & 67 \\
No. Calls & 1.61 & 2.12 & 11.90 & 12.40 & 14.20 & 15.90 \\
\hline
Year & 68 & 69 & 70 & 71 & 72 & 73 \\
No. Calls & 18.20 & 21.20 & 4.30 & 2.40 & 2.70 & 2.90 \\
\hline
\end{tabular}
\end{center}

(a) Calculate the least squares regression line for these data.\\
(b) Plot the points and the least squares regression line on the same graph.\\
(c) What is the reason for the poor least squares fit?\\
9.6.4. Show that the covariance between $\hat{\alpha}$ and $\hat{\beta}$ is zero.\\
9.6.5. Find $(1-\alpha) 100 \%$ confidence intervals for the parameters $\alpha$ and $\beta$ in Model (9.6.1).\\
9.6.6. Consider Model (9.6.1). Let $\eta_{0}=E\left(Y \mid x=x_{0}-\bar{x}\right)$. The least squares estimator of $\eta_{0}$ is $\hat{\eta}_{0}=\hat{\alpha}+\hat{\beta}\left(x_{0}-\bar{x}\right)$.\\
(a) Using (9.6.9), show that $\hat{\eta}_{0}$ is an unbiased estimator and show that its variance is given by

$$
V\left(\hat{\eta}_{0}\right)=\sigma^{2}\left[\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{1}-\bar{x}\right)^{2}}\right]
$$

(b) Obtain the distribution of $\hat{\eta}_{0}$ and use it to determine a ( $1-\alpha$ ) $100 \%$ confidence interval for $\eta_{0}$.\\
9.6.7. Assume that the sample $\left(x_{1}, Y_{1}\right), \ldots,\left(x_{n}, Y_{n}\right)$ follows the linear model (9.6.1). Suppose $Y_{0}$ is a future observation at $x=x_{0}-\bar{x}$ and we want to determine a predictive interval for it. Assume that the model (9.6.1) holds for $Y_{0}$; i.e., $Y_{0}$ has a $N\left(\alpha+\beta\left(x_{0}-\bar{x}\right), \sigma^{2}\right)$ distribution. We use $\hat{\eta}_{0}$ of Exercise 9.6.6 as our prediction of $Y_{0}$.\\
(a) Obtain the distribution of $Y_{0}-\hat{\eta}_{0}$, showing that its variance is:

$$
V\left(Y_{0}-\hat{\eta}_{0}\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{1}-\bar{x}\right)^{2}}\right]
$$

Use the fact that the future observation $Y_{0}$ is independent of the sample $\left(x_{1}, Y_{1}\right), \ldots,\left(x_{n}, Y_{n}\right)$.\\
(b) Determine a $t$-statistic with numerator $Y_{0}-\hat{\eta}_{0}$.\\
(c) Now beginning with $1-\alpha=P\left[-t_{\alpha / 2, n-2}<T<t_{\alpha / 2, n-2}\right]$, where $0<\alpha<1$, determine a $(1-\alpha) 100 \%$ predictive interval for $Y_{0}$.\\
(d) Compare this predictive interval with the confidence interval obtained in Exercise 9.6.6. Intuitively, why is the predictive interval larger?\\
9.6.8. In Example 9.6.1, we obtain the predicted winning time for the men's 1500 meters in the 2020 Olympics. Compute the $95 \%$ predictive interval for this prediction that is given in the last exercise. These computations are performed by the R function cipi.R. The call is cipi(lm(time $\sim$ year), matrix $(c(1,2020)$, ncol=2)). In terms of the problem, what does this predictive interval mean? Next compute the prediction for the 2024 and 2028 Olympics. Why are the intervals increasing in length?\\
9.6.9. Show that\\
$\sum_{i=1}^{n}\left[Y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}=n(\hat{\alpha}-\alpha)^{2}+(\hat{\beta}-\beta)^{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left[Y_{i}-\hat{\alpha}-\hat{\beta}\left(x_{i}-\bar{x}\right)\right]^{2}$.\\
9.6.10. Let the independent random variables $Y_{1}, Y_{2}, \ldots, Y_{n}$ have, respectively, the probability density functions $N\left(\beta x_{i}, \gamma^{2} x_{i}^{2}\right), i=1,2, \ldots, n$, where the given numbers $x_{1}, x_{2}, \ldots, x_{n}$ are not all equal and no one is zero. Find the maximum likelihood estimators of $\beta$ and $\gamma^{2}$.\\
9.6.11. Let the independent random variables $Y_{1}, \ldots, Y_{n}$ have the joint pdf

$$
L\left(\alpha, \beta, \sigma^{2}\right)=\left(\frac{1}{2 \pi \sigma^{2}}\right)^{n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{1}^{n}\left[y_{i}-\alpha-\beta\left(x_{i}-\bar{x}\right)\right]^{2}\right\}
$$

where the given numbers $x_{1}, x_{2}, \ldots, x_{n}$ are not all equal. Let $H_{0}: \beta=0$ ( $\alpha$ and $\sigma^{2}$ unspecified). It is desired to use a likelihood ratio test to test $H_{0}$ against all possible alternatives. Find $\Lambda$ and see whether the test can be based on a familiar statistic.\\
Hint: In the notation of this section, show that

$$
\sum_{1}^{n}\left(Y_{i}-\hat{\alpha}\right)^{2}=Q_{3}+\widehat{\beta}^{2} \sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2}
$$

9.6.12. Using the notation of Section 9.2 , assume that the means $\mu_{j}$ satisfy a linear function of $j$, namely, $\mu_{j}=c+d[j-(b+1) / 2]$. Let independent random samples of size $a$ be taken from the $b$ normal distributions having means $\mu_{1}, \mu_{2}, \ldots, \mu_{b}$, respectively, and common unknown variance $\sigma^{2}$.\\
(a) Show that the maximum likelihood estimators of $c$ and $d$ are, respectively, $\hat{c}=\bar{X}$.. and

$$
\hat{d}=\frac{\sum_{j=1}^{b}[j-(b-1) / 2]\left(\bar{X}_{. j}-\bar{X}_{. .}\right)}{\sum_{j=1}^{b}[j-(b+1) / 2]^{2}} .
$$

(b) Show that

$$
\begin{aligned}
\sum_{i=1}^{a} \sum_{j=1}^{b}\left(X_{i j}-\bar{X}_{. .}\right)^{2}= & \sum_{i=1}^{a} \sum_{j=1}^{b}\left[X_{i j}-\bar{X}_{. .}-\hat{d}\left(j-\frac{b+1}{2}\right)\right]^{2} \\
& +\hat{d}^{2} \sum_{j=1}^{b} a\left(j-\frac{b+1}{2}\right)^{2}
\end{aligned}
$$

(c) Argue that the two terms in the right-hand member of part (b), once divided by $\sigma^{2}$, are independent random variables with $\chi^{2}$ distributions provided that $d=0$.\\
(d) What $F$-statistic would be used to test the equality of the means, that is, $H_{0}: d=0$ ?\\
9.6.13. Consider the discussion in Section 9.6.2.\\
(a) Show that $\hat{\boldsymbol{\theta}}=\hat{\alpha} \mathbf{1}+\hat{\beta} \mathbf{x}_{c}$, where $\hat{\alpha}$ and $\hat{\beta}$ are the least squares estimators derived in this section.\\
(b) Show that the vector $\hat{\mathbf{e}}=\mathbf{Y}-\hat{\boldsymbol{\theta}}$ is the vector of residuals; i.e., its ith entry is $\hat{e}_{i}$, (9.6.7).\\
(c) As depicted in Figure 9.6.3, show that the angle between the vectors $\hat{\boldsymbol{\theta}}$ and $\hat{\mathbf{e}}$ is a right angle.\\
(d) Show that the residuals sum to zero; i.e., $\mathbf{1}^{\prime} \hat{\mathbf{e}}=0$.\\
9.6.14. Fit $y=a+x$ to the data

\begin{center}
\begin{tabular}{l|lll}
$x$ & 0 & 1 & 2 \\
\hline
$y$ & 1 & 3 & 4 \\
\hline
\end{tabular}
\end{center}

by the method of least squares.\\
9.6.15. Fit by the method of least squares the plane $z=a+b x+c y$ to the five points $(x, y, z):(-1,-2,5),(0,-2,4),(0,0,4),(1,0,2),(2,1,0)$.\\
Let the R vectors $\mathrm{x}, \mathrm{y}, \mathrm{z}$ contain the values for $x, y$, and $z$. Then the LS fit is computed by $\operatorname{lm}\left(\mathrm{z}^{\sim} \mathrm{x}+\mathrm{y}\right)$.\\
9.6.16. Let the $4 \times 1$ matrix $\boldsymbol{Y}$ be multivariate normal $N\left(\boldsymbol{X} \boldsymbol{\beta}, \sigma^{2} \boldsymbol{I}\right)$, where the $4 \times 3$ matrix $\boldsymbol{X}$ equals

$$
\boldsymbol{X}=\left[\begin{array}{rrr}
1 & 1 & 2 \\
1 & -1 & 2 \\
1 & 0 & -3 \\
1 & 0 & -1
\end{array}\right]
$$

and $\boldsymbol{\beta}$ is the $3 \times 1$ regression coefficient matrix.\\
(a) Find the mean matrix and the covariance matrix of $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Y}$.\\
(b) If we observe $\boldsymbol{Y}^{\prime}$ to be equal to $(6,1,11,3)$, compute $\hat{\boldsymbol{\beta}}$.\\
9.6.17. Suppose $\boldsymbol{Y}$ is an $n \times 1$ random vector, $\boldsymbol{X}$ is an $n \times p$ matrix of known constants of rank $p$, and $\boldsymbol{\beta}$ is a $p \times 1$ vector of regression coefficients. Let $\boldsymbol{Y}$ have a $N\left(\boldsymbol{X} \boldsymbol{\beta}, \sigma^{2} \boldsymbol{I}\right)$ distribution. Obtain the pdf of $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Y}$.\\
9.6.18. Let the independent normal random variables $Y_{1}, Y_{2}, \ldots, Y_{n}$ have, respectively, the probability density functions $N\left(\mu, \gamma^{2} x_{i}^{2}\right), i=1,2, \ldots, n$, where the given $x_{1}, x_{2}, \ldots, x_{n}$ are not all equal and no one of which is zero. Discuss the test of the hypothesis $H_{0}: \gamma=1, \mu$ unspecified, against all alternatives $H_{1}: \gamma \neq 1, \mu$ unspecified.

\subsection*{9.7 A Test of Independence}
Let $X$ and $Y$ have a bivariate normal distribution with means $\mu_{1}$ and $\mu_{2}$, positive variances $\sigma_{1}^{2}$ and $\sigma_{2}^{2}$, and correlation coefficient $\rho$. We wish to test the hypothesis that $X$ and $Y$ are independent. Because two jointly normally distributed random variables are independent if and only if $\rho=0$, we test the hypothesis $H_{0}: \rho=0$ against the hypothesis $H_{1}: \rho \neq 0$. A likelihood ratio test is used. Let $\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \ldots,\left(X_{n}, Y_{n}\right)$ denote a random sample of size $n>2$ from the\\
bivariate normal distribution; that is, the joint pdf of these $2 n$ random variables is given by

$$
f\left(x_{1}, y_{1}\right) f\left(x_{2}, y_{2}\right) \cdots f\left(x_{n}, y_{n}\right)
$$

Although it is fairly difficult to show, the statistic that is defined by the likelihood ratio $\Lambda$ is a function of the statistic, which is the mle of $\rho$, namely,


\begin{equation*}
R=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}} . \tag{9.7.1}
\end{equation*}


This statistic $R$ is called the sample correlation coefficient of the random sample. Following the discussion after expression (5.4.5), the statistic $R$ is a consistent estimate of $\rho$; see Exercise 9.7.5. The likelihood ratio principle, which calls for the rejection of $H_{0}$ if $\Lambda \leq \lambda_{0}$, is equivalent to the computed value of $|R| \geq c$. That is, if the absolute value of the correlation coefficient of the sample is too large, we reject the hypothesis that the correlation coefficient of the distribution is equal to zero. To determine a value of $c$ for a satisfactory significance level, it is necessary to obtain the distribution of $R$, or a function of $R$, when $H_{0}$ is true, as we outline next.

Let $X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}, n>2$, where $x_{1}, x_{2}, \ldots, x_{n}$ and $\bar{x}=$ $\sum_{1}^{n} x_{i} / n$ are fixed numbers such that $\sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2}>0$. Consider the conditional pdf of $Y_{1}, Y_{2}, \ldots, Y_{n}$ given that $X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}$. Because $Y_{1}, Y_{2}, \ldots, Y_{n}$ are independent and, with $\rho=0$, are also independent of $X_{1}, X_{2}, \ldots, X_{n}$, this conditional pdf is given by

$$
\left(\frac{1}{\sqrt{2 \pi} \sigma_{2}}\right)^{n} \exp \left\{-\frac{1}{2 \sigma_{2}^{2}} \sum_{1}^{n}\left(y_{i}-\mu_{2}\right)^{2}\right\}
$$

Let $R_{c}$ be the correlation coefficient, given $X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}$, so that


\begin{equation*}
\frac{R_{c} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right) Y_{i}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \tag{9.7.2}
\end{equation*}


is $\hat{\beta}$, expression (9.6.5) of Section 9.6. Conditionally the mean of $Y_{i}$ is $\mu_{2}$; i.e., a constant. So here expression (9.7.2) has expectation 0 which implies that $E\left(R_{c}\right)=0$. Next consider the $t$-ratio of $\hat{\beta}$ given by $T_{2}$ of expression (9.6.10) of Section 9.6. In this notation $T_{2}$ can be expressed as


\begin{equation*}
T_{2}=\frac{R_{c} \sqrt{\sum\left(Y_{i}-\bar{Y}\right)^{2}} / \sqrt{\sum\left(x_{i}-\bar{x}\right)^{2}}}{\sqrt{\frac{\sum_{i=1}^{n}\left\{Y_{i}-\bar{Y}-\left[R_{c} \sqrt{\sum_{j=1}^{n}\left(Y_{j}-\bar{Y}\right)^{2}} / \sqrt{\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}\right]\left(x_{i}-\bar{x}\right)\right\}^{2}}{(n-2) \sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}}}=\frac{R_{c} \sqrt{n-2}}{\sqrt{1-R_{c}^{2}}} \tag{9.7.3}
\end{equation*}


Thus $T_{2}$, given $X_{1}=x_{1}, \ldots, X_{n}=x_{n}$, has a conditional $t$-distribution with $n-2$ degrees of freedom. Note that the pdf, say $g(t)$, of this $t$-distribution does not depend upon $x_{1}, x_{2}, \ldots, x_{n}$. Now the joint pdf of $X_{1}, X_{2}, \ldots, X_{n}$ and $R \sqrt{n-2} / \sqrt{1-R^{2}}$, where $R$ is given by expression (9.7.1), is the product of $g(t)$ and the joint pdf of $X_{1}, \ldots, X_{n}$. Integration on $x_{1}, \ldots, x_{n}$ yields the marginal pdf of $R \sqrt{n-2} / \sqrt{1-R^{2}}$; because $g(t)$ does not depend upon $x_{1}, x_{2}, \ldots, x_{n}$, it is obvious that this marginal pdf is $g(t)$, the conditional pdf of $R \sqrt{n-2} / \sqrt{1-R^{2}}$. The change-of-variable technique can now be used to find the pdf of $R$.

Remark 9.7.1. Since $R$ has, when $\rho=0$, a conditional distribution that does not depend upon $x_{1}, x_{2}, \ldots, x_{n}$ (and hence that conditional distribution is, in fact, the marginal distribution of $R$ ), we have the remarkable fact that $R$ is independent of $X_{1}, X_{2}, \ldots, X_{n}$. It follows that $R$ is independent of every function of $X_{1}, X_{2}, \ldots, X_{n}$ alone, that is, a function that does not depend upon any $Y_{i}$. In like manner, $R$ is independent of every function of $Y_{1}, Y_{2}, \ldots, Y_{n}$ alone. Moreover, a careful review of the argument reveals that nowhere did we use the fact that $X$ has a normal marginal distribution. Thus, if $X$ and $Y$ are independent, and if $Y$ has a normal distribution, then $R$ has the same conditional distribution whatever the distribution of $X$, subject to the condition $\sum_{1}^{n}\left(x_{i}-\bar{x}\right)^{2}>0$. Moreover, if $P\left[\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2}>0\right]=1$, then $R$ has the same marginal distribution whatever the distribution of $X$.

If we write $T=R \sqrt{n-2} / \sqrt{1-R^{2}}$, where $T$ has a $t$-distribution with $n-2>0$ degrees of freedom, it is easy to show by the change-of-variable technique (Exercise 9.7.4) that the pdf of $R$ is given by

\[
h(r)= \begin{cases}\frac{\Gamma[(n-1) / 2]}{\Gamma\left(\frac{1}{2}\right) \Gamma[(n-2) / 2]}\left(1-r^{2}\right)^{(n-4) / 2} & -1<r<1  \tag{9.7.4}\\ 0 & \text { elsewhere } .\end{cases}
\]

We have now solved the problem of the distribution of $R$, when $\rho=0$ and $n>2$, or perhaps more conveniently, that of $R \sqrt{n-2} / \sqrt{1-R^{2}}$. The likelihood ratio test of the hypothesis $H_{0}: \rho=0$ against all alternatives $H_{1}: \rho \neq 0$ may be based either on the statistic $R$ or on the statistic $R \sqrt{n-2} / \sqrt{1-R^{2}}=T$, although the latter is easier to use. Therefore, a level $\alpha$ test is to reject $H_{0}: \rho=0$ if $|T| \geq t_{\alpha / 2, n-2}$.

Remark 9.7.2. It is possible to obtain an approximate test of size $\alpha$ by using the fact that

$$
W=\frac{1}{2} \log \left(\frac{1+R}{1-R}\right)
$$

has an approximate normal distribution with mean $\frac{1}{2} \log [(1+\rho) /(1-\rho)]$ and with variance $1 /(n-3)$. We accept this statement without proof. Thus a test of $H_{0}$ : $\rho=0$ can be based on the statistic


\begin{equation*}
Z=\frac{\frac{1}{2} \log [(1+R) /(1-R)]-\frac{1}{2} \log [(1+\rho) /(1-\rho)]}{\sqrt{1 /(n-3)}}, \tag{9.7.5}
\end{equation*}


with $\rho=0$ so that $\frac{1}{2} \log [(1+\rho) /(1-\rho)]=0$. However, using $W$, we can also test a hypothesis like $H_{0}: \rho=\rho_{0}$ against $H_{1}: \rho \neq \rho_{0}$, where $\rho_{0}$ is not necessarily zero. In that case, the hypothesized mean of $W$ is

$$
\frac{1}{2} \log \left(\frac{1+\rho_{0}}{1-\rho_{0}}\right) .
$$

Furthermore, as outlined in Exercise 9.7.6, $Z$ can be used to obtain an asymptotic confidence interval for $\rho$.

\section*{EXERCISES}
9.7.1. Show that

$$
R=\frac{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{1}^{n}\left(X_{i}-\bar{X}\right)^{2} \sum_{1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}=\frac{\sum_{1}^{n} X_{i} Y_{i}-n \overline{X Y}}{\sqrt{\left(\sum_{1}^{n} X_{i}^{2}-n \bar{X}^{2}\right)\left(\sum_{1}^{n} Y_{i}^{2}-n \bar{Y}^{2}\right)}} .
$$

9.7.2. A random sample of size $n=6$ from a bivariate normal distribution yields a value of the correlation coefficient of 0.89 . Would we accept or reject, at the $5 \%$ significance level, the hypothesis that $\rho=0$ ?\\
9.7.3. Verify Equation (9.7.3) of this section.\\
9.7.4. Verify the pdf (9.7.4) of this section.\\
9.7.5. Using the results of Section 4.5, show that $R$, (9.7.1), is a consistent estimate of $\rho$.\\
9.7.6. By doing the following steps, determine a $(1-\alpha) 100 \%$ approximate confidence interval for $\rho$.\\
(a) For $0<\alpha<1$, in the usual way, start with $1-\alpha=P\left(-z_{\alpha / 2}<Z<z_{\alpha / 2}\right)$, where $Z$ is given by expression (9.7.5). Then isolate $h(\rho)=(1 / 2) \log [(1+$ $\rho) /(1-\rho)]$ in the middle part of the inequality. Find $h^{\prime}(\rho)$ and show that it is strictly positive on $-1<\rho<1$; hence, $h$ is strictly increasing and its inverse function exists.\\
(b) Show that this inverse function is the hyperbolic tangent function given by $\tanh (y)=\left(e^{y}-e^{-y}\right) /\left(e^{y}+e^{-y}\right)$.\\
(c) Obtain a $(1-\alpha) 100 \%$ confidence interval for $\rho$.\\
9.7.7. The intrinsic R function cor.test ( $\mathrm{x}, \mathrm{y}$ ) computes the estimate of $\rho$ and the confidence interval in Exercise 9.7.6. Recall the baseball data which is in the file bb.rda.\\
(a) Using the baseball data, determine the estimate and the confidence interval for the correlation coefficient between height and weight for professional baseball players.\\
(b) Separate the pitchers and hitters and for each obtain the estimate and confidence for the correlation coefficient between height and weight. Do they differ significantly?\\
(c) Argue that the difference in the estimates of the correlation coefficients is the mle of $\rho_{1}-\rho_{2}$ for two independent samples, as in Part (b).\\
9.7.8. Two experiments gave the following results:

\begin{center}
\begin{tabular}{cccccc}
\hline
$n$ & $\bar{x}$ & $\bar{y}$ & $s_{x}$ & $s_{y}$ & $r$ \\
\hline
100 & 10 & 20 & 5 & 8 & 0.70 \\
200 & 12 & 22 & 6 & 10 & 0.80 \\
\hline
\end{tabular}
\end{center}

Calculate $r$ for the combined sample.

\subsection*{9.8 The Distributions of Certain Quadratic Forms}
Remark 9.8.1. It is essential that the reader have the background of the multivariate normal distribution as given in Section 3.5 to understand Sections 9.8 and 9.9.

Remark 9.8.2. We make use of the trace of a square matrix. If $\mathbf{A}=\left[a_{i j}\right]$ is an $n \times n$ matrix, then we define the trace of $\mathbf{A},(\operatorname{tr} \mathbf{A})$, to be the sum of its diagonal entries; i.e.,


\begin{equation*}
\operatorname{tr} \mathbf{A}=\sum_{i=1}^{n} a_{i i} . \tag{9.8.1}
\end{equation*}


The trace of a matrix has several interesting properties. One is that it is a linear operator; that is,


\begin{equation*}
\operatorname{tr}(a \mathbf{A}+b \mathbf{B})=a \operatorname{tr} \mathbf{A}+b \operatorname{tr} \mathbf{B} . \tag{9.8.2}
\end{equation*}


A second useful property is: If $\mathbf{A}$ is an $n \times m$ matrix, $\mathbf{B}$ is an $m \times k$ matrix, and $\mathbf{C}$ is a $k \times n$ matrix, then


\begin{equation*}
\operatorname{tr}(\mathbf{A B C})=\operatorname{tr}(\mathbf{B C A})=\operatorname{tr}(\mathbf{C A B}) . \tag{9.8.3}
\end{equation*}


The reader is asked to prove these facts in Exercise 9.8.7. Finally, a simple but useful property is that $\operatorname{tr} a=a$, for any scalar $a$.

We begin this section with a more formal but equivalent definition of a quadratic form. Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be an $n$-dimensional random vector and let $\mathbf{A}$ be a real $n \times n$ symmetric matrix. Then the random variable $Q=\mathbf{X}^{\prime} \mathbf{A X}$ is called a\\
quadratic form in $\mathbf{X}$. Due to the symmetry of $\mathbf{A}$, there are several ways we can write $Q$ :


\begin{align*}
Q=\mathbf{X}^{\prime} \mathbf{A X} & =\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j} X_{i} X_{j}=\sum_{i=1}^{n} a_{i i} X_{i}^{2}+\sum_{i \neq j} a_{i j} X_{i} X_{j}  \tag{9.8.4}\\
& =\sum_{i=1}^{n} a_{i i} X_{i}^{2}+2 \sum_{i<j} \sum_{i j} X_{i} X_{j} . \tag{9.8.5}
\end{align*}


These are very useful random variables in analysis of variance models. As the following theorem shows, the mean of a quadratic form is easily obtained.

Theorem 9.8.1. Suppose the n-dimensional random vector $\mathbf{X}$ has mean $\boldsymbol{\mu}$ and variance-covariance matrix $\boldsymbol{\Sigma}$. Let $Q=\mathbf{X}^{\prime} \mathbf{A X}$, where $\mathbf{A}$ is a real $n \times n$ symmetric matrix. Then


\begin{equation*}
E(Q)=\operatorname{tr} \mathbf{A} \boldsymbol{\Sigma}+\boldsymbol{\mu}^{\prime} \mathbf{A} \boldsymbol{\mu} \tag{9.8.6}
\end{equation*}


Proof: Using the trace operator and property (9.8.3), we have

$$
\begin{aligned}
E(Q)=E\left(\operatorname{tr} \mathbf{X}^{\prime} \mathbf{A X}\right) & =E\left(\operatorname{tr} \mathbf{A} \mathbf{X} \mathbf{X}^{\prime}\right) \\
& =\operatorname{tr} \mathbf{A} E\left(\mathbf{X X}^{\prime}\right) \\
& =\operatorname{tr} \mathbf{A}\left(\boldsymbol{\Sigma}+\boldsymbol{\mu} \boldsymbol{\mu}^{\prime}\right) \\
& =\operatorname{tr} \mathbf{A} \boldsymbol{\Sigma}+\boldsymbol{\mu}^{\prime} \mathbf{A} \boldsymbol{\mu}
\end{aligned}
$$

where the third line follows from Theorem 2.6.3.

Example 9.8.1 (Sample Variance). Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$ be an $n$-dimensional vector of random variables. Let $\mathbf{1}^{\prime}=(1, \ldots, 1)$ be the $n$-dimensional vector whose components are 1 . Let $\mathbf{I}$ be the $n \times n$ identity matrix. Consider the quadratic form $Q=\mathbf{X}^{\prime}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right) \mathbf{X}$, where $\mathbf{J}=\mathbf{1 1}^{\prime}$; i.e., $\mathbf{J}$ is an $n \times n$ matrix with all entries equal to 1 . Note that the off-diagonal entries of $\left(\mathbf{I}-\frac{1}{n}, \mathbf{J}\right)$ are $-n^{-1}$ while the diagonal entries are $1-n^{-1}$; hence, by (9.8.4), $Q$ simplifies to


\begin{align*}
Q & =\sum_{i=1}^{n} X_{i}^{2}\left(1-\frac{1}{n}\right)+\sum_{i \neq j}\left(-\frac{1}{n}\right) X_{i} X_{j} \\
& =\sum_{i=1}^{n} X_{i}^{2}\left(1-\frac{1}{n}\right)-\frac{1}{n} \sum_{i=1}^{n} X_{i} \sum_{j=1}^{n} X_{j}+\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \\
& =\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}=(n-1) S^{2} \tag{9.8.7}
\end{align*}


where $\bar{X}$ and $S^{2}$ denote the sample mean and variance of $X_{1}, \ldots, X_{n}$.\\
Suppose we further assume that $X_{1}, \ldots, X_{n}$ are iid random variables with common mean $\mu$ and variance $\sigma^{2}$. Using Theorem 9.8.1, we can obtain yet another\\
proof that $S^{2}$ is an unbiased estimate of $\sigma^{2}$. Note that the mean of the random vector $\mathbf{X}$ is $\mu \mathbf{1}$ and that its variance-covariance matrix is $\sigma^{2} \mathbf{I}$. Based on Theorem 9.8.1, we find immediately that

$$
E\left(S^{2}\right)=\frac{1}{n-1}\left\{\operatorname{tr}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right) \sigma^{2} \mathbf{I}+\mu^{2}\left(\mathbf{1}^{\prime} \mathbf{1}-\frac{1}{n} \mathbf{1}^{\prime} \mathbf{1} \mathbf{1}^{\prime} \mathbf{1}\right)\right\}=\sigma^{2} .
$$

The spectral decomposition of symmetric matrices proves quite useful in this part of the chapter. As discussed around expression (3.5.8), a real symmetric matrix $\mathbf{A}$ can be diagonalized as


\begin{equation*}
\mathbf{A}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda} \boldsymbol{\Gamma} \tag{9.8.8}
\end{equation*}


where $\boldsymbol{\Lambda}$ is the diagonal matrix $\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right), \lambda_{1} \geq \cdots \geq \lambda_{n}$ are the eigenvalues of $\mathbf{A}$, and the columns of $\boldsymbol{\Gamma}^{\prime}=\left[\mathbf{v}_{1} \cdots \mathbf{v}_{n}\right]$ are the corresponding orthonormal eigenvectors (i.e., $\boldsymbol{\Gamma}$ is an orthogonal matrix). Recall from linear algebra that the rank of $\mathbf{A}$ is the number of nonzero eigenvalues. Further, because $\boldsymbol{\Lambda}$ is diagonal, we can write this expression as


\begin{equation*}
\mathbf{A}=\sum_{i=1}^{n} \lambda_{i} \mathbf{v}_{i} \mathbf{v}_{i}^{\prime} . \tag{9.8.9}
\end{equation*}


The R command to compute the spectral decomposition of $\mathbf{A}$ is sdc=eigen(amat), where amat is the R matrix for $\mathbf{A}$. The eigenvalues and eigenvectors are in the respective attributes sdc\$values and sdc\$vectors. For normal random variables, we make use of equation (9.8.9) to obtain the mgf of the quadratic form $Q$ in the next theorem, Theorem 9.8.2.

Theorem 9.8.2. Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$, where $X_{1}, \ldots, X_{n}$ are iid $N\left(0, \sigma^{2}\right)$. Consider the quadratic form $Q=\sigma^{-2} \mathbf{X}^{\prime} \mathbf{A X}$ for a symmetric matrix $\mathbf{A}$ of rank $r \leq n$. Then $Q$ has the moment generating function


\begin{equation*}
M(t)=\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)^{-1 / 2}=|\mathbf{I}-2 t \mathbf{A}|^{-1 / 2} \tag{9.8.10}
\end{equation*}


where $\lambda_{1}, \ldots, \lambda_{r}$ are the nonzero eigenvalues of $\mathbf{A},|t|<1 /\left(2 \lambda^{*}\right)$, and the value of $\lambda^{*}$ is given by $\lambda^{*}=\max _{1 \leq i \leq r}\left|\lambda_{i}\right|$.

Proof: Write the spectral decomposition of $\mathbf{A}$ as in expression (9.8.9). Since the rank of $\mathbf{A}$ is $r$, exactly $r$ of the eigenvalues are not 0 . Denote the nonzero eigenvalues by $\lambda_{1}, \ldots, \lambda_{r}$. Then we can write $Q$ as


\begin{equation*}
Q=\sum_{i=1}^{r} \lambda_{i}\left(\sigma^{-1} \mathbf{v}_{i}^{\prime} \mathbf{X}\right)^{2} . \tag{9.8.11}
\end{equation*}


Let $\boldsymbol{\Gamma}_{1}^{\prime}=\left[\mathbf{v}_{1} \cdots \mathbf{v}_{r}\right]$ and define the $r$-dimensional random vector $\mathbf{W}$ by $\mathbf{W}=$ $\sigma^{-1} \boldsymbol{\Gamma}_{1} \mathbf{X}$. Since $\mathbf{X}$ is $N_{n}\left(\mathbf{0}, \sigma^{2} \mathbf{I}_{n}\right)$ and $\boldsymbol{\Gamma}_{1}^{\prime} \boldsymbol{\Gamma}_{1}=\mathbf{I}_{r}$, Theorem 3.5.2 shows that $\mathbf{W}$ has a $N_{r}\left(\mathbf{0}, \mathbf{I}_{r}\right)$ distribution. In terms of the $W_{i}$, we can write (9.8.11) as


\begin{equation*}
Q=\sum_{i=1}^{r} \lambda_{i} W_{i}^{2} . \tag{9.8.12}
\end{equation*}


Because $W_{1}, \ldots, W_{r}$ are independent $N(0,1)$ random variables, $W_{1}^{2}, \ldots, W_{r}^{2}$ are independent $\chi^{2}(1)$ random variables. Thus the mgf of $Q$ is


\begin{align*}
E[\exp \{t Q\}] & =E\left[\exp \left\{\sum_{i=1}^{r} t \lambda_{i} W_{i}^{2}\right\}\right] \\
& =\prod_{i=1}^{r} E\left[\exp \left\{t \lambda_{i} W_{i}^{2}\right\}\right]=\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)^{-1 / 2} \tag{9.8.13}
\end{align*}


The last equality holds if we assume that $|t|<1 /\left(2 \lambda^{*}\right)$, where $\lambda^{*}=\max _{1 \leq i \leq r}\left|\lambda_{i}\right|$; see Exercise 9.8.6. To obtain the second form in (9.8.10), recall that the determinant of an orthogonal matrix is 1 . The result then follows from

$$
\begin{aligned}
|\mathbf{I}-2 t \mathbf{A}|=\left|\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Gamma}-2 t \boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda} \boldsymbol{\Gamma}\right| & =\left|\boldsymbol{\Gamma}^{\prime}(\mathbf{I}-2 t \boldsymbol{\Lambda}) \boldsymbol{\Gamma}\right| \\
& =|\mathbf{I}-2 t \boldsymbol{\Lambda}|=\left\{\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)^{-1 / 2}\right\}^{-2} .
\end{aligned}
$$

Example 9.8.2. To illustrate this theorem, suppose $X_{i}, i=1,2, \ldots, n$, are independent random variables with $X_{i}$ distributed as $N\left(\mu_{i}, \sigma_{i}^{2}\right), i=1,2, \ldots, n$, respectively. Let $Z_{i}=\left(X_{i}-\mu_{i}\right) / \sigma_{i}$. We know that $\sum_{i=1}^{n} Z_{i}^{2}$ has a $\chi^{2}$ distribution with $n$ degrees of freedom. To illustrate Theorem 9.8.2, let $\mathbf{Z}^{\prime}=\left(Z_{1}, \ldots, Z_{n}\right)$. Let $Q=\mathbf{Z}^{\prime} \mathbf{I Z}$. Hence the symmetric matrix associated with $Q$ is the identity matrix $\mathbf{I}$, which has $n$ eigenvalues, all of value 1 ; i.e., $\lambda_{i} \equiv 1$. By Theorem 9.8.2, the mgf of $Q$ is $(1-2 t)^{-n / 2}$; i.e., $Q$ is distributed $\chi^{2}$ with $n$ degrees of freedom.

In general, from Theorem 9.8.2, note how close the mgf of the quadratic form $Q$ is to the mgf of a $\chi^{2}$ distribution. The next two theorems give conditions where this is true.\\
Theorem 9.8.3. Let $\mathbf{X}^{\prime}=\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ have a $N_{n}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ distribution, where $\boldsymbol{\Sigma}$ is positive definite. Then $Q=(\mathbf{X}-\boldsymbol{\mu})^{\prime} \boldsymbol{\Sigma}^{-1}(\mathbf{X}-\boldsymbol{\mu})$ has a $\chi^{2}(n)$ distribution.\\
Proof: Write the spectral decomposition of $\boldsymbol{\Sigma}$ as $\boldsymbol{\Sigma}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda} \boldsymbol{\Gamma}$, where $\boldsymbol{\Gamma}$ is an orthogonal matrix and $\boldsymbol{\Lambda}=\operatorname{diag}\left\{\lambda_{1}, \ldots, \lambda_{n}\right\}$ is a diagonal matrix whose diagonal entries are the eigenvalues of $\boldsymbol{\Sigma}$. Because $\boldsymbol{\Sigma}$ is positive definite, all $\lambda_{i}>0$. Hence we can write

$$
\boldsymbol{\Sigma}^{-1}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda}^{-1} \boldsymbol{\Gamma}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda}^{-1 / 2} \boldsymbol{\Gamma} \boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda}^{-1 / 2} \boldsymbol{\Gamma}
$$

where $\boldsymbol{\Lambda}^{-1 / 2}=\operatorname{diag}\left\{\lambda_{1}^{-1 / 2}, \ldots, \lambda_{n}^{-1 / 2}\right\}$. Thus we have

$$
Q=\left\{\boldsymbol{\Lambda}^{-1 / 2} \boldsymbol{\Gamma}(\mathbf{X}-\boldsymbol{\mu})\right\}^{\prime} \mathbf{I}\left\{\boldsymbol{\Lambda}^{-1 / 2} \boldsymbol{\Gamma}(\mathbf{X}-\boldsymbol{\mu})\right\}
$$

But by Theorem 3.5.2, it is easy to show that the random vector $\boldsymbol{\Lambda}^{-1 / 2} \boldsymbol{\Gamma}(\mathbf{X}-\boldsymbol{\mu})$ has a $N_{n}(\mathbf{0}, \mathbf{I})$ distribution; hence, $Q$ has a $\chi^{2}(n)$ distribution.

The remarkable fact that the random variable $Q$ in the last theorem is $\chi^{2}(n)$ stimulates a number of questions about quadratic forms in normally distributed\\
variables. We would like to treat this problem generally, but limitations of space forbid this, and we find it necessary to restrict ourselves to some special cases; see, for instance, Stapleton (2009) for discussion.

Recall from linear algebra that a symmetric matrix $\mathbf{A}$ is idempotent if $\mathbf{A}^{2}=\mathbf{A}$. In Section 9.1, we have already met some idempotent matrices. For example, the matrix $\mathbf{I}-\frac{1}{n} \mathbf{J}$ of Example 9.8.1 is idempotent. Idempotent matrices possess some important characteristics. Suppose $\lambda$ is an eigenvalue of an idempotent matrix A with corresponding eigenvector $\mathbf{v}$. Then the following identity is true:

$$
\lambda \mathbf{v}=\mathbf{A} \mathbf{v}=\mathbf{A}^{2} \mathbf{v}=\lambda \mathbf{A} \mathbf{v}=\lambda^{2} \mathbf{v}
$$

Hence $\lambda(\lambda-1) \mathbf{v}=\mathbf{0}$. Since $\mathbf{v} \neq \mathbf{0}, \lambda=0$ or 1 . Conversely, if the eigenvalues of a real symmetric matrix are only 0 s and 1 s then it is idempotent; see Exercise 9.8.10. Thus the rank of an idempotent matrix $\mathbf{A}$ is the number of its eigenvalues which are 1. Denote the spectral decomposition of $\mathbf{A}$ by $\mathbf{A}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda} \boldsymbol{\Gamma}$, where $\boldsymbol{\Lambda}$ is a diagonal matrix of eigenvalues and $\boldsymbol{\Gamma}$ is an orthogonal matrix whose columns are the corresponding orthonormal eigenvectors. Because the diagonal entries of $\boldsymbol{\Lambda}$ are 0 or 1 and $\boldsymbol{\Gamma}$ is orthogonal, we have

$$
\operatorname{tr} \mathbf{A}=\operatorname{tr} \boldsymbol{\Lambda} \boldsymbol{\Gamma} \boldsymbol{\Gamma}^{\prime}=\operatorname{tr} \boldsymbol{\Lambda}=\operatorname{rank}(\mathbf{A})
$$

i.e., the rank of an idempotent matrix is equal to its trace.

Theorem 9.8.4. Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$, where $X_{1}, \ldots, X_{n}$ are iid $N\left(0, \sigma^{2}\right)$. Let $Q=\sigma^{-2} \mathbf{X}^{\prime} \mathbf{A X}$ for a symmetric matrix $\mathbf{A}$ with rank $r$. Then $Q$ has a $\chi^{2}(r)$ distribution if and only if $\mathbf{A}$ is idempotent.

Proof: By Theorem 9.8.2, the mgf of $Q$ is


\begin{equation*}
M_{Q}(t)=\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)^{-1 / 2} \tag{9.8.14}
\end{equation*}


where $\lambda_{1}, \ldots, \lambda_{r}$ are the $r$ nonzero eigenvalues of $\mathbf{A}$. Suppose, first, that $\mathbf{A}$ is idempotent. Then $\lambda_{1}=\cdots=\lambda_{r}=1$ and the mgf of $Q$ is $M_{Q}(t)=(1-2 t)^{-r / 2}$; i.e., $Q$ has a $\chi^{2}(r)$ distribution. Next, suppose $Q$ has a $\chi^{2}(r)$ distribution. Then for $t$ in a neighborhood of 0 , we have the identity

$$
\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)^{-1 / 2}=(1-2 t)^{-r / 2}
$$

which, upon squaring both sides, leads to

$$
\prod_{i=1}^{r}\left(1-2 t \lambda_{i}\right)=(1-2 t)^{r}
$$

By the uniqueness of the factorization of polynomials, $\lambda_{1}=\cdots=\lambda_{r}=1$. Hence $\mathbf{A}$ is idempotent.

Example 9.8.3. Based on this last theorem, we can obtain quickly the distribution of the sample variance when sampling from a normal distribution. Suppose $X_{1}, X_{2}, \ldots, X_{n}$ are iid $N\left(\mu, \sigma^{2}\right)$. Let $\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{n}\right)^{\prime}$. Then $\mathbf{X}$ has a $N_{n}\left(\mu \mathbf{1}, \sigma^{2} \mathbf{I}\right)$ distribution, where $\mathbf{1}$ denotes a $n \times 1$ vector with all components equal to 1. Let $S^{2}=(n-1)^{-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$. Then by Example 9.8.1, we can write

$$
\frac{(n-1) S^{2}}{\sigma^{2}}=\sigma^{-2} \mathbf{X}^{\prime}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right) \mathbf{X}=\sigma^{-2}(\mathbf{X}-\mu \mathbf{1})^{\prime}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right)(\mathbf{X}-\mu \mathbf{1}),
$$

where the last equality holds because $\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right) \mathbf{1}=\mathbf{0}$. Because the matrix $\mathbf{I}-\frac{1}{n} \mathbf{J}$ is idempotent, $\operatorname{tr}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right)=n-1$, and $\mathbf{X}-\mu \mathbf{1}$ is $N_{n}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right)$, it follows from Theorem 9.8.4 that $(n-1) S^{2} / \sigma^{2}$ has a $\chi^{2}(n-1)$ distribution.

Remark 9.8.3. If the normal distribution in Theorem 9.8.4 is $N_{n}\left(\boldsymbol{\mu}, \sigma^{2} \mathbf{I}\right)$, the condition $\mathbf{A}^{2}=\mathbf{A}$ remains a necessary and sufficient condition that $Q / \sigma^{2}$ have a chi-square distribution. In general, however, $Q / \sigma^{2}$ is not central $\chi^{2}(r)$ but instead, $Q / \sigma^{2}$ has a noncentral chi-square distribution if $\mathbf{A}^{2}=\mathbf{A}$. The number of degrees of freedom is $r$, the rank of $\mathbf{A}$, and the noncentrality parameter is $\boldsymbol{\mu}^{\prime} \mathbf{A} \boldsymbol{\mu} / \sigma^{2}$. If $\boldsymbol{\mu}=\mu \mathbf{1}$, then $\boldsymbol{\mu}^{\prime} \mathbf{A} \boldsymbol{\mu}=\mu^{2} \sum_{i, j} a_{i j}$, where $\mathbf{A}=\left[a_{i j}\right]$. Then, if $\mu \neq 0$, the conditions $\mathbf{A}^{2}=\mathbf{A}$ and $\sum_{i, j} a_{i j}=0$ are necessary and sufficient conditions that $Q / \sigma^{2}$ be central $\chi^{2}(r)$. Moreover, the theorem may be extended to a quadratic form in random variables which have a multivariate normal distribution with positive definite covariance matrix $\boldsymbol{\Sigma}$; here the necessary and sufficient condition that $Q$ have a chi-square distribution is $\mathbf{A} \boldsymbol{\Sigma} \mathbf{A}=\mathbf{A}$. See Exercise 9.8.9.

\section*{EXERCISES}
9.8.1. Let $Q=X_{1} X_{2}-X_{3} X_{4}$, where $X_{1}, X_{2}, X_{3}, X_{4}$ is a random sample of size 4 from a distribution that is $N\left(0, \sigma^{2}\right)$. Show that $Q / \sigma^{2}$ does not have a chi-square distribution. Find the mgf of $Q / \sigma^{2}$.\\
9.8.2. Let $\mathbf{X}^{\prime}=\left[X_{1}, X_{2}\right]$ be bivariate normal with matrix of means $\boldsymbol{\mu}^{\prime}=\left[\mu_{1}, \mu_{2}\right]$ and positive definite covariance matrix $\boldsymbol{\Sigma}$. Let

$$
Q_{1}=\frac{X_{1}^{2}}{\sigma_{1}^{2}\left(1-\rho^{2}\right)}-2 \rho \frac{X_{1} X_{2}}{\sigma_{1} \sigma_{2}\left(1-\rho^{2}\right)}+\frac{X_{2}^{2}}{\sigma_{2}^{2}\left(1-\rho^{2}\right)}
$$

Show that $Q_{1}$ is $\chi^{2}(r, \theta)$ and find $r$ and $\theta$. When and only when does $Q_{1}$ have a central chi-square distribution?\\
9.8.3. Let $\mathbf{X}^{\prime}=\left[X_{1}, X_{2}, X_{3}\right]$ denote a random sample of size 3 from a distribution that is $N(4,8)$ and let

$$
\mathbf{A}=\left(\begin{array}{ccc}
\frac{1}{2} & 0 & \frac{1}{2} \\
0 & 1 & 0 \\
\frac{1}{2} & 0 & \frac{1}{2}
\end{array}\right)
$$

Let $Q=\mathbf{X}^{\prime} \mathbf{A X} / \sigma^{2}$.\\
(a) Use Theorem 9.8.1 to find the $E(Q)$.\\
(b) Justify the assertion that $Q$ is $\chi^{2}(2,6)$.\\
9.8.4. Suppose $X_{1}, \ldots, X_{n}$ are independent random variables with the common mean $\mu$ but with unequal variances $\sigma_{i}^{2}=\operatorname{Var}\left(X_{i}\right)$.\\
(a) Determine the variance of $\bar{X}$.\\
(b) Determine the constant $K$ so that $Q=K \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$ is an unbiased estimate of the variance of $\bar{X}$. (Hint: Proceed as in Example 9.8.3.)\\
9.8.5. Suppose $X_{1}, \ldots, X_{n}$ are correlated random variables, with common mean $\mu$ and variance $\sigma^{2}$ but with correlations $\rho$ (all correlations are the same).\\
(a) Determine the variance of $\bar{X}$.\\
(b) Determine the constant $K$ so that $Q=K \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$ is an unbiased estimate of the variance of $\bar{X}$. (Hint: Proceed as in Example 9.8.3.)\\
9.8.6. Fill in the details for expression (9.8.13).\\
9.8.7. For the trace operator defined in expression (9.8.1), prove the following properties are true.\\
(a) If $\mathbf{A}$ and $\mathbf{B}$ are $n \times n$ matrices and $a$ and $b$ are scalars, then

$$
\operatorname{tr}(a \mathbf{A}+b \mathbf{B})=a \operatorname{tr} \mathbf{A}+b \operatorname{tr} \mathbf{B}
$$

(b) If $\mathbf{A}$ is an $n \times m$ matrix, $\mathbf{B}$ is an $m \times k$ matrix, and $\mathbf{C}$ is a $k \times n$ matrix, then

$$
\operatorname{tr}(\mathbf{A B C})=\operatorname{tr}(\mathbf{B C A})=\operatorname{tr}(\mathbf{C A B}) .
$$

(c) If $\mathbf{A}$ is a square matrix and $\boldsymbol{\Gamma}$ is an orthogonal matrix, use the result of part (a) to show that $\operatorname{tr}\left(\boldsymbol{\Gamma}^{\prime} \mathbf{A} \boldsymbol{\Gamma}\right)=\operatorname{tr} \mathbf{A}$.\\
(d) If $\mathbf{A}$ is a real symmetric idempotent matrix, use the result of part (b) to prove that the rank of $\mathbf{A}$ is equal to $\operatorname{tr} \mathbf{A}$.\\
9.8.8. Let $\mathbf{A}=\left[a_{i j}\right]$ be a real symmetric matrix. Prove that $\sum_{i} \sum_{j} a_{i j}^{2}$ is equal to the sum of the squares of the eigenvalues of $\mathbf{A}$.\\
Hint: If $\boldsymbol{\Gamma}$ is an orthogonal matrix, show that $\sum_{j} \sum_{i} a_{i j}^{2}=\operatorname{tr}\left(\mathbf{A}^{2}\right)=\operatorname{tr}\left(\boldsymbol{\Gamma}^{\prime} \mathbf{A}^{2} \boldsymbol{\Gamma}\right)=$ $\operatorname{tr}\left[\left(\boldsymbol{\Gamma}^{\prime} \mathbf{A} \boldsymbol{\Gamma}\right)\left(\boldsymbol{\Gamma}^{\prime} \mathbf{A} \boldsymbol{\Gamma}\right)\right]$.\\
9.8.9. Suppose $\mathbf{X}$ has a $N_{n}(0, \boldsymbol{\Sigma})$ distribution, where $\boldsymbol{\Sigma}$ is positive definite. Let $Q=\mathbf{X}^{\prime} \mathbf{A X}$ for a symmetric matrix $\mathbf{A}$ with rank $r$. Prove $Q$ has a $\chi^{2}(r)$ distribution if and only if $\mathbf{A} \boldsymbol{\Sigma} \mathbf{A}=\mathbf{A}$.\\
Hint: Write $Q$ as

$$
Q=\left(\boldsymbol{\Sigma}^{-1 / 2} \mathbf{X}\right)^{\prime} \boldsymbol{\Sigma}^{1 / 2} \mathbf{A} \boldsymbol{\Sigma}^{1 / 2}\left(\boldsymbol{\Sigma}^{-1 / 2} \mathbf{X}\right)
$$

where $\boldsymbol{\Sigma}^{1 / 2}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda}^{1 / 2} \boldsymbol{\Gamma}$ and $\boldsymbol{\Sigma}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{\Lambda} \boldsymbol{\Gamma}$ is the spectral decomposition of $\boldsymbol{\Sigma}$. Then use Theorem 9.8.4.\\
9.8.10. Suppose $\mathbf{A}$ is a real symmetric matrix. If the eigenvalues of $\mathbf{A}$ are only 0 s and 1 s then prove that $\mathbf{A}$ is idempotent.

\subsection*{9.9 The Independence of Certain Quadratic Forms}
We have previously investigated the independence of linear functions of normally distributed variables. In this section we shall prove some theorems about the independence of quadratic forms. We shall confine our attention to normally distributed variables that constitute a random sample of size $n$ from a distribution that is $N\left(0, \sigma^{2}\right)$.

Remark 9.9.1. In the proof of the next theorem, we use the fact that if $\mathbf{A}$ is an $m \times n$ matrix of rank $n$ (i.e., $\mathbf{A}$ has full column rank), then the matrix $\mathbf{A}^{\prime} \mathbf{A}$ is nonsingular. A proof of this linear algebra fact is sketched in Exercises 9.9.12 and 9.9.13.

Theorem 9.9.1 (Craig). Let $\mathbf{X}^{\prime}=\left(X_{1}, \ldots, X_{n}\right)$, where $X_{1}, \ldots, X_{n}$ are iid $N\left(0, \sigma^{2}\right)$ random variables. For real symmetric matrices $\mathbf{A}$ and $\mathbf{B}$, let $Q_{1}=\sigma^{-2} \mathbf{X}^{\prime} \mathbf{A X}$ and $Q_{2}=\sigma^{-2} \mathbf{X}^{\prime} \mathbf{B X}$ denote quadratic forms in $\mathbf{X}$. The random variables $Q_{1}$ and $Q_{2}$ are independent if and only if $\mathbf{A B}=\mathbf{0}$.\\
Proof: First, we obtain some preliminary results. Based on these results, the proof follows immediately. Assume the ranks of the matrices $\mathbf{A}$ and $\mathbf{B}$ are $r$ and $s$, respectively. Let $\boldsymbol{\Gamma}_{1}^{\prime} \boldsymbol{\Lambda}_{1} \boldsymbol{\Gamma}_{1}$ denote the spectral decomposition of $\mathbf{A}$. Denote the $r$ nonzero eigenvalues of $\mathbf{A}$ by $\lambda_{1}, \ldots, \lambda_{r}$. Without loss of generality, assume that these nonzero eigenvalues of $\mathbf{A}$ are the first $r$ elements on the main diagonal of $\boldsymbol{\Lambda}_{1}$ and let $\boldsymbol{\Gamma}_{11}^{\prime}$ be the $n \times r$ matrix whose columns are the corresponding eigenvectors. Finally, let $\boldsymbol{\Lambda}_{11}=\operatorname{diag}\left\{\lambda_{1}, \ldots, \lambda_{r}\right\}$. Then we can write the spectral decomposition of $\mathbf{A}$ in either of the two ways


\begin{equation*}
\mathbf{A}=\boldsymbol{\Gamma}_{1}^{\prime} \boldsymbol{\Lambda}_{1} \boldsymbol{\Gamma}_{1}=\boldsymbol{\Gamma}_{11}^{\prime} \boldsymbol{\Lambda}_{11} \boldsymbol{\Gamma}_{11} \tag{9.9.1}
\end{equation*}


Note that we can write $Q_{1}$ as


\begin{equation*}
Q_{1}=\sigma^{-2} \mathbf{X}^{\prime} \boldsymbol{\Gamma}_{11}^{\prime} \boldsymbol{\Lambda}_{11} \boldsymbol{\Gamma}_{11} \mathbf{X}=\sigma^{-2}\left(\boldsymbol{\Gamma}_{11} \mathbf{X}\right)^{\prime} \boldsymbol{\Lambda}_{11}\left(\boldsymbol{\Gamma}_{11} \mathbf{X}\right)=\mathbf{W}_{1}^{\prime} \boldsymbol{\Lambda}_{11} \mathbf{W}_{1} \tag{9.9.2}
\end{equation*}


where $\mathbf{W}_{1}=\sigma^{-1} \boldsymbol{\Gamma}_{11} \mathbf{X}$. Next, obtain a similar representation based on the $s$ nonzero eigenvalues $\gamma_{1}, \ldots, \gamma_{s}$ of $\mathbf{B}$. Let $\boldsymbol{\Lambda}_{22}=\operatorname{diag}\left\{\gamma_{1}, \ldots, \gamma_{s}\right\}$ denote the $s \times s$ diagonal matrix of nonzero eigenvalues and form the $n \times s$ matrix $\boldsymbol{\Gamma}_{21}^{\prime}=\left[\mathbf{u}_{1} \cdots \mathbf{u}_{s}\right]$ of corresponding eigenvectors. Then we can write the spectral decomposition of $\mathbf{B}$ as


\begin{equation*}
\mathbf{B}=\boldsymbol{\Gamma}_{21}^{\prime} \boldsymbol{\Lambda}_{22} \boldsymbol{\Gamma}_{21} \tag{9.9.3}
\end{equation*}


Also, we can write $Q_{2}$ as


\begin{equation*}
Q_{2}=\mathbf{W}_{2}^{\prime} \boldsymbol{\Lambda}_{22} \mathbf{W}_{2}, \tag{9.9.4}
\end{equation*}


where $\mathbf{W}_{2}=\sigma^{-1} \boldsymbol{\Gamma}_{21} \mathbf{X}$. Letting $\mathbf{W}^{\prime}=\left(\mathbf{W}_{1}^{\prime}, \mathbf{W}_{2}^{\prime}\right)$, we have

$$
\mathbf{W}=\sigma^{-1}\left[\begin{array}{l}
\boldsymbol{\Gamma}_{11} \\
\boldsymbol{\Gamma}_{21}
\end{array}\right] \mathbf{X} .
$$

Because $\mathbf{X}$ has a $N_{n}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right)$ distribution, Theorem 3.5.2 shows that $\mathbf{W}$ has an $(r+s)$-dimensional multivariate normal distribution with mean $\mathbf{0}$ and variancecovariance matrix

\[
\operatorname{Var}(\mathbf{W})=\left[\begin{array}{cc}
\mathbf{I}_{r} & \boldsymbol{\Gamma}_{11} \boldsymbol{\Gamma}_{21}^{\prime}  \tag{9.9.5}\\
\boldsymbol{\Gamma}_{21} \boldsymbol{\Gamma}_{11}^{\prime} & \mathbf{I}_{s}
\end{array}\right] .
\]

Finally, using (9.9.1) and (9.9.3), we have the identity


\begin{equation*}
\mathbf{A B}=\left\{\boldsymbol{\Gamma}_{11}^{\prime} \boldsymbol{\Lambda}_{11}\right\} \boldsymbol{\Gamma}_{11} \boldsymbol{\Gamma}_{21}^{\prime}\left\{\boldsymbol{\Lambda}_{22} \boldsymbol{\Gamma}_{21}\right\} . \tag{9.9.6}
\end{equation*}


Let $\mathbf{U}$ denote the matrix in the first set of braces. Note that $\mathbf{U}$ has full column rank, so its kernel is null; i.e., its kernel consists of the vector $\mathbf{0}$. Let $\mathbf{V}$ denote the matrix in the second set of braces. Note that $\mathbf{V}$ has full row rank, hence the kernel of $\mathbf{V}^{\prime}$ is null.

For the proof then, suppose $\mathbf{A B}=\mathbf{0}$. Then

$$
\mathbf{U}\left[\boldsymbol{\Gamma}_{11} \boldsymbol{\Gamma}_{21}^{\prime} \mathbf{V}\right]=\mathbf{0}
$$

Because the kernel of $\mathbf{U}$ is null this implies each column of the matrix in the brackets is the vector $\mathbf{0}$; i.e., the matrix in the brackets is the matrix $\mathbf{0}$. This implies that

$$
\mathbf{V}^{\prime}\left[\boldsymbol{\Gamma}_{21} \boldsymbol{\Gamma}_{11}^{\prime}\right]=\mathbf{0}
$$

In the same way, because the kernel of $\mathbf{V}^{\prime}$ is null, we have $\boldsymbol{\Gamma}_{11} \boldsymbol{\Gamma}_{21}^{\prime}=\mathbf{0}$. Hence, by (9.9.5), the random vectors $\mathbf{W}_{1}$ and $\mathbf{W}_{2}$ are independent. Therefore, by (9.9.2) and (9.9.4), $Q_{1}$ and $Q_{2}$ are independent.

Conversely, if $Q_{1}$ and $Q_{2}$ are independent, then


\begin{equation*}
\left\{E\left[\exp \left\{t_{1} Q_{1}+t_{2} Q_{2}\right\}\right]\right\}^{-2}=\left\{E\left[\exp \left\{t_{1} Q_{1}\right\}\right]\right\}^{-2}\left\{E\left[\exp \left\{t_{2} Q_{2}\right\}\right]\right\}^{-2} \tag{9.9.7}
\end{equation*}


for $\left(t_{1}, t_{2}\right)$ in an open neighborhood of $(0,0)$. Note that $t_{1} Q_{1}+t_{2} Q_{2}$ is a quadratic form in $\mathbf{X}$ with symmetric matrix $t_{1} \mathbf{A}+t_{2} \mathbf{B}$. Recall that the matrix $\boldsymbol{\Gamma}_{1}$ is orthogonal and hence has determinant $\pm 1$. Using this and Theorem 9.8.2, we can write the left side of (9.9.7) as


\begin{align*}
E^{-2}\left[\exp \left\{t_{1} Q_{1}+t_{2} Q_{2}\right\}\right] & =\left|\mathbf{I}_{n}-2 t_{1} \mathbf{A}-2 t_{2} \mathbf{B}\right| \\
& =\left|\boldsymbol{\Gamma}_{1}^{\prime} \boldsymbol{\Gamma}_{1}-2 t_{1} \boldsymbol{\Gamma}_{1}^{\prime} \boldsymbol{\Lambda}_{1} \boldsymbol{\Gamma}_{1}-2 t_{2} \boldsymbol{\Gamma}_{1}^{\prime}\left(\boldsymbol{\Gamma}_{1} \mathbf{B} \boldsymbol{\Gamma}_{1}^{\prime}\right) \boldsymbol{\Gamma}_{1}\right| \\
& =\left|\mathbf{I}_{n}-2 t_{1} \boldsymbol{\Lambda}_{1}-2 t_{2} \mathbf{D}\right| \tag{9.9.8}
\end{align*}


where the matrix $\mathbf{D}$ is given by

\[
\mathbf{D}=\boldsymbol{\Gamma}_{1} \mathbf{B} \boldsymbol{\Gamma}_{1}^{\prime}=\left[\begin{array}{ll}
\mathbf{D}_{11} & \mathbf{D}_{12}  \tag{9.9.9}\\
\mathbf{D}_{21} & \mathbf{D}_{22}
\end{array}\right]
\]

and $\mathbf{D}_{11}$ is $r \times r$. By (9.9.2), (9.9.3), and Theorem 9.8.2, the right side of (9.9.7) can be written as


\begin{equation*}
\left\{E\left[\exp \left\{t_{1} Q_{1}\right\}\right]\right\}^{-2}\left\{E\left[\exp \left\{t_{2} Q_{2}\right\}\right]\right\}^{-2}=\left\{\prod_{i=1}^{r}\left(1-2 t_{1} \lambda_{i}\right)\right\}\left|\mathbf{I}_{n}-2 t_{2} \mathbf{D}\right| \tag{9.9.10}
\end{equation*}


This leads to the identity


\begin{equation*}
\left|\mathbf{I}_{n}-2 t_{1} \boldsymbol{\Lambda}_{1}-2 t_{2} \mathbf{D}\right|=\left\{\prod_{i=1}^{r}\left(1-2 t_{1} \lambda_{i}\right)\right\}\left|\mathbf{I}_{n}-2 t_{2} \mathbf{D}\right| \tag{9.9.11}
\end{equation*}


for $\left(t_{1}, t_{2}\right)$ in an open neighborhood of $(0,0)$.\\
The coefficient of $\left(-2 t_{1}\right)^{r}$ on the right side of (9.9.11) is $\lambda_{1} \cdots \lambda_{r}\left|\mathbf{I}-2 t_{2} \mathbf{D}\right|$. It is not so easy to find the coefficient of $\left(-2 t_{1}\right)^{r}$ in the left side of the equation (9.9.11). Conceive of expanding this determinant in terms of minors of order $r$ formed from the first $r$ columns. One term in this expansion is the product of the minor of order $r$ in the upper left-hand corner, namely, $\left|\mathbf{I}_{r}-2 t_{1} \mathbf{\Lambda}_{11}-2 t_{2} \mathbf{D}_{11}\right|$, and the minor of order $n-r$ in the lower right-hand corner, namely, $\left|\mathbf{I}_{n-r}-2 t_{2} \mathbf{D}_{22}\right|$. Moreover, this product is the only term in the expansion of the determinant that involves $\left(-2 t_{1}\right)^{r}$. Thus the coefficient of $\left(-2 t_{1}\right)^{r}$ in the left-hand member of Equation (9.9.11) is $\lambda_{1} \cdots \lambda_{r}\left|\mathbf{I}_{n-r}-2 t_{2} \mathbf{D}_{22}\right|$. If we equate these coefficients of $\left(-2 t_{1}\right)^{r}$, we have


\begin{equation*}
\left|\mathbf{I}-2 t_{2} \mathbf{D}\right|=\left|\mathbf{I}_{n-r}-2 t_{2} \mathbf{D}_{22}\right|, \tag{9.9.12}
\end{equation*}


for $t_{2}$ in an open neighborhood of 0 . Equation (9.9.12) implies that the nonzero eigenvalues of the matrices $\mathbf{D}$ and $\mathbf{D}_{22}$ are the same (see Exercise 9.9.8). Recall that the sum of the squares of the eigenvalues of a symmetric matrix is equal to the sum of the squares of the elements of that matrix (see Exercise 9.8.8). Thus the sum of the squares of the elements of matrix $\mathbf{D}$ is equal to the sum of the squares of the elements of $\mathbf{D}_{22}$. Since the elements of the matrix $\mathbf{D}$ are real, it follows that each of the elements of $\mathbf{D}_{11}, \mathbf{D}_{12}$, and $\mathbf{D}_{21}$ is zero. Hence we can write

$$
\mathbf{0}=\Lambda_{1} \mathbf{D}=\boldsymbol{\Gamma}_{1} \mathbf{A} \Gamma_{1}^{\prime} \boldsymbol{\Gamma}_{1} \mathbf{B} \Gamma_{1}^{\prime}
$$

because $\boldsymbol{\Gamma}_{1}$ is an orthogonal matrix, $\mathbf{A B}=\mathbf{0}$.

Remark 9.9.2. Theorem 9.9.1 remains valid if the random sample is from a distribution that is $N\left(\mu, \sigma^{2}\right)$, whatever the real value of $\mu$. Moreover, Theorem 9.9.1 may be extended to quadratic forms in random variables that have a joint multivariate normal distribution with a positive definite covariance matrix $\boldsymbol{\Sigma}$. The necessary and sufficient condition for the independence of two such quadratic forms with symmetric matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ then becomes $\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{B}=\mathbf{0}$. In our Theorem 9.9.1, we have $\boldsymbol{\Sigma}=\sigma^{2} \boldsymbol{I}$, so that $\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{B}=\boldsymbol{A} \sigma^{2} \boldsymbol{I} \boldsymbol{B}=\sigma^{2} \boldsymbol{A} \boldsymbol{B}=\mathbf{0}$.

The following theorem is from Hogg and Craig (1958).\\
Theorem 9.9.2 (Hogg and Craig). Define the sum $Q=Q_{1}+\cdots+Q_{k-1}+Q_{k}$, where $Q, Q_{1}, \ldots, Q_{k-1}, Q_{k}$ are $k+1$ random variables that are quadratic forms in the observations of a random sample of size $n$ from a distribution that is $N\left(0, \sigma^{2}\right)$. Let $Q / \sigma^{2}$ be $\chi^{2}(r)$, let $Q_{i} / \sigma^{2}$ be $\chi^{2}\left(r_{i}\right), i=1,2, \ldots, k-1$, and let $Q_{k}$ be nonnegative. Then the random variables $Q_{1}, Q_{2}, \ldots, Q_{k}$ are independent and, hence, $Q_{k} / \sigma^{2}$ is $\chi^{2}\left(r_{k}=r-r_{1}-\cdots-r_{k-1}\right)$.

Proof: Take first the case of $k=2$ and let the real symmetric matrices $Q, Q_{1}$, and $Q_{2}$ be denoted, respectively, by $\boldsymbol{A}, \boldsymbol{A}_{1}, \boldsymbol{A}_{2}$. We are given that $Q=Q_{1}+Q_{2}$ or, equivalently, that $\boldsymbol{A}=\boldsymbol{A}_{1}+\boldsymbol{A}_{2}$. We are also given that $Q / \sigma^{2}$ is $\chi^{2}(r)$ and that $Q_{1} / \sigma^{2}$ is $\chi^{2}\left(r_{1}\right)$. In accordance with Theorem 9.8.4, we have $\boldsymbol{A}^{2}=\boldsymbol{A}$ and $\boldsymbol{A}_{1}^{2}=\boldsymbol{A}$.

Since $Q_{2} \geq 0$, each of the matrices $\boldsymbol{A}, \boldsymbol{A}_{1}$, and $\boldsymbol{A}_{2}$ is positive semidefinite. Because $\boldsymbol{A}^{2}=\boldsymbol{A}$, we can find an orthogonal matrix $\Gamma$ such that

$$
\boldsymbol{\Gamma}^{\prime} \boldsymbol{A} \boldsymbol{\Gamma}=\left[\begin{array}{ll}
\mathbf{I}_{r} & \mathrm{O} \\
\mathbf{O} & \mathrm{O}
\end{array}\right]
$$

If we multiply both members of $\boldsymbol{A}=\boldsymbol{A}_{1}+\boldsymbol{A}_{2}$ on the left by $\boldsymbol{\Gamma}^{\prime}$ and on the right by $\Gamma$, we have

$$
\left[\begin{array}{cc}
\boldsymbol{I}_{r} & 0 \\
0 & 0
\end{array}\right]=\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}+\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{2} \boldsymbol{\Gamma}
$$

Now each of $\boldsymbol{A}_{1}$ and $\boldsymbol{A}_{2}$, and hence each of $\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}$ and $\boldsymbol{\Gamma} / \boldsymbol{A}_{2} \boldsymbol{\Gamma}$ is positive semidefinite. Recall that if a real symmetric matrix is positive semidefinite, each element on the principal diagonal is positive or zero. Moreover, if an element on the principal diagonal is zero, then all elements in that row and all elements in that column are zero. Thus $\boldsymbol{\Gamma}^{\prime} \boldsymbol{A} \boldsymbol{\Gamma}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}+\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{2} \boldsymbol{\Gamma}$ can be written as

\[
\left[\begin{array}{cc}
\boldsymbol{I}_{r} & 0  \tag{9.9.13}\\
\mathbf{0} & \mathbf{0}
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{G}_{r} & 0 \\
\mathbf{0} & \mathbf{0}
\end{array}\right]+\left[\begin{array}{cc}
\boldsymbol{H}_{r} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]
\]

Since $\boldsymbol{A}_{1}^{2}=\boldsymbol{A}_{1}$, we have

$$
\left(\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}\right)^{2}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}=\left[\begin{array}{cc}
\boldsymbol{G}_{r} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]
$$

If we multiply both members of Equation (9.9.13) on the left by the matrix $\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}$, we see that

$$
\left[\begin{array}{cc}
\boldsymbol{G}_{r} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{G}_{r} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]+\left[\begin{array}{cc}
\boldsymbol{G}_{r} \boldsymbol{H}_{r} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]
$$

or, equivalently, $\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}=\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}+\left(\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}\right)\left(\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{2} \boldsymbol{\Gamma}\right)$. Thus $\left(\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{1} \boldsymbol{\Gamma}\right) \times\left(\boldsymbol{\Gamma}^{\prime} \boldsymbol{A}_{2} \boldsymbol{\Gamma}\right)=\mathbf{0}$ and $\boldsymbol{A}_{1} \boldsymbol{A}_{2}=\mathbf{0}$. In accordance with Theorem 9.9.1, $Q_{1}$ and $Q_{2}$ are independent. This independence immediately implies that $Q_{2} / \sigma^{2}$ is $\chi^{2}\left(r_{2}=r-r_{1}\right)$. This completes the proof when $k=2$. For $k>2$, the proof may be made by induction. We shall merely indicate how this can be done by using $k=3$. Take $\boldsymbol{A}=\boldsymbol{A}_{1}+\boldsymbol{A}_{2}+\boldsymbol{A}_{3}$, where $\boldsymbol{A}^{2}=\boldsymbol{A}, \boldsymbol{A}_{1}^{2}=\boldsymbol{A}_{1}, \boldsymbol{A}_{2}^{2}=\boldsymbol{A}_{2}$, and $\boldsymbol{A}_{3}$ is positive semidefinite. Write $\boldsymbol{A}=\boldsymbol{A}_{1}+\left(\boldsymbol{A}_{2}+\boldsymbol{A}_{3}\right)=\boldsymbol{A}_{1}+\boldsymbol{B}_{1}$, say. Now $\boldsymbol{A}^{2}=\boldsymbol{A}, \boldsymbol{A}_{1}^{2}=\boldsymbol{A}_{1}$, and $\boldsymbol{B}_{1}$ is positive semidefinite. In accordance with the case of $k=2$, we have $\boldsymbol{A}_{1} \boldsymbol{B}_{1}=\mathbf{0}$, so that $\boldsymbol{B}_{1}^{2}=\boldsymbol{B}_{1}$. With $\boldsymbol{B}_{1}=\boldsymbol{A}_{2}+\boldsymbol{A}_{3}$, where $\boldsymbol{B}_{1}^{2}=\boldsymbol{B}_{1}, \boldsymbol{A}_{2}^{2}=\boldsymbol{A}_{2}$, it follows from the case of $k=2$ that $\boldsymbol{A}_{2} \boldsymbol{A}_{3}=\mathbf{0}$ and $\boldsymbol{A}_{3}^{2}=\boldsymbol{A}_{3}$. If we regroup by writing $\boldsymbol{A}=\boldsymbol{A}_{2}+\left(\boldsymbol{A}_{1}+\boldsymbol{A}_{3}\right)$, we obtain $\boldsymbol{A}_{1} \boldsymbol{A}_{3}=\mathbf{0}$, and so on.

Remark 9.9.3. In our statement of Theorem 9.9.2, we took $X_{1}, X_{2}, \ldots, X_{n}$ to be observations of a random sample from a distribution that is $N\left(0, \sigma^{2}\right)$. We did this because our proof of Theorem 9.9.1 was restricted to that case. In fact, if $Q^{\prime}, Q_{1}^{\prime}, \ldots, Q_{k}^{\prime}$ are quadratic forms in any normal variables (including multivariate normal variables), if $Q^{\prime}=Q_{1}^{\prime}+\cdots+Q_{k}^{\prime}$, if $Q^{\prime}, Q_{1}^{\prime}, \ldots, Q_{k-1}^{\prime}$ are central or noncentral chi-square, and if $Q_{k}^{\prime}$ is nonnegative, then $Q_{1}^{\prime}, \ldots, Q_{k}^{\prime}$ are independent and $Q_{k}^{\prime}$ is either central or noncentral chi-square.

This section concludes with a proof of a frequently quoted theorem due to Cochran.

Theorem 9.9.3 (Cochran). Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from a distribution that is $N\left(0, \sigma^{2}\right)$. Let the sum of the squares of these observations be written in the form

$$
\sum_{1}^{n} X_{i}^{2}=Q_{1}+Q_{2}+\cdots+Q_{k}
$$

where $Q_{j}$ is a quadratic form in $X_{1}, X_{2}, \ldots, X_{n}$, with matrix $\boldsymbol{A}_{j}$ that has rank $r_{j}, j=1,2, \ldots, k$. The random variables $Q_{1}, Q_{2}, \ldots, Q_{k}$ are independent and $Q_{j} / \sigma^{2}$ is $\chi^{2}\left(r_{j}\right), j=1,2, \ldots, k$, if and only if $\sum_{1}^{k} r_{j}=n$.\\
Proof. First assume the two conditions $\sum_{1}^{k} r_{j}=n$ and $\sum_{1}^{n} X_{i}^{2}=\sum_{1}^{k} Q_{j}$ to be satisfied. The latter equation implies that $\boldsymbol{I}=\boldsymbol{A}_{1}+\boldsymbol{A}_{2}+\cdots+\boldsymbol{A}_{k}$. Let $\boldsymbol{B}_{i}=\boldsymbol{I}-\boldsymbol{A}_{i}$; that is, $\boldsymbol{B}_{i}$ is the sum of the matrices $\boldsymbol{A}_{1}, \ldots, \boldsymbol{A}_{k}$ exclusive of $\boldsymbol{A}_{i}$. Let $R_{i}$ denote the rank of $\boldsymbol{B}_{i}$. Since the rank of the sum of several matrices is less than or equal to the sum of the ranks, we have $R_{i} \leq \sum_{1}^{k} r_{j}-r_{i}=n-r_{i}$. However, $\boldsymbol{I}=\boldsymbol{A}_{i}+\boldsymbol{B}_{i}$, so that $n \leq r_{i}+R_{i}$ and $n-r_{i} \leq R_{i}$. Hence $R_{i}=n-r_{i}$. The eigenvalues of $\boldsymbol{B}_{i}$ are the roots of the equation $\left|\boldsymbol{B}_{i}-\lambda \boldsymbol{I}\right|=0$. Since $\boldsymbol{B}_{i}=\boldsymbol{I}-\boldsymbol{A}_{i}$, this equation can be written as $\left|\boldsymbol{I}-\boldsymbol{A}_{i}-\lambda \boldsymbol{I}\right|=0$. Thus we have $\left|\boldsymbol{A}_{i}-(1-\lambda) \boldsymbol{I}\right|=0$. But each root of the last equation is 1 minus an eigenvalue of $\boldsymbol{A}_{i}$. Since $\boldsymbol{B}_{i}$ has exactly $n-R_{i}=r_{i}$ eigenvalues that are zero, then $\boldsymbol{A}_{i}$ has exactly $r_{i}$ eigenvalues that are equal to 1 . However, $r_{i}$ is the rank of $\boldsymbol{A}_{i}$. Thus each of the $r_{i}$ nonzero eigenvalues of $\boldsymbol{A}_{i}$ is 1 . That is, $\boldsymbol{A}_{i}^{2}=\boldsymbol{A}_{i}$ and thus $Q_{i} / \sigma^{2}$ has a $\chi^{2}\left(r_{i}\right)$, for $i=1,2, \ldots, k$. In accordance with Theorem 9.9.2, the random variables $Q_{1}, Q_{2}, \ldots, Q_{k}$ are independent.

To complete the proof of Theorem 9.9.3, take

$$
\sum_{1}^{n} X_{i}^{2}=Q_{1}+Q_{2}+\cdots+Q_{k}
$$

let $Q_{1}, Q_{2}, \ldots, Q_{k}$ be independent, and let $Q_{j} / \sigma^{2}$ be $\chi^{2}\left(r_{j}\right), j=1,2, \ldots, k$. Then $\sum_{1}^{k} Q_{j} / \sigma^{2}$ is $\chi^{2}\left(\sum_{1}^{k} r_{j}\right)$. But $\sum_{1}^{k} Q_{j} / \sigma^{2}=\sum_{1}^{n} X_{i}^{2} / \sigma^{2}$ is $\chi^{2}(n)$. Thus $\sum_{1}^{k} r_{j}=n$ and the proof is complete.

\section*{EXERCISES}
9.9.1. Let $X_{1}, X_{2}, X_{3}$ be a random sample from the normal distribution $N\left(0, \sigma^{2}\right)$. Are the quadratic forms $X_{1}^{2}+3 X_{1} X_{2}+X_{2}^{2}+X_{1} X_{3}+X_{3}^{2}$ and $X_{1}^{2}-2 X_{1} X_{2}+\frac{2}{3} X_{2}^{2}-$ $2 X_{1} X_{2}-X_{3}^{2}$ independent or dependent?\\
9.9.2. Let $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample of size $n$ from a distribution that is $N\left(0, \sigma^{2}\right)$. Prove that $\sum_{1}^{n} X_{i}^{2}$ and every quadratic form, that is nonidentically zero in $X_{1}, X_{2}, \ldots, X_{n}$, are dependent.\\
9.9.3. Let $X_{1}, X_{2}, X_{3}, X_{4}$ denote a random sample of size 4 from a distribution that is $N\left(0, \sigma^{2}\right)$. Let $Y=\sum_{1}^{4} a_{i} X_{i}$, where $a_{1}, a_{2}, a_{3}$, and $a_{4}$ are real constants. If $Y^{2}$ and $Q=X_{1} X_{2}-X_{3} X_{4}$ are independent, determine $a_{1}, a_{2}, a_{3}$, and $a_{4}$.\\
9.9.4. Let $\boldsymbol{A}$ be the real symmetric matrix of a quadratic form $Q$ in the observations of a random sample of size $n$ from a distribution that is $N\left(0, \sigma^{2}\right)$. Given that $Q$ and the mean $\bar{X}$ of the sample are independent, what can be said of the elements of each row (column) of $\boldsymbol{A}$ ?\\
Hint: Are $Q$ and $\bar{X}^{2}$ independent?\\
9.9.5. Let $\boldsymbol{A}_{1}, \boldsymbol{A}_{2}, \ldots, \boldsymbol{A}_{k}$ be the matrices of $k>2$ quadratic forms $Q_{1}, Q_{2}, \ldots, Q_{k}$ in the observations of a random sample of size $n$ from a distribution that is $N\left(0, \sigma^{2}\right)$. Prove that the pairwise independence of these forms implies that they are mutually independent.\\
Hint: Show that $\boldsymbol{A}_{i} \boldsymbol{A}_{j}=\mathbf{0}, i \neq j$, permits $E\left[\exp \left(t_{1} Q_{1}+t_{2} Q_{2}+\cdots+t_{k} Q_{k}\right)\right]$ to be written as a product of the mgfs of $Q_{1}, Q_{2}, \ldots, Q_{k}$.\\
9.9.6. Let $\boldsymbol{X}^{\prime}=\left[X_{1}, X_{2}, \ldots, X_{n}\right]$, where $X_{1}, X_{2}, \ldots, X_{n}$ are observations of a random sample from a distribution that is $N\left(0, \sigma^{2}\right)$. Let $\boldsymbol{b}^{\prime}=\left[b_{1}, b_{2}, \ldots, b_{n}\right]$ be a real nonzero vector, and let $\boldsymbol{A}$ be a real symmetric matrix of order $n$. Prove that the linear form $\boldsymbol{b}^{\prime} \boldsymbol{X}$ and the quadratic form $\boldsymbol{X}^{\prime} \boldsymbol{A} \boldsymbol{X}$ are independent if and only if $\boldsymbol{b}^{\prime} \boldsymbol{A}=\mathbf{0}$. Use this fact to prove that $\boldsymbol{b}^{\prime} \boldsymbol{X}$ and $\boldsymbol{X}^{\prime} \boldsymbol{A} \boldsymbol{X}$ are independent if and only if the two quadratic forms $\left(\boldsymbol{b}^{\prime} \boldsymbol{X}\right)^{2}=\boldsymbol{X}^{\prime} \boldsymbol{b} \boldsymbol{b}^{\prime} \boldsymbol{X}$ and $\boldsymbol{X}^{\prime} \boldsymbol{A} \boldsymbol{X}$ are independent.\\
9.9.7. Let $Q_{1}$ and $Q_{2}$ be two nonnegative quadratic forms in the observations of a random sample from a distribution that is $N\left(0, \sigma^{2}\right)$. Show that another quadratic form $Q$ is independent of $Q_{1}+Q_{2}$ if and only if $Q$ is independent of each of $Q_{1}$ and $Q_{2}$.\\
Hint: Consider the orthogonal transformation that diagonalizes the matrix of $Q_{1}+Q_{2}$. After this transformation, what are the forms of the matrices $Q, Q_{1}$ and $Q_{2}$ if $Q$ and $Q_{1}+Q_{2}$ are independent?\\
9.9.8. Prove that Equation (9.9.12) of this section implies that the nonzero eigenvalues of the matrices $\boldsymbol{D}$ and $\boldsymbol{D}_{22}$ are the same.\\
Hint: Let $\lambda=1 /\left(2 t_{2}\right), t_{2} \neq 0$, and show that Equation (9.9.12) is equivalent to $|\boldsymbol{D}-\lambda \boldsymbol{I}|=(-\lambda)^{r}\left|\boldsymbol{D}_{22}-\lambda \boldsymbol{I}_{n-r}\right|$.\\
9.9.9. Here $Q_{1}$ and $Q_{2}$ are quadratic forms in observations of a random sample from $N(0,1)$. If $Q_{1}$ and $Q_{2}$ are independent and if $Q_{1}+Q_{2}$ has a chi-square distribution, prove that $Q_{1}$ and $Q_{2}$ are chi-square variables.\\
9.9.10. Often in regression the mean of the random variable $Y$ is a linear function of $p$-values $x_{1}, x_{2}, \ldots, x_{p}$, say $\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\beta_{p} x_{p}$, where $\boldsymbol{\beta}^{\prime}=\left(\beta_{1}, \beta_{2}, \ldots, \beta_{p}\right)$ are the regression coefficients. Suppose that $n$ values, $\boldsymbol{Y}^{\prime}=\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)$, are observed for the $x$-values in $\boldsymbol{X}=\left[x_{i j}\right]$, where $\boldsymbol{X}$ is an $n \times p$ design matrix and its $i$ th row is associated with $Y_{i}, i=1,2, \ldots, n$. Assume that $\boldsymbol{Y}$ is multivariate normal with mean $\boldsymbol{X} \boldsymbol{\beta}$ and variance-covariance matrix $\sigma^{2} \boldsymbol{I}$, where $\boldsymbol{I}$ is the $n \times n$ identity matrix.\\
(a) Note that $Y_{1}, Y_{2}, \ldots, Y_{n}$ are independent. Why?\\
(b) Since $\boldsymbol{Y}$ should approximately equal its mean $\boldsymbol{X} \boldsymbol{\beta}$, we estimate $\boldsymbol{\beta}$ by solving the normal equations $\boldsymbol{X}^{\prime} \boldsymbol{Y}=\boldsymbol{X}^{\prime} \boldsymbol{X} \boldsymbol{\beta}$ for $\boldsymbol{\beta}$. Assuming that $\boldsymbol{X}^{\prime} \boldsymbol{X}$ is nonsingular, solve the equations to get $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Y}$. Show that $\hat{\boldsymbol{\beta}}$ has a\\
multivariate normal distribution with mean $\boldsymbol{\beta}$ and variance-covariance matrix $\sigma^{2}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}$.\\
(c) Show that

$$
(\boldsymbol{Y}-\boldsymbol{X} \boldsymbol{\beta})^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \boldsymbol{\beta})=(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})+(\boldsymbol{Y}-\boldsymbol{X} \hat{\boldsymbol{\beta}})^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \hat{\boldsymbol{\beta}}),
$$

For the remainder of the exercise, let $Q$ denote the quadratic form on the left side of this expression and $Q_{1}$ and $Q_{2}$ denote the respective quadratic forms on the right side. Hence, $Q=Q_{1}+Q_{2}$.\\
(d) Show that $Q_{1} / \sigma^{2}$ is $\chi^{2}(p)$.\\
(e) Show that $Q_{1}$ and $Q_{2}$ are independent.\\
(f) Argue that $Q_{2} / \sigma^{2}$ is $\chi^{2}(n-p)$.\\
(g) Find $c$ so that $c Q_{1} / Q_{2}$ has an $F$-distribution.\\
(h) The fact that a value $d$ can be found so that $P\left(c Q_{1} / Q_{2} \leq d\right)=1-\alpha$ could be used to find a $100(1-\alpha) \%$ confidence ellipsoid for $\boldsymbol{\beta}$. Explain.\\
9.9.11. Say that G.P.A. $(Y)$ is thought to be a linear function of a "coded" high school rank ( $x_{2}$ ) and a "coded" American College Testing score ( $x_{3}$ ), namely, $\beta_{1}+$ $\beta_{2} x_{2}+\beta_{3} x_{3}$. Note that all $x_{1}$ values equal 1. We observe the following five points:

\begin{center}
\begin{tabular}{cccc}
\hline
$x_{1}$ & $x_{2}$ & $x_{3}$ & $Y$ \\
\hline
1 & 1 & 2 & 3 \\
1 & 4 & 3 & 6 \\
1 & 2 & 2 & 4 \\
1 & 4 & 2 & 4 \\
1 & 3 & 2 & 4 \\
\hline
\end{tabular}
\end{center}

(a) Compute $\boldsymbol{X}^{\prime} \boldsymbol{X}$ and $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Y}$.\\
(b) Compute a $95 \%$ confidence ellipsoid for $\boldsymbol{\beta}^{\prime}=\left(\beta_{1}, \beta_{2}, \beta_{3}\right)$. See part (h) of Exercise 9.9.10.\\
9.9.12. Assume that $\mathbf{X}$ is an $n \times p$ matrix. Then the kernel of $\mathbf{X}$ is defined to be the space $\operatorname{ker}(\mathbf{X})=\{\mathbf{b}: \mathbf{X b}=\mathbf{0}\}$.\\
(a) Show that $\operatorname{ker}(\mathbf{X})$ is a subspace of $R^{p}$.\\
(b) The dimension of $\operatorname{ker}(\mathbf{X})$ is called the nullity of $\mathbf{X}$ and is denoted by $\nu(\mathbf{X})$. Let $\rho(\mathbf{X})$ denote the rank of $\mathbf{X}$. A fundamental theorem of linear algebra says that $\rho(\mathbf{X})+\nu(\mathbf{X})=p$. Use this to show that if $\mathbf{X}$ has full column rank, then $\operatorname{ker}(\mathbf{X})=\{\mathbf{0}\}$.\\
9.9.13. Suppose $\mathbf{X}$ is an $n \times p$ matrix with rank $p$.\\
(a) Show that $\operatorname{ker}\left(\mathbf{X}^{\prime} \mathbf{X}\right)=\operatorname{ker}(\mathbf{X})$.\\
(b) Use part (a) and the last exercise to show that if $\mathbf{X}$ has full column rank, then $\mathbf{X}^{\prime} \mathbf{X}$ is nonsingular.
